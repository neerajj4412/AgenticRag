{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19ab913a",
   "metadata": {},
   "source": [
    "### Agentic RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2985b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T20:33:26.034194Z",
     "iopub.status.busy": "2026-02-16T20:33:26.034096Z",
     "iopub.status.idle": "2026-02-16T20:33:26.041102Z",
     "shell.execute_reply": "2026-02-16T20:33:26.040795Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"]=os.getenv(\"GROQ_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64c96f04",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T20:33:26.042355Z",
     "iopub.status.busy": "2026-02-16T20:33:26.042211Z",
     "iopub.status.idle": "2026-02-16T20:33:31.973230Z",
     "shell.execute_reply": "2026-02-16T20:33:31.972926Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "706e2335",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T20:33:31.974760Z",
     "iopub.status.busy": "2026-02-16T20:33:31.974533Z",
     "iopub.status.idle": "2026-02-16T20:33:33.155667Z",
     "shell.execute_reply": "2026-02-16T20:33:33.155400Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Document(metadata={'source': 'https://langchain-ai.github.io/langgraph/tutorials/introduction/', 'title': 'Redirecting...', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nRedirecting...\\n\\n\\n\\n\\n\\n\\nRedirecting...\\n\\n\\n')],\n",
       " [Document(metadata={'source': 'https://langchain-ai.github.io/langgraph/tutorials/workflows/', 'title': 'Redirecting...', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nRedirecting...\\n\\n\\n\\n\\n\\n\\nRedirecting...\\n\\n\\n')],\n",
       " [Document(metadata={'source': 'https://langchain-ai.github.io/langgraph/how-tos/map-reduce/', 'title': 'Redirecting...', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nRedirecting...\\n\\n\\n\\n\\n\\n\\nRedirecting...\\n\\n\\n')]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls=[\n",
    "    \"https://langchain-ai.github.io/langgraph/tutorials/introduction/\",\n",
    "    \"https://langchain-ai.github.io/langgraph/tutorials/workflows/\",\n",
    "    \"https://langchain-ai.github.io/langgraph/how-tos/map-reduce/\"\n",
    "]\n",
    "\n",
    "docs=[WebBaseLoader(url).load() for url in urls]\n",
    "docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a08aee2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T20:33:33.156727Z",
     "iopub.status.busy": "2026-02-16T20:33:33.156615Z",
     "iopub.status.idle": "2026-02-16T20:33:36.515988Z",
     "shell.execute_reply": "2026-02-16T20:33:36.515178Z"
    }
   },
   "outputs": [],
   "source": [
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=100\n",
    ")\n",
    "\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "## Add alll these text to vectordb\n",
    "\n",
    "vectorstore=FAISS.from_documents(\n",
    "    documents=doc_splits,\n",
    "    embedding=HuggingFaceEmbeddings()\n",
    ")\n",
    "\n",
    "\n",
    "retriever=vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ea4cf11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T20:33:36.517662Z",
     "iopub.status.busy": "2026-02-16T20:33:36.517533Z",
     "iopub.status.idle": "2026-02-16T20:33:36.536613Z",
     "shell.execute_reply": "2026-02-16T20:33:36.536273Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='99ad579c-5846-4ef9-9672-291a55d611f8', metadata={'source': 'https://langchain-ai.github.io/langgraph/tutorials/introduction/', 'title': 'Redirecting...', 'language': 'en'}, page_content='Redirecting...\\n\\n\\n\\n\\n\\n\\nRedirecting...'),\n",
       " Document(id='8caf839b-da4f-432a-9696-6bd1fae4f21d', metadata={'source': 'https://langchain-ai.github.io/langgraph/tutorials/workflows/', 'title': 'Redirecting...', 'language': 'en'}, page_content='Redirecting...\\n\\n\\n\\n\\n\\n\\nRedirecting...'),\n",
       " Document(id='d0c5ad4a-a27e-4db9-bb1b-b954f07f7b9c', metadata={'source': 'https://langchain-ai.github.io/langgraph/how-tos/map-reduce/', 'title': 'Redirecting...', 'language': 'en'}, page_content='Redirecting...\\n\\n\\n\\n\\n\\n\\nRedirecting...')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"what is langgraph\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0a7a99e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T20:33:36.537705Z",
     "iopub.status.busy": "2026-02-16T20:33:36.537581Z",
     "iopub.status.idle": "2026-02-16T20:33:36.548107Z",
     "shell.execute_reply": "2026-02-16T20:33:36.547763Z"
    }
   },
   "outputs": [],
   "source": [
    "### Retriever To Retriever Tools\n",
    "from langchain_core.tools import create_retriever_tool\n",
    "\n",
    "\n",
    "\n",
    "retriever_tool=create_retriever_tool(\n",
    "    retriever,\n",
    "    \"retriever_vector_db_blog\",\n",
    "    \"Search and run information about Langgraph\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abcc6a53",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T20:33:36.549348Z",
     "iopub.status.busy": "2026-02-16T20:33:36.549205Z",
     "iopub.status.idle": "2026-02-16T20:33:36.552061Z",
     "shell.execute_reply": "2026-02-16T20:33:36.551765Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructuredTool(name='retriever_vector_db_blog', description='Search and run information about Langgraph', args_schema=<class 'langchain_core.tools.retriever.RetrieverInput'>, func=<function create_retriever_tool.<locals>.func at 0x000002864C3D3CE0>, coroutine=<function create_retriever_tool.<locals>.afunc at 0x000002864C3D3A60>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever_tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ac6e08",
   "metadata": {},
   "source": [
    "### Langchain Blogs- Seperate Vector Store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c2ed2c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T20:33:36.553131Z",
     "iopub.status.busy": "2026-02-16T20:33:36.552997Z",
     "iopub.status.idle": "2026-02-16T20:33:43.604695Z",
     "shell.execute_reply": "2026-02-16T20:33:43.604374Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/', 'title': 'LangChain overview - Docs by LangChain', 'description': 'LangChain is an open source framework with a pre-built agent architecture and integrations for any model or tool — so you can build agents that adapt as fast as the ecosystem evolves', 'language': 'en'}, page_content='LangChain overview - Docs by LangChainSkip to main contentDocs by LangChain home pageOpen sourceSearch...⌘KAsk AIGitHubTry LangSmithTry LangSmithSearch...NavigationLangChain overviewDeep AgentsLangChainLangGraphIntegrationsLearnReferenceContributePythonOverviewGet startedInstallQuickstartChangelogPhilosophyCore componentsAgentsModelsMessagesToolsShort-term memoryStreamingStructured outputMiddlewareOverviewPrebuilt middlewareCustom middlewareAdvanced usageGuardrailsRuntimeContext engineeringModel Context Protocol (MCP)Human-in-the-loopMulti-agentRetrievalLong-term memoryAgent developmentLangSmith StudioTestAgent Chat UIDeploy with LangSmithDeploymentObservabilityOn this page Create an agent Core benefitsLangChain overviewCopy pageLangChain is an open source framework with a pre-built agent architecture and integrations for any model or tool — so you can build agents that adapt as fast as the ecosystem evolvesCopy pageLangChain is the easy way to start building completely custom agents and applications powered by LLMs.\\nWith under 10 lines of code, you can connect to OpenAI, Anthropic, Google, and more.\\nLangChain provides a pre-built agent architecture and model integrations to help you get started quickly and seamlessly incorporate LLMs into your agents and applications.\\nLangChain vs. LangGraph vs. Deep AgentsIf you are looking to build an agent, we recommend you start with Deep Agents which comes “batteries-included”, with modern features like automatic compression of long conversations, a virtual filesystem, and subagent-spawning for managing and isolating context.Deep Agents are implementations of LangChain agents. If you don’t need these capabilities or would like to customize your own for your agents and autonomous applications, start with LangChain.Use LangGraph, our low-level agent orchestration framework and runtime, when you have more advanced needs that require a combination of deterministic and agentic workflows and heavy customization.\\nLangChain agents are built on top of LangGraph in order to provide durable execution, streaming, human-in-the-loop, persistence, and more. You do not need to know LangGraph for basic LangChain agent usage.\\nWe recommend you use LangChain if you want to quickly build agents and autonomous applications.\\n\\u200b Create an agent\\nCopy# pip install -qU langchain \"langchain[anthropic]\"\\nfrom langchain.agents import create_agent\\n\\ndef get_weather(city: str) -> str:\\n    \"\"\"Get weather for a given city.\"\"\"\\n    return f\"It\\'s always sunny in {city}!\"\\n\\nagent = create_agent(\\n    model=\"claude-sonnet-4-5-20250929\",\\n    tools=[get_weather],\\n    system_prompt=\"You are a helpful assistant\",\\n)\\n\\n# Run the agent\\nagent.invoke(\\n    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}\\n)\\n\\nSee the Installation instructions and Quickstart guide to get started building your own agents and applications with LangChain.\\n\\u200b Core benefits\\nStandard model interfaceDifferent providers have unique APIs for interacting with models, including the format of responses. LangChain standardizes how you interact with models so that you can seamlessly swap providers and avoid lock-in.Learn moreEasy to use, highly flexible agentLangChain’s agent abstraction is designed to be easy to get started with, letting you build a simple agent in under 10 lines of code. But it also provides enough flexibility to allow you to do all the context engineering your heart desires.Learn moreBuilt on top of LangGraphLangChain’s agents are built on top of LangGraph. This allows us to take advantage of LangGraph’s durable execution, human-in-the-loop support, persistence, and more.Learn moreDebug with LangSmithGain deep visibility into complex agent behavior with visualization tools that trace execution paths, capture state transitions, and provide detailed runtime metrics.Learn more\\n\\nEdit this page on GitHub or file an issue.\\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.Was this page helpful?YesNoInstall LangChainNext⌘IDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyHomeAboutCareersBloggithubxlinkedinyoutubePowered by\\n')],\n",
       " [Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/chatbot/', 'title': 'Build a RAG agent with LangChain - Docs by LangChain', 'language': 'en'}, page_content='Build a RAG agent with LangChain - Docs by LangChainSkip to main contentDocs by LangChain home pageOpen sourceSearch...‚åòKAsk AIGitHubTry LangSmithTry LangSmithSearch...NavigationLangChainBuild a RAG agent with LangChainDeep AgentsLangChainLangGraphIntegrationsLearnReferenceContributePythonLearnTutorialsDeep AgentsLangChainSemantic searchRAG agentSQL agentVoice agentMulti-agentLangGraphConceptual overviewsLangChain vs. LangGraph vs. Deep AgentsComponent architectureMemoryContextGraph APIFunctional APIAdditional resourcesLangChain AcademyCase studiesGet helpOn this pageOverviewConceptsPreviewSetupInstallationLangSmithComponents1. IndexingLoading documentsSplitting documentsStoring documents2. Retrieval and generationRAG agentsRAG chainsNext stepsTutorialsLangChainBuild a RAG agent with LangChainCopy pageCopy page‚ÄãOverview\\nOne of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.\\nThis tutorial will show how to build a simple Q&A application over an unstructured text data source. We will demonstrate:\\n\\nA RAG agent that executes searches with a simple tool. This is a good general-purpose implementation.\\nA two-step RAG chain that uses just a single LLM call per query. This is a fast and effective method for simple queries.\\n\\n‚ÄãConcepts\\nWe will cover the following concepts:\\n\\n\\nIndexing: a pipeline for ingesting data from a source and indexing it. This usually happens in a separate process.\\n\\n\\nRetrieval and generation: the actual RAG process, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.\\n\\n\\nOnce we‚Äôve indexed our data, we will use an agent as our orchestration framework to implement the retrieval and generation steps.\\nThe indexing portion of this tutorial will largely follow the semantic search tutorial.If your data is already available for search (i.e., you have a function to execute a search), or you‚Äôre comfortable with the content from that tutorial, feel free to skip to the section on retrieval and generation\\n‚ÄãPreview\\nIn this guide we‚Äôll build an app that answers questions about the website‚Äôs content. The specific website we will use is the LLM Powered Autonomous Agents blog post by Lilian Weng, which allows us to ask questions about the contents of the post.\\nWe can create a simple indexing pipeline and RAG chain to do this in ~40 lines of code. See below for the full code snippet:\\nExpand for full code snippetCopyimport bs4\\nfrom langchain.agents import AgentState, create_agent\\nfrom langchain_community.document_loaders import WebBaseLoader\\nfrom langchain.messages import MessageLikeRepresentation\\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\n\\n# Load and chunk contents of the blog\\nloader = WebBaseLoader(\\n    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\\n    bs_kwargs=dict(\\n        parse_only=bs4.SoupStrainer(\\n            class_=(\"post-content\", \"post-title\", \"post-header\")\\n        )\\n    ),\\n)\\ndocs = loader.load()\\n\\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\\nall_splits = text_splitter.split_documents(docs)\\n\\n# Index chunks\\n_ = vector_store.add_documents(documents=all_splits)\\n\\n# Construct a tool for retrieving context\\n@tool(response_format=\"content_and_artifact\")\\ndef retrieve_context(query: str):\\n    \"\"\"Retrieve information to help answer a query.\"\"\"\\n    retrieved_docs = vector_store.similarity_search(query, k=2)\\n    serialized = \"\\\\n\\\\n\".join(\\n        (f\"Source: {doc.metadata}\\\\nContent: {doc.page_content}\")\\n        for doc in retrieved_docs\\n    )\\n    return serialized, retrieved_docs\\n\\ntools = [retrieve_context]\\n# If desired, specify custom instructions\\nprompt = (\\n    \"You have access to a tool that retrieves context from a blog post. \"\\n    \"Use the tool to help answer user queries.\"\\n)\\nagent = create_agent(model, tools, system_prompt=prompt)\\nCopyquery = \"What is task decomposition?\"\\nfor step in agent.stream(\\n    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\\n    stream_mode=\"values\",\\n):\\n    step[\"messages\"][-1].pretty_print()\\nCopy================================ Human Message =================================\\n\\nWhat is task decomposition?\\n================================== Ai Message ==================================\\nTool Calls:\\n  retrieve_context (call_xTkJr8njRY0geNz43ZvGkX0R)\\n Call ID: call_xTkJr8njRY0geNz43ZvGkX0R\\n  Args:\\n    query: task decomposition\\n================================= Tool Message =================================\\nName: retrieve_context\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Task decomposition can be done by...\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Component One: Planning...\\n================================== Ai Message ==================================\\n\\nTask decomposition refers to...\\nCheck out the LangSmith trace.\\n‚ÄãSetup\\n‚ÄãInstallation\\nThis tutorial requires these langchain dependencies:\\npipuvCopypip install langchain langchain-text-splitters langchain-community bs4\\n\\nFor more details, see our Installation guide.\\n‚ÄãLangSmith\\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with LangSmith.\\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\\nCopyexport LANGSMITH_TRACING=\"true\"\\nexport LANGSMITH_API_KEY=\"...\"\\n\\nOr, set them in Python:\\nCopyimport getpass\\nimport os\\n\\nos.environ[\"LANGSMITH_TRACING\"] = \"true\"\\nos.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\\n\\n‚ÄãComponents\\nWe will need to select three components from LangChain‚Äôs suite of integrations.\\nSelect a chat model:\\n OpenAI Anthropic Azure Google Gemini AWS Bedrock HuggingFace\\uf8ffüëâ Read the OpenAI chat model integration docsCopypip install -U \"langchain[openai]\"\\ninit_chat_modelModel ClassCopyimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\\n\\nmodel = init_chat_model(\"gpt-4.1\")\\n\\uf8ffüëâ Read the Anthropic chat model integration docsCopypip install -U \"langchain[anthropic]\"\\ninit_chat_modelModel ClassCopyimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"ANTHROPIC_API_KEY\"] = \"sk-...\"\\n\\nmodel = init_chat_model(\"claude-sonnet-4-5-20250929\")\\n\\uf8ffüëâ Read the Azure chat model integration docsCopypip install -U \"langchain[openai]\"\\ninit_chat_modelModel ClassCopyimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"AZURE_OPENAI_API_KEY\"] = \"...\"\\nos.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"...\"\\nos.environ[\"OPENAI_API_VERSION\"] = \"2025-03-01-preview\"\\n\\nmodel = init_chat_model(\\n    \"azure_openai:gpt-4.1\",\\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\\n)\\n\\uf8ffüëâ Read the Google GenAI chat model integration docsCopypip install -U \"langchain[google-genai]\"\\ninit_chat_modelModel ClassCopyimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"GOOGLE_API_KEY\"] = \"...\"\\n\\nmodel = init_chat_model(\"google_genai:gemini-2.5-flash-lite\")\\n\\uf8ffüëâ Read the AWS Bedrock chat model integration docsCopypip install -U \"langchain[aws]\"\\ninit_chat_modelModel ClassCopyfrom langchain.chat_models import init_chat_model\\n\\n# Follow the steps here to configure your credentials:\\n# https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html\\n\\nmodel = init_chat_model(\\n    \"anthropic.claude-3-5-sonnet-20240620-v1:0\",\\n    model_provider=\"bedrock_converse\",\\n)\\n\\uf8ffüëâ Read the HuggingFace chat model integration docsCopypip install -U \"langchain[huggingface]\"\\ninit_chat_modelModel ClassCopyimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"hf_...\"\\n\\nmodel = init_chat_model(\\n    \"microsoft/Phi-3-mini-4k-instruct\",\\n    model_provider=\"huggingface\",\\n    temperature=0.7,\\n    max_tokens=1024,\\n)\\n\\nSelect an embeddings model:\\n OpenAI Azure Google Gemini Google Vertex AWS HuggingFace Ollama Cohere MistralAI Nomic NVIDIA Voyage AI IBM watsonx Fake IsaacusCopypip install -U \"langchain-openai\"\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"OPENAI_API_KEY\"):\\n    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\\n\\nfrom langchain_openai import OpenAIEmbeddings\\n\\nembeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\\nCopypip install -U \"langchain-openai\"\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"AZURE_OPENAI_API_KEY\"):\\n    os.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for Azure: \")\\n\\nfrom langchain_openai import AzureOpenAIEmbeddings\\n\\nembeddings = AzureOpenAIEmbeddings(\\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\\n)\\nCopypip install -qU langchain-google-genai\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"GOOGLE_API_KEY\"):\\n    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")\\n\\nfrom langchain_google_genai import GoogleGenerativeAIEmbeddings\\n\\nembeddings = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\")\\nCopypip install -qU langchain-google-vertexai\\nCopyfrom langchain_google_vertexai import VertexAIEmbeddings\\n\\nembeddings = VertexAIEmbeddings(model=\"text-embedding-005\")\\nCopypip install -qU langchain-aws\\nCopyfrom langchain_aws import BedrockEmbeddings\\n\\nembeddings = BedrockEmbeddings(model_id=\"amazon.titan-embed-text-v2:0\")\\nCopypip install -qU langchain-huggingface\\nCopyfrom langchain_huggingface import HuggingFaceEmbeddings\\n\\nembeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\\nCopypip install -qU langchain-ollama\\nCopyfrom langchain_ollama import OllamaEmbeddings\\n\\nembeddings = OllamaEmbeddings(model=\"llama3\")\\nCopypip install -qU langchain-cohere\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"COHERE_API_KEY\"):\\n    os.environ[\"COHERE_API_KEY\"] = getpass.getpass(\"Enter API key for Cohere: \")\\n\\nfrom langchain_cohere import CohereEmbeddings\\n\\nembeddings = CohereEmbeddings(model=\"embed-english-v3.0\")\\nCopypip install -qU langchain-mistralai\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"MISTRALAI_API_KEY\"):\\n    os.environ[\"MISTRALAI_API_KEY\"] = getpass.getpass(\"Enter API key for MistralAI: \")\\n\\nfrom langchain_mistralai import MistralAIEmbeddings\\n\\nembeddings = MistralAIEmbeddings(model=\"mistral-embed\")\\nCopypip install -qU langchain-nomic\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"NOMIC_API_KEY\"):\\n    os.environ[\"NOMIC_API_KEY\"] = getpass.getpass(\"Enter API key for Nomic: \")\\n\\nfrom langchain_nomic import NomicEmbeddings\\n\\nembeddings = NomicEmbeddings(model=\"nomic-embed-text-v1.5\")\\nCopypip install -qU langchain-nvidia-ai-endpoints\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"NVIDIA_API_KEY\"):\\n    os.environ[\"NVIDIA_API_KEY\"] = getpass.getpass(\"Enter API key for NVIDIA: \")\\n\\nfrom langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\\n\\nembeddings = NVIDIAEmbeddings(model=\"NV-Embed-QA\")\\nCopypip install -qU langchain-voyageai\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"VOYAGE_API_KEY\"):\\n    os.environ[\"VOYAGE_API_KEY\"] = getpass.getpass(\"Enter API key for Voyage AI: \")\\n\\nfrom langchain-voyageai import VoyageAIEmbeddings\\n\\nembeddings = VoyageAIEmbeddings(model=\"voyage-3\")\\nCopypip install -qU langchain-ibm\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"WATSONX_APIKEY\"):\\n    os.environ[\"WATSONX_APIKEY\"] = getpass.getpass(\"Enter API key for IBM watsonx: \")\\n\\nfrom langchain_ibm import WatsonxEmbeddings\\n\\nembeddings = WatsonxEmbeddings(\\n    model_id=\"ibm/slate-125m-english-rtrvr\",\\n    url=\"https://us-south.ml.cloud.ibm.com\",\\n    project_id=\"<WATSONX PROJECT_ID>\",\\n)\\nCopypip install -qU langchain-core\\nCopyfrom langchain_core.embeddings import DeterministicFakeEmbedding\\n\\nembeddings = DeterministicFakeEmbedding(size=4096)\\nCopypip install -qU langchain-isaacus\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"ISAACUS_API_KEY\"):\\nos.environ[\"ISAACUS_API_KEY\"] = getpass.getpass(\"Enter API key for Isaacus: \")\\n\\nfrom langchain_isaacus import IsaacusEmbeddings\\n\\nembeddings = IsaacusEmbeddings(model=\"kanon-2-embedder\")\\n\\nSelect a vector store:\\n In-memory Amazon OpenSearch AstraDB Chroma FAISS Milvus MongoDB PGVector PGVectorStore Pinecone QdrantCopypip install -U \"langchain-core\"\\nCopyfrom langchain_core.vectorstores import InMemoryVectorStore\\n\\nvector_store = InMemoryVectorStore(embeddings)\\nCopypip install -qU  boto3\\nCopyfrom opensearchpy import RequestsHttpConnection\\n\\nservice = \"es\"  # must set the service as \\'es\\'\\nregion = \"us-east-2\"\\ncredentials = boto3.Session(\\n    aws_access_key_id=\"xxxxxx\", aws_secret_access_key=\"xxxxx\"\\n).get_credentials()\\nawsauth = AWS4Auth(\"xxxxx\", \"xxxxxx\", region, service, session_token=credentials.token)\\n\\nvector_store = OpenSearchVectorSearch.from_documents(\\n    docs,\\n    embeddings,\\n    opensearch_url=\"host url\",\\n    http_auth=awsauth,\\n    timeout=300,\\n    use_ssl=True,\\n    verify_certs=True,\\n    connection_class=RequestsHttpConnection,\\n    index_name=\"test-index\",\\n)\\nCopypip install -U \"langchain-astradb\"\\nCopyfrom langchain_astradb import AstraDBVectorStore\\n\\nvector_store = AstraDBVectorStore(\\n    embedding=embeddings,\\n    api_endpoint=ASTRA_DB_API_ENDPOINT,\\n    collection_name=\"astra_vector_langchain\",\\n    token=ASTRA_DB_APPLICATION_TOKEN,\\n    namespace=ASTRA_DB_NAMESPACE,\\n)\\nCopypip install -qU langchain-chroma\\nCopyfrom langchain_chroma import Chroma\\n\\nvector_store = Chroma(\\n    collection_name=\"example_collection\",\\n    embedding_function=embeddings,\\n    persist_directory=\"./chroma_langchain_db\",  # Where to save data locally, remove if not necessary\\n)\\nCopypip install -qU langchain-community faiss-cpu\\nCopyimport faiss\\nfrom langchain_community.docstore.in_memory import InMemoryDocstore\\nfrom langchain_community.vectorstores import FAISS\\n\\nembedding_dim = len(embeddings.embed_query(\"hello world\"))\\nindex = faiss.IndexFlatL2(embedding_dim)\\n\\nvector_store = FAISS(\\n    embedding_function=embeddings,\\n    index=index,\\n    docstore=InMemoryDocstore(),\\n    index_to_docstore_id={},\\n)\\nCopypip install -qU langchain-milvus\\nCopyfrom langchain_milvus import Milvus\\n\\nURI = \"./milvus_example.db\"\\n\\nvector_store = Milvus(\\n    embedding_function=embeddings,\\n    connection_args={\"uri\": URI},\\n    index_params={\"index_type\": \"FLAT\", \"metric_type\": \"L2\"},\\n)\\nCopypip install -qU langchain-mongodb\\nCopyfrom langchain_mongodb import MongoDBAtlasVectorSearch\\n\\nvector_store = MongoDBAtlasVectorSearch(\\n    embedding=embeddings,\\n    collection=MONGODB_COLLECTION,\\n    index_name=ATLAS_VECTOR_SEARCH_INDEX_NAME,\\n    relevance_score_fn=\"cosine\",\\n)\\nCopypip install -qU langchain-postgres\\nCopyfrom langchain_postgres import PGVector\\n\\nvector_store = PGVector(\\n    embeddings=embeddings,\\n    collection_name=\"my_docs\",\\n    connection=\"postgresql+psycopg://...\",\\n)\\nCopypip install -qU langchain-postgres\\nCopyfrom langchain_postgres import PGEngine, PGVectorStore\\n\\npg_engine = PGEngine.from_connection_string(\\n    url=\"postgresql+psycopg://...\"\\n)\\n\\nvector_store = PGVectorStore.create_sync(\\n    engine=pg_engine,\\n    table_name=\\'test_table\\',\\n    embedding_service=embeddings\\n)\\nCopypip install -qU langchain-pinecone\\nCopyfrom langchain_pinecone import PineconeVectorStore\\nfrom pinecone import Pinecone\\n\\npc = Pinecone(api_key=...)\\nindex = pc.Index(index_name)\\n\\nvector_store = PineconeVectorStore(embedding=embeddings, index=index)\\nCopypip install -qU langchain-qdrant\\nCopyfrom qdrant_client.models import Distance, VectorParams\\nfrom langchain_qdrant import QdrantVectorStore\\nfrom qdrant_client import QdrantClient\\n\\nclient = QdrantClient(\":memory:\")\\n\\nvector_size = len(embeddings.embed_query(\"sample text\"))\\n\\nif not client.collection_exists(\"test\"):\\n    client.create_collection(\\n        collection_name=\"test\",\\n        vectors_config=VectorParams(size=vector_size, distance=Distance.COSINE)\\n    )\\nvector_store = QdrantVectorStore(\\n    client=client,\\n    collection_name=\"test\",\\n    embedding=embeddings,\\n)\\n\\n‚Äã1. Indexing\\nThis section is an abbreviated version of the content in the semantic search tutorial.If your data is already indexed and available for search (i.e., you have a function to execute a search), or if you‚Äôre comfortable with document loaders, embeddings, and vector stores, feel free to skip to the next section on retrieval and generation.\\nIndexing commonly works as follows:\\n\\nLoad: First we need to load our data. This is done with Document Loaders.\\nSplit: Text splitters break large Documents into smaller chunks. This is useful both for indexing data and passing it into a model, as large chunks are harder to search over and won‚Äôt fit in a model‚Äôs finite context window.\\nStore: We need somewhere to store and index our splits, so that they can be searched over later. This is often done using a VectorStore and Embeddings model.\\n\\n\\n‚ÄãLoading documents\\nWe need to first load the blog post contents. We can use DocumentLoaders for this, which are objects that load in data from a source and return a list of Document objects.\\nIn this case we‚Äôll use the WebBaseLoader, which uses urllib to load HTML from web URLs and BeautifulSoup to parse it to text. We can customize the HTML -> text parsing by passing in parameters into the BeautifulSoup parser via bs_kwargs (see BeautifulSoup docs). In this case only HTML tags with class ‚Äúpost-content‚Äù, ‚Äúpost-title‚Äù, or ‚Äúpost-header‚Äù are relevant, so we‚Äôll remove all others.\\nCopyimport bs4\\nfrom langchain_community.document_loaders import WebBaseLoader\\n\\n# Only keep post title, headers, and content from the full HTML.\\nbs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))\\nloader = WebBaseLoader(\\n    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\\n    bs_kwargs={\"parse_only\": bs4_strainer},\\n)\\ndocs = loader.load()\\n\\nassert len(docs) == 1\\nprint(f\"Total characters: {len(docs[0].page_content)}\")\\n\\nCopyTotal characters: 43131\\n\\nCopyprint(docs[0].page_content[:500])\\n\\nCopy      LLM Powered Autonomous Agents\\n\\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn\\n\\nGo deeper\\nDocumentLoader: Object that loads data from a source as list of Documents.\\n\\nIntegrations: 160+ integrations to choose from.\\nBaseLoader: API reference for the base interface.\\n\\n‚ÄãSplitting documents\\nOur loaded document is over 42k characters which is too long to fit into the context window of many models. Even for those models that could fit the full post in their context window, models can struggle to find information in very long inputs.\\nTo handle this we‚Äôll split the Document into chunks for embedding and vector storage. This should help us retrieve only the most relevant parts of the blog post at run time.\\nAs in the semantic search tutorial, we use a RecursiveCharacterTextSplitter, which will recursively split the document using common separators like new lines until each chunk is the appropriate size. This is the recommended text splitter for generic text use cases.\\nCopyfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\n\\ntext_splitter = RecursiveCharacterTextSplitter(\\n    chunk_size=1000,  # chunk size (characters)\\n    chunk_overlap=200,  # chunk overlap (characters)\\n    add_start_index=True,  # track index in original document\\n)\\nall_splits = text_splitter.split_documents(docs)\\n\\nprint(f\"Split blog post into {len(all_splits)} sub-documents.\")\\n\\nCopySplit blog post into 66 sub-documents.\\n\\nGo deeper\\nTextSplitter: Object that splits a list of Document objects into smaller\\nchunks for storage and retrieval.\\n\\nIntegrations\\nInterface: API reference for the base interface.\\n\\n‚ÄãStoring documents\\nNow we need to index our 66 text chunks so that we can search over them at runtime. Following the semantic search tutorial, our approach is to embed the contents of each document split and insert these embeddings into a vector store. Given an input query, we can then use vector search to retrieve relevant documents.\\nWe can embed and store all of our document splits in a single command using the vector store and embeddings model selected at the start of the tutorial.\\nCopydocument_ids = vector_store.add_documents(documents=all_splits)\\n\\nprint(document_ids[:3])\\n\\nCopy[\\'07c18af6-ad58-479a-bfb1-d508033f9c64\\', \\'9000bf8e-1993-446f-8d4d-f4e507ba4b8f\\', \\'ba3b5d14-bed9-4f5f-88be-44c88aedc2e6\\']\\n\\nGo deeper\\nEmbeddings: Wrapper around a text embedding model, used for converting text to embeddings.\\n\\nIntegrations: 30+ integrations to choose from.\\nInterface: API reference for the base interface.\\n\\nVectorStore: Wrapper around a vector database, used for storing and querying embeddings.\\n\\nIntegrations: 40+ integrations to choose from.\\nInterface: API reference for the base interface.\\n\\nThis completes the Indexing portion of the pipeline. At this point we have a query-able vector store containing the chunked contents of our blog post. Given a user question, we should ideally be able to return the snippets of the blog post that answer the question.\\n‚Äã2. Retrieval and generation\\nRAG applications commonly work as follows:\\n\\nRetrieve: Given a user input, relevant splits are retrieved from storage using a Retriever.\\nGenerate: A model produces an answer using a prompt that includes both the question with the retrieved data\\n\\n\\nNow let‚Äôs write the actual application logic. We want to create a simple application that takes a user question, searches for documents relevant to that question, passes the retrieved documents and initial question to a model, and returns an answer.\\nWe will demonstrate:\\n\\nA RAG agent that executes searches with a simple tool. This is a good general-purpose implementation.\\nA two-step RAG chain that uses just a single LLM call per query. This is a fast and effective method for simple queries.\\n\\n‚ÄãRAG agents\\nOne formulation of a RAG application is as a simple agent with a tool that retrieves information. We can assemble a minimal RAG agent by implementing a tool that wraps our vector store:\\nCopyfrom langchain.tools import tool\\n\\n@tool(response_format=\"content_and_artifact\")\\ndef retrieve_context(query: str):\\n    \"\"\"Retrieve information to help answer a query.\"\"\"\\n    retrieved_docs = vector_store.similarity_search(query, k=2)\\n    serialized = \"\\\\n\\\\n\".join(\\n        (f\"Source: {doc.metadata}\\\\nContent: {doc.page_content}\")\\n        for doc in retrieved_docs\\n    )\\n    return serialized, retrieved_docs\\n\\nHere we use the tool decorator to configure the tool to attach raw documents as artifacts to each ToolMessage. This will let us access document metadata in our application, separate from the stringified representation that is sent to the model.\\nRetrieval tools are not limited to a single string query argument, as in the above example. You can\\nforce the LLM to specify additional search parameters by adding arguments‚Äî for example, a category:Copyfrom typing import Literal\\n\\ndef retrieve_context(query: str, section: Literal[\"beginning\", \"middle\", \"end\"]):\\n\\nGiven our tool, we can construct the agent:\\nCopyfrom langchain.agents import create_agent\\n\\n\\ntools = [retrieve_context]\\n# If desired, specify custom instructions\\nprompt = (\\n    \"You have access to a tool that retrieves context from a blog post. \"\\n    \"Use the tool to help answer user queries.\"\\n)\\nagent = create_agent(model, tools, system_prompt=prompt)\\n\\nLet‚Äôs test this out. We construct a question that would typically require an iterative sequence of retrieval steps to answer:\\nCopyquery = (\\n    \"What is the standard method for Task Decomposition?\\\\n\\\\n\"\\n    \"Once you get the answer, look up common extensions of that method.\"\\n)\\n\\nfor event in agent.stream(\\n    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\\n    stream_mode=\"values\",\\n):\\n    event[\"messages\"][-1].pretty_print()\\n\\nCopy================================ Human Message =================================\\n\\nWhat is the standard method for Task Decomposition?\\n\\nOnce you get the answer, look up common extensions of that method.\\n================================== Ai Message ==================================\\nTool Calls:\\n  retrieve_context (call_d6AVxICMPQYwAKj9lgH4E337)\\n Call ID: call_d6AVxICMPQYwAKj9lgH4E337\\n  Args:\\n    query: standard method for Task Decomposition\\n================================= Tool Message =================================\\nName: retrieve_context\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Task decomposition can be done...\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Component One: Planning...\\n================================== Ai Message ==================================\\nTool Calls:\\n  retrieve_context (call_0dbMOw7266jvETbXWn4JqWpR)\\n Call ID: call_0dbMOw7266jvETbXWn4JqWpR\\n  Args:\\n    query: common extensions of the standard method for Task Decomposition\\n================================= Tool Message =================================\\nName: retrieve_context\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Task decomposition can be done...\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Component One: Planning...\\n================================== Ai Message ==================================\\n\\nThe standard method for Task Decomposition often used is the Chain of Thought (CoT)...\\n\\nNote that the agent:\\n\\nGenerates a query to search for a standard method for task decomposition;\\nReceiving the answer, generates a second query to search for common extensions of it;\\nHaving received all necessary context, answers the question.\\n\\nWe can see the full sequence of steps, along with latency and other metadata, in the LangSmith trace.\\nYou can add a deeper level of control and customization using the LangGraph framework directly‚Äî for example, you can add steps to grade document relevance and rewrite search queries. Check out LangGraph‚Äôs Agentic RAG tutorial for more advanced formulations.\\n‚ÄãRAG chains\\nIn the above agentic RAG formulation we allow the LLM to use its discretion in generating a tool call to help answer user queries. This is a good general-purpose solution, but comes with some trade-offs:\\n‚úÖ Benefits‚ö†Ô∏è DrawbacksSearch only when needed ‚Äì The LLM can handle greetings, follow-ups, and simple queries without triggering unnecessary searches.Two inference calls ‚Äì When a search is performed, it requires one call to generate the query and another to produce the final response.Contextual search queries ‚Äì By treating search as a tool with a query input, the LLM crafts its own queries that incorporate conversational context.Reduced control ‚Äì The LLM may skip searches when they are actually needed, or issue extra searches when unnecessary.Multiple searches allowed ‚Äì The LLM can execute several searches in support of a single user query.\\nAnother common approach is a two-step chain, in which we always run a search (potentially using the raw user query) and incorporate the result as context for a single LLM query. This results in a single inference call per query, buying reduced latency at the expense of flexibility.\\nIn this approach we no longer call the model in a loop, but instead make a single pass.\\nWe can implement this chain by removing tools from the agent and instead incorporating the retrieval step into a custom prompt:\\nCopyfrom langchain.agents.middleware import dynamic_prompt, ModelRequest\\n\\n@dynamic_prompt\\ndef prompt_with_context(request: ModelRequest) -> str:\\n    \"\"\"Inject context into state messages.\"\"\"\\n    last_query = request.state[\"messages\"][-1].text\\n    retrieved_docs = vector_store.similarity_search(last_query)\\n\\n    docs_content = \"\\\\n\\\\n\".join(doc.page_content for doc in retrieved_docs)\\n\\n    system_message = (\\n        \"You are a helpful assistant. Use the following context in your response:\"\\n        f\"\\\\n\\\\n{docs_content}\"\\n    )\\n\\n    return system_message\\n\\n\\nagent = create_agent(model, tools=[], middleware=[prompt_with_context])\\n\\nLet‚Äôs try this out:\\nCopyquery = \"What is task decomposition?\"\\nfor step in agent.stream(\\n    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\\n    stream_mode=\"values\",\\n):\\n    step[\"messages\"][-1].pretty_print()\\n\\nCopy================================ Human Message =================================\\n\\nWhat is task decomposition?\\n================================== Ai Message ==================================\\n\\nTask decomposition is...\\n\\nIn the LangSmith trace we can see the retrieved context incorporated into the model prompt.\\nThis is a fast and effective method for simple queries in constrained settings, when we typically do want to run user queries through semantic search to pull additional context.\\nReturning source documentsThe above RAG chain incorporates retrieved context into a single system message for that run.As in the agentic RAG formulation, we sometimes want to include raw source documents in the application state to have access to document metadata. We can do this for the two-step chain case by:\\nAdding a key to the state to store the retrieved documents\\nAdding a new node via a pre-model hook to populate that key (as well as inject the context).\\nCopyfrom typing import Any\\nfrom langchain_core.documents import Document\\nfrom langchain.agents.middleware import AgentMiddleware, AgentState\\n\\n\\nclass State(AgentState):\\n    context: list[Document]\\n\\n\\nclass RetrieveDocumentsMiddleware(AgentMiddleware[State]):\\n    state_schema = State\\n\\n    def before_model(self, state: AgentState) -> dict[str, Any] | None:\\n        last_message = state[\"messages\"][-1]\\n        retrieved_docs = vector_store.similarity_search(last_message.text)\\n\\n        docs_content = \"\\\\n\\\\n\".join(doc.page_content for doc in retrieved_docs)\\n\\n        augmented_message_content = (\\n            f\"{last_message.text}\\\\n\\\\n\"\\n            \"Use the following context to answer the query:\\\\n\"\\n            f\"{docs_content}\"\\n        )\\n        return {\\n            \"messages\": [last_message.model_copy(update={\"content\": augmented_message_content})],\\n            \"context\": retrieved_docs,\\n        }\\n\\n\\nagent = create_agent(\\n    model,\\n    tools=[],\\n    middleware=[RetrieveDocumentsMiddleware()],\\n)\\n\\n‚ÄãNext steps\\nNow that we‚Äôve implemented a simple RAG application via create_agent, we can easily incorporate new features and go deeper:\\n\\nStream tokens and other information for responsive user experiences\\nAdd conversational memory to support multi-turn interactions\\nAdd long-term memory to support memory across conversational threads\\nAdd structured responses\\nDeploy your application with LangSmith Deployment\\n\\n\\nEdit this page on GitHub or file an issue.\\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.Was this page helpful?YesNoBuild a semantic search engine with LangChainPreviousBuild a SQL agentNext‚åòIDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyHomeAboutCareersBloggithubxlinkedinyoutubePowered by\\n')],\n",
       " [Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/qa_chat_history/', 'title': 'Build a RAG agent with LangChain - Docs by LangChain', 'language': 'en'}, page_content='Build a RAG agent with LangChain - Docs by LangChainSkip to main contentDocs by LangChain home pageOpen sourceSearch...‚åòKAsk AIGitHubTry LangSmithTry LangSmithSearch...NavigationLangChainBuild a RAG agent with LangChainDeep AgentsLangChainLangGraphIntegrationsLearnReferenceContributePythonLearnTutorialsDeep AgentsLangChainSemantic searchRAG agentSQL agentVoice agentMulti-agentLangGraphConceptual overviewsLangChain vs. LangGraph vs. Deep AgentsComponent architectureMemoryContextGraph APIFunctional APIAdditional resourcesLangChain AcademyCase studiesGet helpOn this pageOverviewConceptsPreviewSetupInstallationLangSmithComponents1. IndexingLoading documentsSplitting documentsStoring documents2. Retrieval and generationRAG agentsRAG chainsNext stepsTutorialsLangChainBuild a RAG agent with LangChainCopy pageCopy page‚ÄãOverview\\nOne of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.\\nThis tutorial will show how to build a simple Q&A application over an unstructured text data source. We will demonstrate:\\n\\nA RAG agent that executes searches with a simple tool. This is a good general-purpose implementation.\\nA two-step RAG chain that uses just a single LLM call per query. This is a fast and effective method for simple queries.\\n\\n‚ÄãConcepts\\nWe will cover the following concepts:\\n\\n\\nIndexing: a pipeline for ingesting data from a source and indexing it. This usually happens in a separate process.\\n\\n\\nRetrieval and generation: the actual RAG process, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.\\n\\n\\nOnce we‚Äôve indexed our data, we will use an agent as our orchestration framework to implement the retrieval and generation steps.\\nThe indexing portion of this tutorial will largely follow the semantic search tutorial.If your data is already available for search (i.e., you have a function to execute a search), or you‚Äôre comfortable with the content from that tutorial, feel free to skip to the section on retrieval and generation\\n‚ÄãPreview\\nIn this guide we‚Äôll build an app that answers questions about the website‚Äôs content. The specific website we will use is the LLM Powered Autonomous Agents blog post by Lilian Weng, which allows us to ask questions about the contents of the post.\\nWe can create a simple indexing pipeline and RAG chain to do this in ~40 lines of code. See below for the full code snippet:\\nExpand for full code snippetCopyimport bs4\\nfrom langchain.agents import AgentState, create_agent\\nfrom langchain_community.document_loaders import WebBaseLoader\\nfrom langchain.messages import MessageLikeRepresentation\\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\n\\n# Load and chunk contents of the blog\\nloader = WebBaseLoader(\\n    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\\n    bs_kwargs=dict(\\n        parse_only=bs4.SoupStrainer(\\n            class_=(\"post-content\", \"post-title\", \"post-header\")\\n        )\\n    ),\\n)\\ndocs = loader.load()\\n\\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\\nall_splits = text_splitter.split_documents(docs)\\n\\n# Index chunks\\n_ = vector_store.add_documents(documents=all_splits)\\n\\n# Construct a tool for retrieving context\\n@tool(response_format=\"content_and_artifact\")\\ndef retrieve_context(query: str):\\n    \"\"\"Retrieve information to help answer a query.\"\"\"\\n    retrieved_docs = vector_store.similarity_search(query, k=2)\\n    serialized = \"\\\\n\\\\n\".join(\\n        (f\"Source: {doc.metadata}\\\\nContent: {doc.page_content}\")\\n        for doc in retrieved_docs\\n    )\\n    return serialized, retrieved_docs\\n\\ntools = [retrieve_context]\\n# If desired, specify custom instructions\\nprompt = (\\n    \"You have access to a tool that retrieves context from a blog post. \"\\n    \"Use the tool to help answer user queries.\"\\n)\\nagent = create_agent(model, tools, system_prompt=prompt)\\nCopyquery = \"What is task decomposition?\"\\nfor step in agent.stream(\\n    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\\n    stream_mode=\"values\",\\n):\\n    step[\"messages\"][-1].pretty_print()\\nCopy================================ Human Message =================================\\n\\nWhat is task decomposition?\\n================================== Ai Message ==================================\\nTool Calls:\\n  retrieve_context (call_xTkJr8njRY0geNz43ZvGkX0R)\\n Call ID: call_xTkJr8njRY0geNz43ZvGkX0R\\n  Args:\\n    query: task decomposition\\n================================= Tool Message =================================\\nName: retrieve_context\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Task decomposition can be done by...\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Component One: Planning...\\n================================== Ai Message ==================================\\n\\nTask decomposition refers to...\\nCheck out the LangSmith trace.\\n‚ÄãSetup\\n‚ÄãInstallation\\nThis tutorial requires these langchain dependencies:\\npipuvCopypip install langchain langchain-text-splitters langchain-community bs4\\n\\nFor more details, see our Installation guide.\\n‚ÄãLangSmith\\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with LangSmith.\\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\\nCopyexport LANGSMITH_TRACING=\"true\"\\nexport LANGSMITH_API_KEY=\"...\"\\n\\nOr, set them in Python:\\nCopyimport getpass\\nimport os\\n\\nos.environ[\"LANGSMITH_TRACING\"] = \"true\"\\nos.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\\n\\n‚ÄãComponents\\nWe will need to select three components from LangChain‚Äôs suite of integrations.\\nSelect a chat model:\\n OpenAI Anthropic Azure Google Gemini AWS Bedrock HuggingFace\\uf8ffüëâ Read the OpenAI chat model integration docsCopypip install -U \"langchain[openai]\"\\ninit_chat_modelModel ClassCopyimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\\n\\nmodel = init_chat_model(\"gpt-4.1\")\\n\\uf8ffüëâ Read the Anthropic chat model integration docsCopypip install -U \"langchain[anthropic]\"\\ninit_chat_modelModel ClassCopyimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"ANTHROPIC_API_KEY\"] = \"sk-...\"\\n\\nmodel = init_chat_model(\"claude-sonnet-4-5-20250929\")\\n\\uf8ffüëâ Read the Azure chat model integration docsCopypip install -U \"langchain[openai]\"\\ninit_chat_modelModel ClassCopyimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"AZURE_OPENAI_API_KEY\"] = \"...\"\\nos.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"...\"\\nos.environ[\"OPENAI_API_VERSION\"] = \"2025-03-01-preview\"\\n\\nmodel = init_chat_model(\\n    \"azure_openai:gpt-4.1\",\\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\\n)\\n\\uf8ffüëâ Read the Google GenAI chat model integration docsCopypip install -U \"langchain[google-genai]\"\\ninit_chat_modelModel ClassCopyimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"GOOGLE_API_KEY\"] = \"...\"\\n\\nmodel = init_chat_model(\"google_genai:gemini-2.5-flash-lite\")\\n\\uf8ffüëâ Read the AWS Bedrock chat model integration docsCopypip install -U \"langchain[aws]\"\\ninit_chat_modelModel ClassCopyfrom langchain.chat_models import init_chat_model\\n\\n# Follow the steps here to configure your credentials:\\n# https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html\\n\\nmodel = init_chat_model(\\n    \"anthropic.claude-3-5-sonnet-20240620-v1:0\",\\n    model_provider=\"bedrock_converse\",\\n)\\n\\uf8ffüëâ Read the HuggingFace chat model integration docsCopypip install -U \"langchain[huggingface]\"\\ninit_chat_modelModel ClassCopyimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"hf_...\"\\n\\nmodel = init_chat_model(\\n    \"microsoft/Phi-3-mini-4k-instruct\",\\n    model_provider=\"huggingface\",\\n    temperature=0.7,\\n    max_tokens=1024,\\n)\\n\\nSelect an embeddings model:\\n OpenAI Azure Google Gemini Google Vertex AWS HuggingFace Ollama Cohere MistralAI Nomic NVIDIA Voyage AI IBM watsonx Fake IsaacusCopypip install -U \"langchain-openai\"\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"OPENAI_API_KEY\"):\\n    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\\n\\nfrom langchain_openai import OpenAIEmbeddings\\n\\nembeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\\nCopypip install -U \"langchain-openai\"\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"AZURE_OPENAI_API_KEY\"):\\n    os.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for Azure: \")\\n\\nfrom langchain_openai import AzureOpenAIEmbeddings\\n\\nembeddings = AzureOpenAIEmbeddings(\\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\\n)\\nCopypip install -qU langchain-google-genai\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"GOOGLE_API_KEY\"):\\n    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")\\n\\nfrom langchain_google_genai import GoogleGenerativeAIEmbeddings\\n\\nembeddings = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\")\\nCopypip install -qU langchain-google-vertexai\\nCopyfrom langchain_google_vertexai import VertexAIEmbeddings\\n\\nembeddings = VertexAIEmbeddings(model=\"text-embedding-005\")\\nCopypip install -qU langchain-aws\\nCopyfrom langchain_aws import BedrockEmbeddings\\n\\nembeddings = BedrockEmbeddings(model_id=\"amazon.titan-embed-text-v2:0\")\\nCopypip install -qU langchain-huggingface\\nCopyfrom langchain_huggingface import HuggingFaceEmbeddings\\n\\nembeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\\nCopypip install -qU langchain-ollama\\nCopyfrom langchain_ollama import OllamaEmbeddings\\n\\nembeddings = OllamaEmbeddings(model=\"llama3\")\\nCopypip install -qU langchain-cohere\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"COHERE_API_KEY\"):\\n    os.environ[\"COHERE_API_KEY\"] = getpass.getpass(\"Enter API key for Cohere: \")\\n\\nfrom langchain_cohere import CohereEmbeddings\\n\\nembeddings = CohereEmbeddings(model=\"embed-english-v3.0\")\\nCopypip install -qU langchain-mistralai\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"MISTRALAI_API_KEY\"):\\n    os.environ[\"MISTRALAI_API_KEY\"] = getpass.getpass(\"Enter API key for MistralAI: \")\\n\\nfrom langchain_mistralai import MistralAIEmbeddings\\n\\nembeddings = MistralAIEmbeddings(model=\"mistral-embed\")\\nCopypip install -qU langchain-nomic\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"NOMIC_API_KEY\"):\\n    os.environ[\"NOMIC_API_KEY\"] = getpass.getpass(\"Enter API key for Nomic: \")\\n\\nfrom langchain_nomic import NomicEmbeddings\\n\\nembeddings = NomicEmbeddings(model=\"nomic-embed-text-v1.5\")\\nCopypip install -qU langchain-nvidia-ai-endpoints\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"NVIDIA_API_KEY\"):\\n    os.environ[\"NVIDIA_API_KEY\"] = getpass.getpass(\"Enter API key for NVIDIA: \")\\n\\nfrom langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\\n\\nembeddings = NVIDIAEmbeddings(model=\"NV-Embed-QA\")\\nCopypip install -qU langchain-voyageai\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"VOYAGE_API_KEY\"):\\n    os.environ[\"VOYAGE_API_KEY\"] = getpass.getpass(\"Enter API key for Voyage AI: \")\\n\\nfrom langchain-voyageai import VoyageAIEmbeddings\\n\\nembeddings = VoyageAIEmbeddings(model=\"voyage-3\")\\nCopypip install -qU langchain-ibm\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"WATSONX_APIKEY\"):\\n    os.environ[\"WATSONX_APIKEY\"] = getpass.getpass(\"Enter API key for IBM watsonx: \")\\n\\nfrom langchain_ibm import WatsonxEmbeddings\\n\\nembeddings = WatsonxEmbeddings(\\n    model_id=\"ibm/slate-125m-english-rtrvr\",\\n    url=\"https://us-south.ml.cloud.ibm.com\",\\n    project_id=\"<WATSONX PROJECT_ID>\",\\n)\\nCopypip install -qU langchain-core\\nCopyfrom langchain_core.embeddings import DeterministicFakeEmbedding\\n\\nembeddings = DeterministicFakeEmbedding(size=4096)\\nCopypip install -qU langchain-isaacus\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"ISAACUS_API_KEY\"):\\nos.environ[\"ISAACUS_API_KEY\"] = getpass.getpass(\"Enter API key for Isaacus: \")\\n\\nfrom langchain_isaacus import IsaacusEmbeddings\\n\\nembeddings = IsaacusEmbeddings(model=\"kanon-2-embedder\")\\n\\nSelect a vector store:\\n In-memory Amazon OpenSearch AstraDB Chroma FAISS Milvus MongoDB PGVector PGVectorStore Pinecone QdrantCopypip install -U \"langchain-core\"\\nCopyfrom langchain_core.vectorstores import InMemoryVectorStore\\n\\nvector_store = InMemoryVectorStore(embeddings)\\nCopypip install -qU  boto3\\nCopyfrom opensearchpy import RequestsHttpConnection\\n\\nservice = \"es\"  # must set the service as \\'es\\'\\nregion = \"us-east-2\"\\ncredentials = boto3.Session(\\n    aws_access_key_id=\"xxxxxx\", aws_secret_access_key=\"xxxxx\"\\n).get_credentials()\\nawsauth = AWS4Auth(\"xxxxx\", \"xxxxxx\", region, service, session_token=credentials.token)\\n\\nvector_store = OpenSearchVectorSearch.from_documents(\\n    docs,\\n    embeddings,\\n    opensearch_url=\"host url\",\\n    http_auth=awsauth,\\n    timeout=300,\\n    use_ssl=True,\\n    verify_certs=True,\\n    connection_class=RequestsHttpConnection,\\n    index_name=\"test-index\",\\n)\\nCopypip install -U \"langchain-astradb\"\\nCopyfrom langchain_astradb import AstraDBVectorStore\\n\\nvector_store = AstraDBVectorStore(\\n    embedding=embeddings,\\n    api_endpoint=ASTRA_DB_API_ENDPOINT,\\n    collection_name=\"astra_vector_langchain\",\\n    token=ASTRA_DB_APPLICATION_TOKEN,\\n    namespace=ASTRA_DB_NAMESPACE,\\n)\\nCopypip install -qU langchain-chroma\\nCopyfrom langchain_chroma import Chroma\\n\\nvector_store = Chroma(\\n    collection_name=\"example_collection\",\\n    embedding_function=embeddings,\\n    persist_directory=\"./chroma_langchain_db\",  # Where to save data locally, remove if not necessary\\n)\\nCopypip install -qU langchain-community faiss-cpu\\nCopyimport faiss\\nfrom langchain_community.docstore.in_memory import InMemoryDocstore\\nfrom langchain_community.vectorstores import FAISS\\n\\nembedding_dim = len(embeddings.embed_query(\"hello world\"))\\nindex = faiss.IndexFlatL2(embedding_dim)\\n\\nvector_store = FAISS(\\n    embedding_function=embeddings,\\n    index=index,\\n    docstore=InMemoryDocstore(),\\n    index_to_docstore_id={},\\n)\\nCopypip install -qU langchain-milvus\\nCopyfrom langchain_milvus import Milvus\\n\\nURI = \"./milvus_example.db\"\\n\\nvector_store = Milvus(\\n    embedding_function=embeddings,\\n    connection_args={\"uri\": URI},\\n    index_params={\"index_type\": \"FLAT\", \"metric_type\": \"L2\"},\\n)\\nCopypip install -qU langchain-mongodb\\nCopyfrom langchain_mongodb import MongoDBAtlasVectorSearch\\n\\nvector_store = MongoDBAtlasVectorSearch(\\n    embedding=embeddings,\\n    collection=MONGODB_COLLECTION,\\n    index_name=ATLAS_VECTOR_SEARCH_INDEX_NAME,\\n    relevance_score_fn=\"cosine\",\\n)\\nCopypip install -qU langchain-postgres\\nCopyfrom langchain_postgres import PGVector\\n\\nvector_store = PGVector(\\n    embeddings=embeddings,\\n    collection_name=\"my_docs\",\\n    connection=\"postgresql+psycopg://...\",\\n)\\nCopypip install -qU langchain-postgres\\nCopyfrom langchain_postgres import PGEngine, PGVectorStore\\n\\npg_engine = PGEngine.from_connection_string(\\n    url=\"postgresql+psycopg://...\"\\n)\\n\\nvector_store = PGVectorStore.create_sync(\\n    engine=pg_engine,\\n    table_name=\\'test_table\\',\\n    embedding_service=embeddings\\n)\\nCopypip install -qU langchain-pinecone\\nCopyfrom langchain_pinecone import PineconeVectorStore\\nfrom pinecone import Pinecone\\n\\npc = Pinecone(api_key=...)\\nindex = pc.Index(index_name)\\n\\nvector_store = PineconeVectorStore(embedding=embeddings, index=index)\\nCopypip install -qU langchain-qdrant\\nCopyfrom qdrant_client.models import Distance, VectorParams\\nfrom langchain_qdrant import QdrantVectorStore\\nfrom qdrant_client import QdrantClient\\n\\nclient = QdrantClient(\":memory:\")\\n\\nvector_size = len(embeddings.embed_query(\"sample text\"))\\n\\nif not client.collection_exists(\"test\"):\\n    client.create_collection(\\n        collection_name=\"test\",\\n        vectors_config=VectorParams(size=vector_size, distance=Distance.COSINE)\\n    )\\nvector_store = QdrantVectorStore(\\n    client=client,\\n    collection_name=\"test\",\\n    embedding=embeddings,\\n)\\n\\n‚Äã1. Indexing\\nThis section is an abbreviated version of the content in the semantic search tutorial.If your data is already indexed and available for search (i.e., you have a function to execute a search), or if you‚Äôre comfortable with document loaders, embeddings, and vector stores, feel free to skip to the next section on retrieval and generation.\\nIndexing commonly works as follows:\\n\\nLoad: First we need to load our data. This is done with Document Loaders.\\nSplit: Text splitters break large Documents into smaller chunks. This is useful both for indexing data and passing it into a model, as large chunks are harder to search over and won‚Äôt fit in a model‚Äôs finite context window.\\nStore: We need somewhere to store and index our splits, so that they can be searched over later. This is often done using a VectorStore and Embeddings model.\\n\\n\\n‚ÄãLoading documents\\nWe need to first load the blog post contents. We can use DocumentLoaders for this, which are objects that load in data from a source and return a list of Document objects.\\nIn this case we‚Äôll use the WebBaseLoader, which uses urllib to load HTML from web URLs and BeautifulSoup to parse it to text. We can customize the HTML -> text parsing by passing in parameters into the BeautifulSoup parser via bs_kwargs (see BeautifulSoup docs). In this case only HTML tags with class ‚Äúpost-content‚Äù, ‚Äúpost-title‚Äù, or ‚Äúpost-header‚Äù are relevant, so we‚Äôll remove all others.\\nCopyimport bs4\\nfrom langchain_community.document_loaders import WebBaseLoader\\n\\n# Only keep post title, headers, and content from the full HTML.\\nbs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))\\nloader = WebBaseLoader(\\n    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\\n    bs_kwargs={\"parse_only\": bs4_strainer},\\n)\\ndocs = loader.load()\\n\\nassert len(docs) == 1\\nprint(f\"Total characters: {len(docs[0].page_content)}\")\\n\\nCopyTotal characters: 43131\\n\\nCopyprint(docs[0].page_content[:500])\\n\\nCopy      LLM Powered Autonomous Agents\\n\\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn\\n\\nGo deeper\\nDocumentLoader: Object that loads data from a source as list of Documents.\\n\\nIntegrations: 160+ integrations to choose from.\\nBaseLoader: API reference for the base interface.\\n\\n‚ÄãSplitting documents\\nOur loaded document is over 42k characters which is too long to fit into the context window of many models. Even for those models that could fit the full post in their context window, models can struggle to find information in very long inputs.\\nTo handle this we‚Äôll split the Document into chunks for embedding and vector storage. This should help us retrieve only the most relevant parts of the blog post at run time.\\nAs in the semantic search tutorial, we use a RecursiveCharacterTextSplitter, which will recursively split the document using common separators like new lines until each chunk is the appropriate size. This is the recommended text splitter for generic text use cases.\\nCopyfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\n\\ntext_splitter = RecursiveCharacterTextSplitter(\\n    chunk_size=1000,  # chunk size (characters)\\n    chunk_overlap=200,  # chunk overlap (characters)\\n    add_start_index=True,  # track index in original document\\n)\\nall_splits = text_splitter.split_documents(docs)\\n\\nprint(f\"Split blog post into {len(all_splits)} sub-documents.\")\\n\\nCopySplit blog post into 66 sub-documents.\\n\\nGo deeper\\nTextSplitter: Object that splits a list of Document objects into smaller\\nchunks for storage and retrieval.\\n\\nIntegrations\\nInterface: API reference for the base interface.\\n\\n‚ÄãStoring documents\\nNow we need to index our 66 text chunks so that we can search over them at runtime. Following the semantic search tutorial, our approach is to embed the contents of each document split and insert these embeddings into a vector store. Given an input query, we can then use vector search to retrieve relevant documents.\\nWe can embed and store all of our document splits in a single command using the vector store and embeddings model selected at the start of the tutorial.\\nCopydocument_ids = vector_store.add_documents(documents=all_splits)\\n\\nprint(document_ids[:3])\\n\\nCopy[\\'07c18af6-ad58-479a-bfb1-d508033f9c64\\', \\'9000bf8e-1993-446f-8d4d-f4e507ba4b8f\\', \\'ba3b5d14-bed9-4f5f-88be-44c88aedc2e6\\']\\n\\nGo deeper\\nEmbeddings: Wrapper around a text embedding model, used for converting text to embeddings.\\n\\nIntegrations: 30+ integrations to choose from.\\nInterface: API reference for the base interface.\\n\\nVectorStore: Wrapper around a vector database, used for storing and querying embeddings.\\n\\nIntegrations: 40+ integrations to choose from.\\nInterface: API reference for the base interface.\\n\\nThis completes the Indexing portion of the pipeline. At this point we have a query-able vector store containing the chunked contents of our blog post. Given a user question, we should ideally be able to return the snippets of the blog post that answer the question.\\n‚Äã2. Retrieval and generation\\nRAG applications commonly work as follows:\\n\\nRetrieve: Given a user input, relevant splits are retrieved from storage using a Retriever.\\nGenerate: A model produces an answer using a prompt that includes both the question with the retrieved data\\n\\n\\nNow let‚Äôs write the actual application logic. We want to create a simple application that takes a user question, searches for documents relevant to that question, passes the retrieved documents and initial question to a model, and returns an answer.\\nWe will demonstrate:\\n\\nA RAG agent that executes searches with a simple tool. This is a good general-purpose implementation.\\nA two-step RAG chain that uses just a single LLM call per query. This is a fast and effective method for simple queries.\\n\\n‚ÄãRAG agents\\nOne formulation of a RAG application is as a simple agent with a tool that retrieves information. We can assemble a minimal RAG agent by implementing a tool that wraps our vector store:\\nCopyfrom langchain.tools import tool\\n\\n@tool(response_format=\"content_and_artifact\")\\ndef retrieve_context(query: str):\\n    \"\"\"Retrieve information to help answer a query.\"\"\"\\n    retrieved_docs = vector_store.similarity_search(query, k=2)\\n    serialized = \"\\\\n\\\\n\".join(\\n        (f\"Source: {doc.metadata}\\\\nContent: {doc.page_content}\")\\n        for doc in retrieved_docs\\n    )\\n    return serialized, retrieved_docs\\n\\nHere we use the tool decorator to configure the tool to attach raw documents as artifacts to each ToolMessage. This will let us access document metadata in our application, separate from the stringified representation that is sent to the model.\\nRetrieval tools are not limited to a single string query argument, as in the above example. You can\\nforce the LLM to specify additional search parameters by adding arguments‚Äî for example, a category:Copyfrom typing import Literal\\n\\ndef retrieve_context(query: str, section: Literal[\"beginning\", \"middle\", \"end\"]):\\n\\nGiven our tool, we can construct the agent:\\nCopyfrom langchain.agents import create_agent\\n\\n\\ntools = [retrieve_context]\\n# If desired, specify custom instructions\\nprompt = (\\n    \"You have access to a tool that retrieves context from a blog post. \"\\n    \"Use the tool to help answer user queries.\"\\n)\\nagent = create_agent(model, tools, system_prompt=prompt)\\n\\nLet‚Äôs test this out. We construct a question that would typically require an iterative sequence of retrieval steps to answer:\\nCopyquery = (\\n    \"What is the standard method for Task Decomposition?\\\\n\\\\n\"\\n    \"Once you get the answer, look up common extensions of that method.\"\\n)\\n\\nfor event in agent.stream(\\n    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\\n    stream_mode=\"values\",\\n):\\n    event[\"messages\"][-1].pretty_print()\\n\\nCopy================================ Human Message =================================\\n\\nWhat is the standard method for Task Decomposition?\\n\\nOnce you get the answer, look up common extensions of that method.\\n================================== Ai Message ==================================\\nTool Calls:\\n  retrieve_context (call_d6AVxICMPQYwAKj9lgH4E337)\\n Call ID: call_d6AVxICMPQYwAKj9lgH4E337\\n  Args:\\n    query: standard method for Task Decomposition\\n================================= Tool Message =================================\\nName: retrieve_context\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Task decomposition can be done...\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Component One: Planning...\\n================================== Ai Message ==================================\\nTool Calls:\\n  retrieve_context (call_0dbMOw7266jvETbXWn4JqWpR)\\n Call ID: call_0dbMOw7266jvETbXWn4JqWpR\\n  Args:\\n    query: common extensions of the standard method for Task Decomposition\\n================================= Tool Message =================================\\nName: retrieve_context\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Task decomposition can be done...\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Component One: Planning...\\n================================== Ai Message ==================================\\n\\nThe standard method for Task Decomposition often used is the Chain of Thought (CoT)...\\n\\nNote that the agent:\\n\\nGenerates a query to search for a standard method for task decomposition;\\nReceiving the answer, generates a second query to search for common extensions of it;\\nHaving received all necessary context, answers the question.\\n\\nWe can see the full sequence of steps, along with latency and other metadata, in the LangSmith trace.\\nYou can add a deeper level of control and customization using the LangGraph framework directly‚Äî for example, you can add steps to grade document relevance and rewrite search queries. Check out LangGraph‚Äôs Agentic RAG tutorial for more advanced formulations.\\n‚ÄãRAG chains\\nIn the above agentic RAG formulation we allow the LLM to use its discretion in generating a tool call to help answer user queries. This is a good general-purpose solution, but comes with some trade-offs:\\n‚úÖ Benefits‚ö†Ô∏è DrawbacksSearch only when needed ‚Äì The LLM can handle greetings, follow-ups, and simple queries without triggering unnecessary searches.Two inference calls ‚Äì When a search is performed, it requires one call to generate the query and another to produce the final response.Contextual search queries ‚Äì By treating search as a tool with a query input, the LLM crafts its own queries that incorporate conversational context.Reduced control ‚Äì The LLM may skip searches when they are actually needed, or issue extra searches when unnecessary.Multiple searches allowed ‚Äì The LLM can execute several searches in support of a single user query.\\nAnother common approach is a two-step chain, in which we always run a search (potentially using the raw user query) and incorporate the result as context for a single LLM query. This results in a single inference call per query, buying reduced latency at the expense of flexibility.\\nIn this approach we no longer call the model in a loop, but instead make a single pass.\\nWe can implement this chain by removing tools from the agent and instead incorporating the retrieval step into a custom prompt:\\nCopyfrom langchain.agents.middleware import dynamic_prompt, ModelRequest\\n\\n@dynamic_prompt\\ndef prompt_with_context(request: ModelRequest) -> str:\\n    \"\"\"Inject context into state messages.\"\"\"\\n    last_query = request.state[\"messages\"][-1].text\\n    retrieved_docs = vector_store.similarity_search(last_query)\\n\\n    docs_content = \"\\\\n\\\\n\".join(doc.page_content for doc in retrieved_docs)\\n\\n    system_message = (\\n        \"You are a helpful assistant. Use the following context in your response:\"\\n        f\"\\\\n\\\\n{docs_content}\"\\n    )\\n\\n    return system_message\\n\\n\\nagent = create_agent(model, tools=[], middleware=[prompt_with_context])\\n\\nLet‚Äôs try this out:\\nCopyquery = \"What is task decomposition?\"\\nfor step in agent.stream(\\n    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\\n    stream_mode=\"values\",\\n):\\n    step[\"messages\"][-1].pretty_print()\\n\\nCopy================================ Human Message =================================\\n\\nWhat is task decomposition?\\n================================== Ai Message ==================================\\n\\nTask decomposition is...\\n\\nIn the LangSmith trace we can see the retrieved context incorporated into the model prompt.\\nThis is a fast and effective method for simple queries in constrained settings, when we typically do want to run user queries through semantic search to pull additional context.\\nReturning source documentsThe above RAG chain incorporates retrieved context into a single system message for that run.As in the agentic RAG formulation, we sometimes want to include raw source documents in the application state to have access to document metadata. We can do this for the two-step chain case by:\\nAdding a key to the state to store the retrieved documents\\nAdding a new node via a pre-model hook to populate that key (as well as inject the context).\\nCopyfrom typing import Any\\nfrom langchain_core.documents import Document\\nfrom langchain.agents.middleware import AgentMiddleware, AgentState\\n\\n\\nclass State(AgentState):\\n    context: list[Document]\\n\\n\\nclass RetrieveDocumentsMiddleware(AgentMiddleware[State]):\\n    state_schema = State\\n\\n    def before_model(self, state: AgentState) -> dict[str, Any] | None:\\n        last_message = state[\"messages\"][-1]\\n        retrieved_docs = vector_store.similarity_search(last_message.text)\\n\\n        docs_content = \"\\\\n\\\\n\".join(doc.page_content for doc in retrieved_docs)\\n\\n        augmented_message_content = (\\n            f\"{last_message.text}\\\\n\\\\n\"\\n            \"Use the following context to answer the query:\\\\n\"\\n            f\"{docs_content}\"\\n        )\\n        return {\\n            \"messages\": [last_message.model_copy(update={\"content\": augmented_message_content})],\\n            \"context\": retrieved_docs,\\n        }\\n\\n\\nagent = create_agent(\\n    model,\\n    tools=[],\\n    middleware=[RetrieveDocumentsMiddleware()],\\n)\\n\\n‚ÄãNext steps\\nNow that we‚Äôve implemented a simple RAG application via create_agent, we can easily incorporate new features and go deeper:\\n\\nStream tokens and other information for responsive user experiences\\nAdd conversational memory to support multi-turn interactions\\nAdd long-term memory to support memory across conversational threads\\nAdd structured responses\\nDeploy your application with LangSmith Deployment\\n\\n\\nEdit this page on GitHub or file an issue.\\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.Was this page helpful?YesNoBuild a semantic search engine with LangChainPreviousBuild a SQL agentNext‚åòIDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyHomeAboutCareersBloggithubxlinkedinyoutubePowered by\\n')]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "langchain_urls=[\n",
    "    \"https://python.langchain.com/docs/tutorials/\",\n",
    "    \"https://python.langchain.com/docs/tutorials/chatbot/\",\n",
    "    \"https://python.langchain.com/docs/tutorials/qa_chat_history/\"\n",
    "]\n",
    "\n",
    "docs=[WebBaseLoader(url).load() for url in langchain_urls]\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f834d0de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T20:33:43.605732Z",
     "iopub.status.busy": "2026-02-16T20:33:43.605627Z",
     "iopub.status.idle": "2026-02-16T20:34:01.095917Z",
     "shell.execute_reply": "2026-02-16T20:34:01.095430Z"
    }
   },
   "outputs": [],
   "source": [
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=100\n",
    ")\n",
    "\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "## Add alll these text to vectordb\n",
    "\n",
    "vectorstorelangchain=FAISS.from_documents(\n",
    "    documents=doc_splits,\n",
    "    embedding=HuggingFaceEmbeddings()\n",
    ")\n",
    "\n",
    "\n",
    "retrieverlangchain=vectorstorelangchain.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf70c2f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T20:34:01.097602Z",
     "iopub.status.busy": "2026-02-16T20:34:01.097477Z",
     "iopub.status.idle": "2026-02-16T20:34:01.099723Z",
     "shell.execute_reply": "2026-02-16T20:34:01.099421Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.tools import create_retriever_tool\n",
    "\n",
    "retriever_tool_langchain=create_retriever_tool(\n",
    "    retrieverlangchain,\n",
    "    \"retriever_vector_langchain_blog\",\n",
    "    \"Search and run information about Langchain\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad6b196c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T20:34:01.100727Z",
     "iopub.status.busy": "2026-02-16T20:34:01.100615Z",
     "iopub.status.idle": "2026-02-16T20:34:01.645207Z",
     "shell.execute_reply": "2026-02-16T20:34:01.644858Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bengaluru is 19.3°C'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from langchain.tools import tool\n",
    "\n",
    "@tool\n",
    "def get_weather(city: str) -> str:\n",
    "    \"\"\"Get current weather for a given city.\"\"\"\n",
    "    \n",
    "    api_key = os.getenv(\"WEATHER_API_KEY\")\n",
    "    if not api_key:\n",
    "        return \"Weather API key not set\"\n",
    "\n",
    "    url = f\"https://api.weatherapi.com/v1/current.json?key={api_key}&q={city}\"\n",
    "    resp = requests.get(url)\n",
    "    data = resp.json()\n",
    "\n",
    "    if \"error\" in data:\n",
    "        return f\"Weather API error: {data['error']['message']}\"\n",
    "\n",
    "    return f\"{data['location']['name']} is {data['current']['temp_c']}°C\"\n",
    "\n",
    "get_weather.invoke({\"city\": \"bengaluru\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "edbc7887",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T20:34:01.646281Z",
     "iopub.status.busy": "2026-02-16T20:34:01.646178Z",
     "iopub.status.idle": "2026-02-16T20:34:01.648091Z",
     "shell.execute_reply": "2026-02-16T20:34:01.647854Z"
    }
   },
   "outputs": [],
   "source": [
    "tools=[retriever_tool,retriever_tool_langchain,get_weather]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bab96e38",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T20:34:01.649079Z",
     "iopub.status.busy": "2026-02-16T20:34:01.648977Z",
     "iopub.status.idle": "2026-02-16T20:34:01.651018Z",
     "shell.execute_reply": "2026-02-16T20:34:01.650804Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StructuredTool(name='retriever_vector_db_blog', description='Search and run information about Langgraph', args_schema=<class 'langchain_core.tools.retriever.RetrieverInput'>, func=<function create_retriever_tool.<locals>.func at 0x000002864C3D3CE0>, coroutine=<function create_retriever_tool.<locals>.afunc at 0x000002864C3D3A60>),\n",
       " StructuredTool(name='retriever_vector_langchain_blog', description='Search and run information about Langchain', args_schema=<class 'langchain_core.tools.retriever.RetrieverInput'>, func=<function create_retriever_tool.<locals>.func at 0x00000286329C6A20>, coroutine=<function create_retriever_tool.<locals>.afunc at 0x00000286329C6DE0>),\n",
       " StructuredTool(name='get_weather', description='Get current weather for a given city.', args_schema=<class 'langchain_core.utils.pydantic.get_weather'>, func=<function get_weather at 0x000002864C43BE20>)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760a65ca",
   "metadata": {},
   "source": [
    "### LangGraph Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "127c309b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T20:34:01.651880Z",
     "iopub.status.busy": "2026-02-16T20:34:01.651787Z",
     "iopub.status.idle": "2026-02-16T20:34:01.654103Z",
     "shell.execute_reply": "2026-02-16T20:34:01.653816Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Annotated, Sequence\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage],add_messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b581a0a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T20:34:01.655012Z",
     "iopub.status.busy": "2026-02-16T20:34:01.654920Z",
     "iopub.status.idle": "2026-02-16T20:34:02.259172Z",
     "shell.execute_reply": "2026-02-16T20:34:02.258804Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I help you today?', additional_kwargs={'reasoning_content': 'The user says \"hi\". We need to respond politely. No special instructions. Just greet back.'}, response_metadata={'token_usage': {'completion_tokens': 39, 'prompt_tokens': 72, 'total_tokens': 111, 'completion_time': 0.082169569, 'completion_tokens_details': {'reasoning_tokens': 21}, 'prompt_time': 0.002877434, 'prompt_tokens_details': None, 'queue_time': 0.045750216, 'total_time': 0.085047003}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_626f3fc5e0', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019c6828-fab1-78c2-9f8d-e9e18cca4e1c-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 72, 'output_tokens': 39, 'total_tokens': 111, 'output_token_details': {'reasoning': 21}})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(model=\"openai/gpt-oss-120b\")\n",
    "llm.invoke(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f5176f4c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T20:34:02.260592Z",
     "iopub.status.busy": "2026-02-16T20:34:02.260380Z",
     "iopub.status.idle": "2026-02-16T20:34:02.263045Z",
     "shell.execute_reply": "2026-02-16T20:34:02.262803Z"
    }
   },
   "outputs": [],
   "source": [
    "def agent(state):\n",
    "    \"\"\"\n",
    "    Invokes the agent model to generate a response based on current state. Given the question, it will decide to retrieve using \n",
    "    the retriever tools, for weather related questions use get_weather tool.\n",
    "    \n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "\n",
    "    Returns:\n",
    "        dict: The updated state with the response added to the messages\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"---CALL AGENT---\")\n",
    "    messages = state[\"messages\"]\n",
    "    model = ChatGroq(model=\"openai/gpt-oss-120b\")\n",
    "    model = model.bind_tools(tools)\n",
    "    response = model.invoke(messages)\n",
    "    return {\n",
    "        \"messages\": messages + [response]\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "67749188",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T20:34:02.264035Z",
     "iopub.status.busy": "2026-02-16T20:34:02.263933Z",
     "iopub.status.idle": "2026-02-16T20:34:02.269479Z",
     "shell.execute_reply": "2026-02-16T20:34:02.269149Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Annotated, Literal, Sequence\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langchain_classic import hub\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ed80d86d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T20:34:02.270477Z",
     "iopub.status.busy": "2026-02-16T20:34:02.270375Z",
     "iopub.status.idle": "2026-02-16T20:34:02.273447Z",
     "shell.execute_reply": "2026-02-16T20:34:02.273167Z"
    }
   },
   "outputs": [],
   "source": [
    "### Edges\n",
    "def grade_documents(state) -> Literal[\"generate\", \"rewrite\"]:\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question.\n",
    "\n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "\n",
    "    Returns:\n",
    "        str: A decision for whether the documents are relevant or not\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK RELEVANCE---\")\n",
    "\n",
    "    # Data model\n",
    "    class grade(BaseModel):\n",
    "        \"\"\"Binary score for relevance check.\"\"\"\n",
    "\n",
    "        binary_score: str = Field(description=\"Relevance score 'yes' or 'no'\")\n",
    "\n",
    "    # LLM\n",
    "    model = ChatGroq(model=\"openai/gpt-oss-120b\")\n",
    "\n",
    "    # LLM with tool and validation\n",
    "    llm_with_tool = model.with_structured_output(grade)\n",
    "\n",
    "    # Prompt\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "        Here is the retrieved document: \\n\\n {context} \\n\\n\n",
    "        Here is the user question: {question} \\n\n",
    "        If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n",
    "        Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\",\n",
    "        input_variables=[\"context\", \"question\"],\n",
    "    )\n",
    "\n",
    "    # Chain\n",
    "    chain = prompt | llm_with_tool\n",
    "\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "\n",
    "    question = messages[0].content\n",
    "    docs = last_message.content\n",
    "\n",
    "    scored_result = chain.invoke({\"question\": question, \"context\": docs})\n",
    "\n",
    "    score = scored_result.binary_score\n",
    "\n",
    "    if score == \"yes\":\n",
    "        print(\"---DECISION: DOCS RELEVANT---\")\n",
    "        return \"generate\"\n",
    "\n",
    "    else:\n",
    "        print(\"---DECISION: DOCS NOT RELEVANT---\")\n",
    "        print(score)\n",
    "        return \"rewrite\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "884aa9a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T20:34:02.274535Z",
     "iopub.status.busy": "2026-02-16T20:34:02.274429Z",
     "iopub.status.idle": "2026-02-16T20:34:02.276697Z",
     "shell.execute_reply": "2026-02-16T20:34:02.276435Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer\n",
    "\n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "\n",
    "    Returns:\n",
    "         dict: The updated message\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE---\")\n",
    "    messages = state[\"messages\"]\n",
    "    question = messages[0].content\n",
    "    last_message = messages[-1]\n",
    "\n",
    "    docs = last_message.content\n",
    "\n",
    "    # Prompt\n",
    "    prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "    # LLM\n",
    "    llm = ChatGroq(model=\"openai/gpt-oss-120b\")\n",
    "\n",
    "    # Post-processing\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "    # Chain\n",
    "    rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "    # Run\n",
    "    response = rag_chain.invoke({\"context\": docs, \"question\": question})\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aa2a563b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T20:34:02.277640Z",
     "iopub.status.busy": "2026-02-16T20:34:02.277531Z",
     "iopub.status.idle": "2026-02-16T20:34:02.279606Z",
     "shell.execute_reply": "2026-02-16T20:34:02.279313Z"
    }
   },
   "outputs": [],
   "source": [
    "def rewrite(state):\n",
    "    \"\"\"\n",
    "    Transform the query to produce a better question.\n",
    "\n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "\n",
    "    Returns:\n",
    "        dict: The updated state with re-phrased question\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---TRANSFORM QUERY---\")\n",
    "    messages = state[\"messages\"]\n",
    "    question = messages[0].content\n",
    "\n",
    "    msg = [\n",
    "        HumanMessage(\n",
    "            content=f\"\"\" \\n \n",
    "    Look at the input and try to reason about the underlying semantic intent / meaning. \\n \n",
    "    Here is the initial question:\n",
    "    \\n ------- \\n\n",
    "    {question} \n",
    "    \\n ------- \\n\n",
    "    Formulate an improved question: \"\"\",\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Grader\n",
    "    model = ChatGroq(model=\"openai/gpt-oss-120b\")\n",
    "    response = model.invoke(msg)\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "26bbd7fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T20:34:02.280661Z",
     "iopub.status.busy": "2026-02-16T20:34:02.280559Z",
     "iopub.status.idle": "2026-02-16T20:34:02.521815Z",
     "shell.execute_reply": "2026-02-16T20:34:02.521483Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAARIAAAHICAIAAAAN8PI9AAAQAElEQVR4nOydB2AT9RfH3yXpnrTQllKgtOwhe4rsqQgiyJYtIKDIEgeIgP6R7cAFCAiCgEzZe5UpmzILpYsuaEv3SJP7v+TaUNIk7bVNcpe8DzVe7ncjudz3fu+93+/3fjKWZYEgCD7IgCAInpBsCII3JBuC4A3JhiB4Q7IhCN6QbAiCNxYom6vHXkQ/ycxMVeTKWXmWkgWWAQbXs4zqHyNRbYNrGS7wLmFBqSplGCUwEqUCF1SrNXupNpExylz25Uo8ghJACqDIO6MSWAm3MaPaCE/BKtXv1AsSGShzX348PJ9EKmEVL9dIbUEqk9raSzy8beq1cPPytwVC2DAW025zYG3s08cZOdmsTMbY2ktt7BiplJFnKxgJy+YJQ/Vl1bJhVApQf3FGyrAKbkH1wqpUpj4cC/mqAakNKOSqBe5QjES1mUZLatRyUZ8D1KfIl416SxtGKX95kVF5EkneSTlkdhIly8jTFZmZKjHhLp4+9m17V/CrZQeEILEE2ez8KTo2PNPBWVq1rlOn9ypobneRciso5e6F5ITYbAdnWa/Rlbyq2gAhMMQtm/uXU09uj3ctZ9tzlK+HjxQsi31/xIbdSfOp7NB/aiUghISIZbN/TWzEw/SO7/nUbu4Elsv6+eE5mYpxCwOAEAxilc2tsyn/HU4c840/WAEH18Y/fZIxdoE/EMJAlLLZ+cvTpGi5lWiG4/jf8SE30iYsojpHEEhAbATtSkqIyrEqzSCdB3tVre20dm4YEAJAfLK5EZQ4ZkE1sD56jvLG6PTe32OAMDcik83ar8Mr13CQWFrMrLiMnlctMiQDFECYFzHJ5t6ltOz03D4TfMGK8a7isGlJBBBmRUyyuXgowTfAkmPNxaHfZN+k+BwgzIqYZJOeLO81xgdMyOPHj3v16gX8+eyzz/bs2QPGQAJObrK9q2KBMB+ikc2xTfF29lKpaXs53r17F0pEiXcsDpVrOsWEZwJhPkQjm5jwLLcKxuqdlZqaumTJkj59+rzxxhvjx4/fvXs3rvztt9/mzZsXGxvbrFmzTZs24ZqtW7dOnjy5Q4cO3bt3//zzz6Oiorjdt2zZgmtOnTrVokWLpUuX4vbR0dELFizALcEINO/kmZutBMJ8iEY2mWm5Fas5gHFAedy6dQuVsH379vr16y9cuBDfTpgwYfjw4T4+PleuXBk6dOiNGzdQWg0bNkRh4PaJiYmzZ8/mdre1tU1PT8d958+fP2DAgHPnzuHKOXPmoJDACLh5q4YphN7MAsJMiGa8jULO+vobSzbXrl1DhbRq1QqXP/rooy5duri7u2tt06BBg23btlWpUkUmU100uVw+derU5ORkNzc3hmGysrJGjBjRvHlzLMrOzgYjY2MnjQ3PDGhoD4Q5EI1sWJZ19jDWp23UqNFff/314sWLJk2atG7duk6dOoW3kUqlaJUtW7YsODgY6xZuJdY5KBtuuV69emA6lOkpciDMhGiMNKVq9JexPu3XX389ZMiQCxcuTJs2rWvXrr/++mtubq7WNqdPn8bSunXrrl69+r///lu5cqXWBmiqgSmhvJDmQzS1jUSiCkBXMM4HdnV1HT169KhRo27evHny5Mk//vjDxcVl2LBhBbfZtWsXVkqTJk3i3mIUAcwHq2DsnWn4mtkQTW0jkYKRoq7on2CIDJ0TdFFQGOixYCjs/v37hTfz8vLSvD1x4gSYj1y5soIfDZk2G6KRjYOTNPqJUWJH6OKvWrVq1qxZWNUkJCTs378fNYP6wSIMADx//hwDYuHh4TVr1rx48SJG1dB+4+LRSEyMjo6VdnZ2KDDNxlDW5GSyCoXSsgfnCRzRyMazol1CjFEiVE5OThhZjo+PHzNmDDa/bNiw4ZNPPnn33XexqG3btqifGTNmHD58eOLEiW3atEH3BmMG2JiDMWj0cz7++ONDhw4VPiaafOj/TJ8+PTOz7GvISwcTZDbi67puSYhmmFpaEvvnt6GTlgaC1bNubpiTm3TAtMpAmAnRPLScyzEyGbN/LfXFgrQUeZch3kCYDzGlF3ztdbfrp5MMbICN9Po8dfQxuGbKwmD02Ui9YBADRzbwkbBdtWD4oSC7f3lq7yj18KEUhOZEZLkEfpsVWqOhc+chum+ppKQkfb4Ettyjp66zyMPDw97eWM3t0dHR+ooMfCRvb29sXdVZ9PP0R30nVvYNpDCaORGZbKIf5+z8OWLy8upglfy9KBIkzOCZfkCYFZEFZHwDbavUtNJMFJcOJSYnykkzQkB8cczeEyrK7JgtS6PAmkh9xl45lkQJnwSCWNML7l8T+zw6e8RXVcEKeHA1/cSW2A+XUPBdKIg4me2m7yKy0hUWn/xp10/R0RGZk0gzQkLcqdOPbIx/eCOlcg2nPhMqgsVx62TKuYPPbG0lY76xxrxwQkb0E3UocmDDwrCMFAU2ZbTp5VW1jiVEZg+vj39yL41VsHVfd2/f1xMIgWEh00JFPsg+vSsuJSGXYcDeSeLoKnN2w7ZEJif7lVR8eROb5U/iJJVJFLkvB+XLbJjc/PmbVDM3qWdoyl9mNTM95U0ppZ7dSclNKSXJm0aKgbwtJRLVrGq4u1TGKHK5aackrEIplalmjFIqufmj8IySXLlSZgusQpKTxT6PycrOUCiVrL2TTY3GLh36kWAEiuXMpsZx50Jq6O205AS5PFuJ3yw785VUFVIJKAqskEhBWXAywPxZ00A9LVqeOPKmSNPMrsbi3c+o2yJRG0pl3sYFJl9T6UG1RqlahTpRcH2gVYJiJFLVhG5cEe4jkSqVCgmeV8Iwtg5SmQ34VXdo924FIISNpcnG2Bw/fvzIkSOLFi0CwoqhmaL5YaAjGWE90B3AD5INASQbvpBsCCDZ8EUul9vYUO4La4dkww+qbQgg2fCFZEMAyYYvJBsCSDZ8Id+GADGOtzEvVNsQQLLhC8mGADLS+EKyIYBkwxeSDQEkG76gbCgkQJBs+EG1DQEkG76QbAgg2fCFZEMAyYYv2NxJsiHoDuAH1TYEkGz4QrIhgGTDF5INASQbvpBsCCDZ8IV6QBNAsuEL1TYEkGz44uHhQbIh6A7gR3Jyck5ODhDWDcmGH1jVoHsDhHVDsuEHygbdGyCsG5INP0g2BJBs+EKyIYBkwxeSDQEkG75gWyeFBAiSDT+otiGAZMMXkg0BJBu+kGwIINnwhWRDAMmGLxQSIIBkwxeqbQgg2fCFZEMAyYYvJBsCaMYBvpBsCKDahi8kGwJhWJYFoijeeuutmJgYXGAYhlujVCr9/Pz27t0LhPVBRlqxGDJkCIaeJRIJkw8ud+3aFQirhGRTLAYMGFC5cuWCa7CqwZVAWCUkm2KBVc3QoUPt7Ow0a1q3bu3j4wOEVUKyKS59+/atVKkSt4yCGTRoEBDWCsmGB8OHD3d0dMSFpk2b+vv7A2GtWGAk7eap1NiozJxMVZhYIsGQl2olIwFWia8Mq2TV6xmlksWVDKgWCqzBQBnLKvMOlbcXo4qgcTtev349IzPttdcauTi7qHfPO37+Drh33lm4U3NHUIEPKGX+qy5ktlIHF1mHPp4gBULgWJRsHl/POr4tGu9bmQ2Tk6m6PTV3Lcuo/nG39cv1uEqlB/XtzuTf0PnbvFyWqF/VK1mVLPBQ6lqaYVWSKiQb1Xo8F6cQzRkZYDVFupDaoNyYnBxl+Yp2A6ZVAkLAWI5sngRnHv4rpnk3r5pNnUHMbF8RWd5X9va4ikAIFQuRTVIcbF36eOjsQLAIdv0c6eQi7feRLxCCxEJCAgfXR3n6OoKl0HWAX3xkJhBCxUJkk5Ys961hObJxrqDqhXDnfCoQgsRCunLm5rB29gxYEBjWS3lBw0gFioXIRqFQ5sotKpKuzGVZhRIIQUIDBwiCNyQbgaJqZrIoq9OiINkIFAmQaoSLhciGUXcAsCRYvb1wCPNjIbJhWbC0rnUsWNxXshzISBMwZKUJFZINQfCGZCNgyEgTKiQbguCN5UTSVENZLAmGItDCxXIiafqGf4kVFiiDnWAhI02gYFUjodpGqFAKDqPz5MnjQUN6AU+wqlFSbSNUqLYxOg8e3gXCsrCgkABPLlw4e+Lk4Vu3r6ekJNepXf/998c2btSMK/p3745t2zampKa0atV2zKiJWFfM/vLbzp26Y9GdO7f+3LDq/v07bu7lWrd6Y8TwcU5OTrh+3vzPGIbp0rnnd4u/zszMqFu3wYRxU+rUqb9u/W8bNq7BDTp2bva/b1a0bv1GMT8egxEOKdkCAsVCfhi+3nNWVta3C2dnZ2d/Nmve/779vkoV/y9nT01MTMCie/fvrPh+Yfv2XTb+ubNDuy7zv/kcVOmgVBcq6mnkjE8nZmVnrfxp3YJ5S0NDQ6ZOG8dNQCCTye7cvXX02IHfft14cH+Qna3dwkVzcf2okRMGDRzu7e1z8viV4mtG/Y0YoPE2QsVKn2f29vZrVm2ZPu1LrGHwb8L4TzIzM28H38CiI0f2eXh44u3u5ubepk275s1aafY6duygjcwGBYMy8/cPmDF9TsijB0HnTnGlmRkZM2d85VuxEkqoc6cekZHhGRkZUGIY6lwjXCxFNvzbbTIy0n9auaT/gB5oPvV8qy2uefEiCV9DnzxC4wpvfW6zdm901uxy587N2rXroZy4tz4+FX19/dDM495WruLP5exEnJ1d8DU1NQVKDHXlFDCWEhLg2W4TFxc7ZerYJo1bzPnyf+iHoFvStXterZKWlurl9TInukYkXNH9B3dRZgUPlaQ27SDfkCOsASvtJXDq9NGcnBx0bBwcHCC/nuGws7PPLTCFekLic82yh2f5Bg0aof1W8FBuru5gDFCDEqpuBIqV9hLA6JmLiyunGeT0meOaokqVKoeE3Ne8PZfvuiCBATWOHN3f8LUmmoolLCzUz68KGAMMByjJuREoVmpXBATUSEh4joFmjINdunz+2rXLaIzFx8di0ett2oeHP9n893qWZf+7cvH27Ruavfr3H6pUKlf+sgwDcejx/77qx9FjB6IvZPhcqCs8V1DQqefPnwEvSDVCxUplg40w7w8bs2HjanRpduzY/PFHn3bt8iZKZfmK/7V7o1PfdwZg40zffl137d46duxkUE8Lha+uLq5/rNnqYO8w/sNhw0f2u3Hz6swZc2rWqG34XK1atm1Qv9GcuTO4SB0PyEYTKhaSA/qnaY9adKtQt7UblBqsf9D0ql69JvcWm3EmThqx+vfNmjWmYcO8x407urV5uzwQwsNSapuy6y+MdcIH44f88OOi2NiYu3dv//DDd/XqvRYYWANMC8Nwk+0QQoT6pGmDrZ/YDHrw0L+jxw7A5pdmTVtNmPAJY/KxLyybNxEVIUBINjro9VZf/APzQr0EBAzJRqhQLwEBY0HNndQ4SJgKC2rutLDGQRrdKWDISBMslEpAuJBsBApjgXmtLQfLkY2lpU5X5RKg+kagWI5sLO0WU1U3zIkTJ+RyeXZ2dkZGBr6m0tP7HwAAEABJREFUqZk1axYQZoWMNKHCwrat24Jjd3KyQUAVMFR1htqzZ8/58+eBMB80skq4dOrc0cbGBqsXVI5EDddZgTRjdkg2wsXTs/xHH33k6upacKW9vT0Q5sZCZGNjx9hZ1u1kYyeR2Ul69uzZp08fW1tbzXqpVLpw4cKoqCggzIelyMZWFh+ZAxaEQqGsWscZF6ZMmdKyZUtufAdq5uzZszVr1pw8efL06dNv3OA5gIcoIyxENn7VHZ4+LkV2JYFx5Uiija3Eu3JeJbNixYqAgAClUunjo8oN0q9fv927d/fu3XvlypUjR448duwYEKbFQoapIX98Fe7mbtt9TEUQP5u+fdJrTEW/Wq/YnV26dCmskDt37mzcuDE4OHjYsGGDBg0CwiRYiGwSExPxphnWYZUyh0HbxrOSg0KR+7JYFbiFl7PiYjwKvzX3qm4nZfNju5D/nlupKVNvzy2/+gp5B1H/X7UdNrew6pOpe/4zqvznXENswe01o3fYl6USqSQrnQ27m5oUkzn6qwBbZx7tt7GxsX/99df27dvff/991I+bWxmMciUMYCGyuXLlCpoxHh4eB9bFx4RmyHOUuTmvZoJl9DaI5sknf4OCd7XmZtcscxvnbVNQNqrMOap/L0+kWlRfXubVI7Gvfp78BYmUkdpIXNxlQz6uDA5QAnJzc7Hm2bRp0xtvvIHiCQwMBMI4iFs2Dx48QOf46NGjYCpOnjx54MCBJUuWgIDZu3cviqdChQpY+bRo0QKIskbcIYEzZ87s3LkTTAg2m3h5eYGwefvtt7ds2TJkyJA///xz8ODBqHMgyhRR1jaollOnTn311VdAFEVISAi6PefPn+fcHsq4WyaITzZowc+aNWvx4sXYiAEmJyMjIzMz09PTE0RFUlISigc9H1XgZNgw4VeYAkdMskEfxsHBoU2bNmZ8ZO7fv//y5cvz5s0DcbJ582bUT+PGjYcOHVq3bl0gSoRoqmyMlZ04caJt27bmNTNE4dsYAB0edHXat2//3XffjR8/Hs1dIPgjgtoGK5muXbti0wTXRk6UFVevXsWAW3h4OJptffuaO8GVqBC6bNasWRMVFfX111+DMEhLS0Pnyt3dOJNzmAOUDZptR44cGaZGMwsDYQDhyub69etogt+5c6devXogGLZu3RoRETFz5kywLNLT0/9S8+abb2LMzc/PDwj9CNS3+eijjyIjI3FBUJpBnJycRBdGKw74vdDVoe7VxURwtU1cXJyrqyv+Zq1btwbCTJw+fRqj1WiOotnWpUsXIF5FQLLJzs7++OOPsU0mICAAhEpqaqpSqbSSvpLUvVofApLNwYMHMbbbtGlTEDBr167NysqaOHEiWA3Uvbow5vdtEhMT0ZjGhZ49ewpcM6CaOd3Zw8MDrAmM+8+YMSMoKMjR0bFfv37Y1Pv48WOwbsxf28yZMwcNAKG5/oQ+qHs1mFE2GCg7duzYqFGjQFQkJydLJBIXFxewbi5cuICWG1oKKB6MWYOVYR7ZoPePNcyvv/4quob/n376CY374cOHA2HF3atN/T3RLMb4DGp1165dYuwsg8Fx8ok11KhRA12dbdu2YSXcqlWr5cuXx8fHgxVg0toGBbNgwYL169dTjjyLxHq6V5tINo8ePapevfrdu3fFfjVfvHghk8kwngaEHo4cOYLicXBwQPG0a9cOLBFTyGbnzp1Hjx5FTwbEz3fffYf679+/PxAGsezu1cb1bWJiYkDd1mEZmkHc1ABRFNgEt1wNmhjt27dfvXp1ZmYmWApGrG3wknHRfSCsG8vrXm0U2eBlysnJOXjw4JAhQ8CywJYKjGdgezkQ/NmxY8fGjRsDAwNRPI0aNQLRUvayWbRoUb9+/QICAoQZxZfL5VlZWVBSzpw5gw/L0nQ2dXJysvL0MRbQvbqMZYPev0KheO+990CoYE1YGiM7LS3NVg2UFHSNbGxswOoRdffqMpPNDz/8MGXKFLTNSnNLmYBSyqb0kGwKItLu1WVjLXz44Yd16tTBBYFrpvQolUqLmaNBCIi0e3Vpa5vDhw937949Ozvbzs4OxEApa5vk5GRsyCMjzUiIpXt1yWsbdKxbtmzp7++Py2LRTOnRzDtbkIEDB27evBmIUiOW7NUlkQ1WUE+fPsVn9vnz52vVqgUi59tvv8U6s5gbu7i4UF1hbFq3bv3zzz/Pnz//0qVLXbt23bBhA9rGICR4yyY8PBxrT7x7ypUrZ5YszGVOSEhI8Tcm38ZkCLl7NQ/fhouSoffWtm1bEC1avk2PHj24BWxOwcY4yB+AFRkZ6erqig1zkyZN0mSvxSI0HqKjo7WK0Ejr06cPmhZ4MXfv3n306FGsjStXrty0adPhw4drPVzItykZgupeXdza5ty5c1yTv6g1U5g9e/bg69SpUznNXLt2bcGCBdgGh00KX3zxBT7eVq5cyW3JFbVv337dunVaRQWPhqZ53759UV1vvfXWoUOH/vnnHyDKAkFlry5aNljJgLpxCoPrYOmgGf3666/jfY91Aj7Sxo0bd/ny5YcPH2qK0E/19PTUKtJw+/ZtNC3QHHd3d+/Zs+eKFSuaN28ORNnRrVs3/CHw4mOtjgHrXbt2gTkoQjboK69fvx4X8IOCFfDkyZOCQY6aNWuCeqpDTZHGtylYpAHldP36dbTCjxw5kpKS4uvrSxNoGoPC3asVCgWYEEOywRbcixcvWolgQO32aDVAcXnEMzIyNEVpaWncL6QpKngErKYmT5784sUL/EXRqFi8eHFCQgIQxqFq1apffvklWm54wTHsBiZEZqAMW3Dnzp0LVgMnmIIdPTlVeHh4GCgqeARs1empBuONN27cQBcW9SbeOaREAcZy8IIvW7YMTIih2iYqKgpdGrAaZDIZeib37t3TrEEbAF+rVaumKcIYGi4XLCp4BIyhhYWFgfpBiLG1d955hzLxWSSGZHPlyhVzuVwmA6uR8uXLX7169ebNm7m5ub1798Y2XHQ3U1NTcc2qVasaNWpUvXp13JIr2rlzJzotWkUaTp06hdE2tGxxGwwYYPiRJvqzSAwZaX5+fkJrnTUGgwYNwnAzPiMwRIOhZ/RGMGb422+/YZtMkyZNNAkQNUUoGK0iDVOmTMEduUmssDkYjQeM9gBhcYhygvXSUMqunFgL2dvbl6a9kpo7y5zg4GD0bbA9DUwF+Tb8oD5pBJBvwxfqk0YA+TZ8wXYbNNIsfjQeYRhDsmmmBogCWHn2DIKDfBt+ODs7U1VDkG/DD/JtCLBC38bR0bE0Q7iXLFnSvHnzDh06QEkhM88CsDrfhmEYrndMyeAC0KU5AmEBGPr50bdJTk6mWTULws3OS1g55NvwIyEhITU1FQjrxpBs0LehnoharF69uvhpbghLhdpt+FGhQgWaSo0g34YfY8aMAcLqId+GH4mJiSkpKUBYN9QnjR+bNm1yc3MbPnw4EFYM+Tb88PT0pNnhCfJt+GF50yoSJYB8G37gc+TFixdAWDfk2/Bjx44dWVlZEydOBMKKId+GH+XKlTPvHIaEECDfhh99+/YFwuoh34Yf2GiDTTdAWDfUJ40fBw8e/OOPP4Cwbsi34Qf6NtRLgCDfplj06NEjPj6eUYPRxd9++41lWWz6PHr0KBDWB/k2xWLAgAEymYybI1ozWTRVxVYL+TbFAmVTuXLlgmt8fX2px4DVYkg2+DR99913gVDneerdu3fB3B34QGnQoAEQVgnlSSsugwYNqlKlCrdcvnz5gQMHAmGtkG9TXLCq6devn6OjI6irmqZNmwJhrYi+T1rEveyMjGwo9DHRa2dB9c/QevTsC+YKfPWtakvuLQOMkmEZtn7Vbo2qP05NTe3UvN+DK6lYqvssmuMw6rcFS5lX3mJsQane8uW5XkVqI6tay9HWAQhBIeJ2m3++j3oenYO3qDxHyejc4tV7VAPe5gzo2INVSUP3kdh8CdR06wduEHYZ/+L0Hk1z3sKyKd4n0SCzlSiVrIOTdNBUfwc3IASCWNttNi+Jys1me47286xo+RmZz+6IX/9t6Mgvqzq4SYEQAKL0bTZ+GyEFpu9Hla1BM8gb/bwGfxaw7tswIISB+NptQm9npqfI3xxXCawJqRQ8vO3/XhIJhAAQX7vN7XPJDs7WOA1gtbquaYm5QAgA8bXbZKXKrTNnv4uHLFdBg20Fgfh8m6xshVxujTPMYGOAwiq/uAChXAIEwRsab0MQvBGfb4PtmwwDBGFGxOfbqPqgkIVPmBVR+jakGsK8iM+3YSRkpBFmRny+DasEmuGcMC803kY00LNCOFC7jZgg5QgE8fk2EgmTy1jj/cMAkE8nEMTn2yiVegeTEYRpIN+GB336dt6wcQ0QVo/4xtswqrrGWEbakyePBw3ppa904ID3X2vQGAirR3y+DYs2mtGM/AcP7xooHTJ4JBCEGH0biaq5k19tg8bVjh1/T5n6QcfOzVJSVYnPDx3eO3HyyJ5vtcXX7Ts2c1lj1q3/bdHieXFxsbjZP9s3hYY+woWLF4P6D+gxdtxgeNVIu3Pn1qezJvfu0/H9Ee/+8uuK9PR0XPnflYu4S3DwTc2p792/ozrIpXP6diHEiPh8G6WquZNfbWNjY7PvwK7q1WstWfyzo4PjseOHUB41a9Te/Ne/Y8dMQtms/GUZbjZq5IRBA4d7e/ucPH7lvf5DcS9cueGvNWibTZ82u+ABo55Gzvh0YlZ21sqf1i2YtzQ0NGTqtHG5ublNGjd3cXY5c/aEZsugoJO4pnmzVvp2AUKEWEUOaIZhXF3dPpo0o1nTljKZ7MCB3a+91viTKZ+VK+eBN/qoERN2796WlJRYeC98xTseJVSn9ivpe44dO2gjs8G7v0oVf3//gBnT54Q8ehB07pRUKu3YsduZs8c1W6KEOnfugev17QKECBFfLgFJifqk1aqZp39swA2+c7N5s9aaosaNm+PKW7ev69yxZo06hVfeuXOzdu16bm7u3Fsfn4q+vn7cETp06Ipm3sOQ+6AOMERFRXTu1MPwLjygwLseCqbnNgHiy5NWsiCarW1eaqicnBy5XP7H2l/wr+AGhWubvB11/R5paan3H9xFp+WVIyQm4Gujhk2xEjtz5jgagWeDTlao4FW/fkPDuxQTFljqjKeP7OxsMCGGZIO+TXBwsODSC+rMUVts7O3tHR0du3V9q127zgXX+1b0K/5BPDzLN2jQCH2hgivdXFU1CZp2aKeh9YVeEzo2Xbu8WeQuxYSG5wkHkfZJK9X9ExhYMzUttXGjvAc/Vj4xMU+9vLx5HCGgxpGj+xu+1kSSn0QnLCzUzy9vPoJOHbrt3LkFQ3DovXzx+YLi7EKIC/H5NqUf3fnBmMnnzp06cHAPPhRu374xf8Hn02ZMQOMNVE+KKgkJz4OCTkVGhhs4Qv/+Q3FfjL9lZWXhlr+v+nH02IGhTx5xpfXqvYYixHB2QEB19P6LswshLkTYbiNVjVQrDWgsrfpt061b1/v264GMoegAABAASURBVIpB4fT0tG8WLOd8ylYt2zao32jO3BnHTxw2cARXF9c/1mx1sHcY/+Gw4SP73bh5deaMOejMaDbo0L4rRgU6dexe/F0IEcEYcDN3796Nvs3s2bNBSPz5TZgyl+k/tSpYGeF3009ti5m8ojoQr4J36bJly9atWwemQny+DdY2SgUQhBkRX5801AzFYQnzIsI8aZSCgzA3IsyTZrUpOGh4p2AQn28jlTGs0hp1g88LoGGtwkB8vo0il1UqrfHuUZum5NUJAnGOt7HK+W0I4SDO8TaUhYowK+LzbRgpMCQbwqyIcH4bhZWGBAjhID7fBusaCicR5kWMedIYlto7CbMiPt+GpoUizI74fBs7O6lcapU5oKUSqQ1Vs4JAfL6Nk5vMOucZT47LksqoxUoQiM+3adnTMzPNGkcOhN5NL+dl0vwshD7ElyfNq7It3j07f4gEayLqQU5GYs57n/gCIQDEl0sAGTSjkqevzbbl4Q8upYKl8zw65+D66NM7no5bFACEMBBfnjSOXmN9DqyNvX7q+eUj8UqFUiuZOqOKtzH63pYStmD//VfeqDJRMfnvVadkNB9Ad/CPYfP7NLO6BwVIJapIgGs52YRF1YAQDCLMk5bPm6N9VP9TQGamArScHaxElfrfgrpDqGrgjuYtAwV7HhR4e/z48Ws3b8ycNv1lKd7LCqWOHTUCyFtmXg4MkqjnFuHeaj6M1vYvP4wkPTV10KCBa9etq+Dt4+AMhNAQ/9ydUnBwlkJJKNZed0KuN2hU08FNWoJ9S4yDi+v+o7vPnj1bJdAHCOHBUIJUgTNhwoTvv//e3t4eCD2YPnONCPukmZa0tDQwKx9//PHChQuBEBI0d6chLl26NGvWLDAr2AYwb948XNi2bRsQwsAq5rcpMY8ePXr99ddBGFSqVGnEiBFACAARjrcxIUOHDgXBgAKuXl2VkvPevXt16tQBwnyQb2OI8PBwQYVMvL1V0yIkJSVNmzYNCPNBvo1e0EJDx4YR3tieNm3avPPOO/hQo0lzzQX5NnqJjIzs3r07CJJ27drhr4PK+f3334EwOeTb6KVjx44gbGrVqnXmzJmgoKC2bdsCYULIt9HLrVu3TDwjZAn44IMP6tevr1AoTpw4AYSpIN9GN8nJyeh2m3j+4ZLh7u4ulUoPHz584MABIEwC+Ta6efr06ZAhQ0A8LFq0yMdH1YEtLi4OCCNDvo1u6qoBUdGkSRN8Xb58OXplPXr0AMJokG+jm4sXL2LzCIgQrHawqgTCmJBvo5uPP/7Yzc0NxMmYMWPw9ddff7158yYQRoB8Gx1ER0dPmjRJIhF3mpixY8f++OOPWVlZQJQ1NN7GwsEY+t27d2vWrOnk5AQWCo23EQRnz54NDw8HiwBj6IGBgW+++eazZ8+AKCPIt9EBetWWNJrS1dX19OnT8fHxmZmZQJQF5Ntok56ePnjwYK6vsSVRr1499NbeeecdbMkFonSIMk+aUUEfQFDDbMoQNNh+/vnn3bt3A1E6yLfR5sSJEzdu3AALRTNEdMmSJUCUFEOywQAF2sRgZZw6dcoa0sS0b9/+f//7H1gEaHxWqVIFTIihzjXly5fPyMgAK6NDhw4VKlQAS6dFixaNGzfGhSdPnlSrJu6Unw8fPrSxsQETQr6NNp06dfL09AQrgLvVtm/ffv78eRAzjx494rIsmAzybbTZsmVLREQEWA0zZ84U+68cEhJSo0YNMCHUbqNNUFBQdHQ0WBMffPABvm7atAnEibBqG+tstxk0aJCJ/UuBULt27blz54LYePbsma2trYn73dJ4G22sdlx+06ZN3d3dQd3gK6IObKa30IB8m8L8+++/Dx48AKskMDAQX3/44QeMTYFIQAtNWLKxTt/m8uXLGJMFK+aLL774448/QCSgbDi1mxLybbTp3bs3Wvlg3SxatAjULb8geMxipNF4G0Ivp0+fRotj+vTpIGCaN2/+33//gWkR69ydxuPw4cNeXl5cC7qV0759e4VC0HPZP3782PQWGpBvU5jbt29bbUigMJ06dQJ1kECYY3XQQjNxiw2H+OfuLGu6d+8uiqyCpmTcuHEDBw7EGCMIDNM3dHKQb0PwAOvhWrVqgWCYMmXKgAEDTD91F7XbaHPmzBmxd200HmgUCaoPjrlqG/JttMGWvlu3bgGhi169eglnUp3U1NSMjAyzDF8n30abN954Qy6XA6EH9HPwdefOnWYfVGKWFhsO6pOWR9euXRMSErhlhslz+dzd3WkCDJ20atUK24ULBgk6dOiAnkbfvn3BVJilWw0H+TZ5tGvXDqUiUYOy4VJyWvmMvwbw9fVds2YNqCcSxdcePXqgyYRVEJgQs3Sr4SDfJo/hw4cHBAQUXIONnhh1BUIPeH3w9dixY926dXv+/Dk+a2JiYkzZYG9GI436pOVRtWrVtm3bFpzgFn+Spk2bAmGQrVu3JiYmcstY8+zbtw9MhUBlY225BAYNGoTi4Zbd3NywQQCIoggNDdUs40Pn6tWrppmX6unTp+XLlzdXjiHybV5SsWLFjh07chVOlSpVMKQGhEEK18bx8fGmqXDM1a2Gg3ybVxg8eDAKxtHREWseIIpi1KhRDRs29PHxcXV1VarJzc09fPgwGB8zWmhguHMNyiYiIsKUdtrFA0l3LibnZCpzFUrQ87nw8xZwQHSU45fSV8YAGOhKxKiPDSWFBZYxuDurBKaoKXOkMkYmlZT3s3t3si8Imyc3Mk/vjs/MUChzWSWfLlpFXijdO5Xipyk+6uvPePra9fu4koHNBNQn7cK+pOCLL/zruNdr5SpDk1XTY51R38+aZdVl55ZfFYF6Mxajx0pWZ5FqAcPKBhpw8Z5Wgt59If+He/WCsepWnpe7a+0CBT6qgSPnI5VKn9xPu3c+KZdlR30l3Ewg4fczDq2Pq1Tdqd7r7q6utjrGF0gYUBa6tZj8K8jqWl/4VsQrBOrLq3WtCt4GOk6hLtJ1efWeKB+8/mEP0u9dTMxMz/3gW71ZFw3JxpTjbY5tehYanD74M38g1JzakhAflTpmgT8Ij+DzaUG744d+GQCWy8nNzxPj00bO9ddZKhTf5uGN1H6T/YHIp8MgT4mUObguHoTH+X3PGre38Hy/HYeUZ1m9118QfdJO70iwtZfYOgNREO+qTrFhaSAwQm9nKpVs3bYuYOlUrOYU/Vj39RdEn7QXz3OlMnHPL2sM3MrbRj4UXFfahNhshjGFd252nDxs5Pd1X39BtNvIs+Q5WblAvIpcniPPFtwgQnlOrjzbKvrFK+Ryfdef2m0Igjc03obgh9pAs/aB9DTeRtAI0IlQN1hYhW9joHmc+qQJGsqPYk4Y0Bf8EIRvw0iE+FgldMJYSRwNVA8tfY8tgfg2LGv15rJooJ9KIL4Nq2RUvfsIUcCwVhMSIN+GKCOsKSSgt9s1tdsIGKqBhQq12wgYhpwIs6IahaK7RBC+DSOxovCM2JHIGIl19B9kJUpWyL4Ny290oNUgyIeJaiyndZgg6tiHkH0blqH+GjpgaTqIotmxc0vnri3AtFCetLJk1+5tCxfNBcKE1K1T//1hY7llk11/6pNWljx4cBcI01KnTn3845bL9vozoNeLE8TcnfjhJDyt+KSkxIXffXXn7q0qlf379HkvKiribNDJP9dtx6Lc3Nw/1v5y8VJQfHxs/fqN+vYZ0KpVW26vd97tMmrkhOTkF39uWOXg4NC8WevJk2Z4epbHosTEhF9+XR5852ZWVlbz5q2HDxtbuXJVUKXPezTmg0ELv/1+6fJv3N3LrVn195Mnj//du/3a9f9iY6P9qwa8+eY7fXr3xy0/mTbu5s1ruHDkyP7ff/urZo3ad+7cwhPdv3/Hzb1c61ZvjBg+zsnJCUQOIwW+IYG5X38qlUq9vStu2bph3teL273RSeeV+Xfvjp9/WbZ/7xmZTHVbLl/xv737dq5ds7VaNVWiZyz99bcVe/ec6vded/x1zgSduHXr+p7dJ44ePYA/3PGjl8v8+rP6rWRB+DboYvKNCSxeOj8iMmzJ4l++WbD80qVz+CfJ/zF//Gnx9h2b+74zcPOmve3bdZ4779PTZ45zRTY2Nlu3bsAtd+86/ue6HbeDb6z/83dcr1Aopk4ff+Pm1amffIG/Uzl3j4mTRjyNjuJ2wdcNf60ZOOD96dNm4zL+tP/9d2HKx7O+W/gjauaHHxddvHQO13+/fBU+9rp1e+vk8Sv4m0U9jZzx6cSs7KyVP61bMG9paGjI1GnjUNIgdpQMX48Lr2Hok0f49+2C5a81aKzvyjRt2jInJyck5D63F/463t4++GTk3uITrVnTVqgoPNq+A7uqV6+1ZPHPjg6OmrMY4/qzJQgJmMy3kUj5tTsnpyRfvBg04L330a7FugLvZnzwc0XZ2dmHj+wbMnhk77f7ubm6vdmzT+dOPTZsXK3Zt1KlysOGjnZxdsEdsbZ5+PAeqKa5vREREfbF5wtatmjj4eH54YRPXN3cd+zYDJDXB7Z5s1bv9R9ap7aq4p0zZ+GSJb80ady8caNmWM/Uqlnn8n86Zl87duygjcwGf7AqVfz9/QNmTJ8T8uhB0LlTUGwYdc4iEBgs/0AFXkP8gebNXdymTTussfVdmUq+fhqdoDURHv6kW9e3bt2+zh0k+PaNJk1acEdzdXX7aNKMZk1bcvWSTkp//Q0giBzQSgW/QFpkRBi+1q/fkHvr7OzMXVBQzYV2D59YqAfNxo0aNkVDC5XGva1Zs46myMXFNT1dlWMBH2z4DEMlcOvxh8G9bt66ptmyZo2Xe+GNs3PnluEj+3Xs3Az/7j+4+yIpsfCHvHPnZu3a9dzc3Lm3Pj4VfX39NDdBcWBLl+5QUFStUk2Tr9nAlWnapGVw8E1cwLc1qtdq3Lj53TsqFT17Fh8TG4064XapVbPop3npr78BBOHb8CVNfa87Ob1MdYOPn7yitFR8/WjKGK1dkhIT3NTb6GwKwb3kcjlqoOBKfC5qlm3z545WKpWffTFFLs/5YOzkRo2aYa1V+FyaY6KitI6JHwOsEtsCk28buDKok59WLsGFmzevNmjQuG6dBrFxMagZtJ+9vLw5b1N1NFvbIs9Y+uuvmulIqrvIkGzQtwkODjZNSIBXwx43Abo8J0ezJulF3vPes7wqf9f0aV+iMVZwFy8vHwMHRIMNIwTffrOi4EqpRMc1exhyH13MpUt+aZpfv+HPU6G8V+EtPTzLN2jQCCMQBVe6ubpDsTFR/laeSBhWIi2V6WjgymAwJiUlGSsWrBaGv/8B/tC1atVFWyA4+EaTxvwaZ0p//fERySp0FwmiTxqehJe9XNFHlZ/3SdhjtFlBdeOmXbt2GQM1uOxXqQonKnQ8uI3RSsaDOzo6GjhgYGDNzMxMlBaa19ya6Jin7m7lCm+JUTh81egkLCwU/6r565jTKzCgxpGj+xu+1kQTq8At/fx45KcVZhswCxJWUSo5G7gyaBFUD6yPABAqAAAPzklEQVR5/tzpx49DcANc06B+o9u3r1+9dllLAKU5S+kRxvw2PPssop1atWo1jC1isAs18/0PCytWzEt0jfIYOWI8xgDQy0cnB2NoGE75/ofvDB8Qq44WLdosXbogLi4WhbF7zz8TPnz/0KF/C2+JEWd0Q7du25iSmoJRBLQoMFqAhgRXilXcvXvBGJtGrfbvPxQfOit/WYYR7cjI8N9X/Th67ECMJoHIKX3XBcNXBu20nbu24AORc0vq12uIYdKnTyM1jo0BTHb9hTHehn8P+U9nfIVPkfeH98WoInr5eHExbMIVDRo4fOaMrzZvWf92nw4YHfat6Dd9+uwiD4gtM+3bd5n/zefYtoM/W5cuPd99V8dcHRjq+fKLb+7eu93nnU5fzJ46dsyk3r374081YpSq6ebtt95Fa3Pmp5Meh4a4urj+sWarg73D+A+HYfwArfOZM+ZgYBSsHsNXBgMzWNVjnJp7i4YW2mwYHtA49wYw2fU3lDp99+7d6NvMnl30PVdKtv8QlRCbM+QzHqm4sU7ApwjexNzbz7/8RCaVLZi/FCyIq8efBwclT15unlld9XF+//Nrx5NHzBXWpzIGV08k3D33YuJSHd9UrONt5s3/DJsCPvxwKj6WsP346tVLWg49YSQkjITh2UtApDD6m6gEMt4G+DJ37qIlS+evXrPy2bM4bBOYO+c79DHA0hBiow2L8SXr6JjN6u8lIIh2G5Z/lYYhl2/mLwMLR4jjbdSDPKx9TCHlEhAyNHpPoAjCt2EkgnyuErrApnMr8W0MGMnCGG/DUtpW0aBybawlL4vee1IguQRoSLQuBJlLgBFo7wUjwOitb8i3ETCCzCXACrSvnBHQn35UEL6NhHwb8WBF7TYlGxRtMt9GqWTJtxELSlZpJb4NC3pTWwkkBzTlnyTEBPk2BMEbQfg2MhmDf0C8ilQqk9kI7rJIJVIBfipjYOD6C8K3cXKzS4inCda1yUpT2tgIzvt2dre1krnvcrKUMpnu6y8I36Z51/I5mSQbbeIj0t0q2IDAqNfaCT3l2CeZYOnEPE5399J9/QXh27h7g5un3c4fo4DIJzkJUl/k9p9SCYRHlRpOp7fFgUWT9gL/9F5/Q8PUUDYREREmGhcN8M/3T3Mz2W4j/Wwdwco5vychNPjFyK8CHZxBmJzZ8Tz0dka7/j4VKhedREZ0XNyb+OjWizHfBOjLkMMIqiH6n+VPE2KyGRko5Lo7PmFDm+71jKq3sIR5NWFkwV4gnDXOqjP2aX1j9WaMhFVNIZp/NB3HkQIoCh224MYSlmFffoCCB8lblrCgfNk35eUGBQ4oxd9Jwdg6SoZ96W8r7Bty35rYqJAM7lvg76VapfkiOq82k5+Mh1W3OBQefZC/V947reuj9WsWKNL6vfLeMuqzKAt1Bnr1LFqlUlsGFKyNg3TQdH8nN9CHIdmYK0/ajTMpWam5OoN4EqlEqdCxPj99pc7rl7fMqHurMEyhr5wnGwbb8fBdXFxceER4i+YttDaQSiDvzNqyyT+gRCUcTV9/1HCBZVR1/pb5n0qzY8GPZGsvCahfzqOiaHzuexfSkhOzlUot2eR9x4IX4eXPwd3Jhe87Bgr+gi8vi3pHJu+ZyL5yJPU2WrLhTsqFLVhWx0OQKdAHUqvUxk4S2KDo6y+IPGlaNGrnCubjxIlb0Q9PtXn7TSCKQZ3WaEcK1ZQ0GjR3pza5ubkGMgsTBND8NoVB2XCzDBCEPgTSJ01AUG1DFAn1SdOGZEMUCfk22pBsiCIh30YbuVxOsiEMQ76NNlTbEEVCvo02JBuiSMi30YZkQxQJ+TbaoGwMzyFFEOTbaIMhAWruJAxDvo02ZKQRRUK+jTYkG6JIyLfRhmRDFAn5NtqQb0MUCfk22lBtQxQJ+TbakGyIIiHfRhuSDVEk5NtoQ7IhioR8G21odCdRJOTbaIOykUqlQBD6Id9GGzLSiCIxZKQ9ePAgIiICrAxfX19UDhCEfgzJplatWmvXrt23bx9YDT///HNAQEDTpk2BIPRTdDLb5ORke3t7Ozs7sHQ2btyYmJg4ZcoUIAiDFD19ipub28WLFyMjI8Gi2bNnT1hYGGmGKA7FmnWoffv2P/zww4ULF8BCOXnyZFBQ0Jw5c4AgioGwZhwwC1evXl21atXvv/8OBFE8+M1xh9Z/VJRFTd4UEhKydOlS0gzBC961zaxZs8aPH4/hJhA/cXFxo0eP3r9/PxAEH6zXSMvMzOzWrdvZs2eBIHhSwomI58+fn5CQAGKmU6dOJ06cAILgTwll89VXX61YsSIlJQXESffu3bEZl7psEiXDGo209957b/HixdWqVQOCKBElrG00DB06NCcnB8TDmDFjZs+eTZohSkNpZbNp06Zly5aBSJg6derIkSMbNmwIBFEKrMhImzt3bsuWLd98k+ayJUpLaWsbjoyMjK5du4KAwSqxdu3apBmiTCgb2Tg6OmKj4bZt20CQrF692tnZefDgwUAQZUFZGmlKpTIxMbF8+fIgJLZu3RoRETFz5kwgiDKibGqbvGNJJNnZ2X369NGs6dWrF7rgYFqwTUazfPDgweDgYNIMUbaUpWyQSpUqbd68+eLFi7iM+omNjY2Pjw8JCQFTsWHDBqzxmjRpgsvnzp07dOjQggULgCDKlLLPNeHk5IQRXnS+UTD49tmzZ5cuXapRowaYhDNnzigUCqz3mjVrZmNjY8FjhAgzUsa1DceAAQM4zSB4E+NTH0xCdHR0XFwcaoZ7K5fLe/fuDQRR1pS9bNA2i4mJeXkCiSQqKso0Y6pv376t1cEUhURBZ6LMKXvZYDyNYZiCeQmxBrh8+TIYn6CgIIxJFPwkbm5uXl5eQBBlStn7Nnv37sUGnCNHjnAmE6sG7bR+/fqBMUGT7NatW5xi7e3tvb29W7dujVE16kpDlDmlare5cyE15Hrq85icXLlSkYvVCwPqF2BVryyrBJZhQX18lkVrTbWaYdRvNefPX365gHsweWvyt2QkwCoL7fLqNrib+rvgzoz6NIzmc0pkEjyshAGZncTHz75Be/cqNe2BIEpKSWSjyIHtP0clROfgnjJbqY1MautiI7PBWxtNPgXesqA+pur+VUmAUZ1D/YpFrFpJkKesvFL1B8nfS72kXgN52+UfKv8j522ptQ0nJ/albAsgxbPIcjJysjKyc7NzFXIl6rByDcfe432BIPjDWzZblkRi9WLnZONVzd2tohOIk/jHyUlRyfIcRZVaJB6CNzxkE/Uge8+ap3b2suptKoFFkJmUE3YzRiZjPviWht8QPCiubP47knTpcIJfXW93X0ewLKLvJiU+ffH+7GpuHjQ/B1EsiiWb+1fSj/8dW6+LP1goudmK+2cjR3zp70LKIYpB0bK5sC/p5tmk2h2qgqUTfCzs/c+quVUwSs8JwpIo4hZJe6G8eiLBGjSD+DXw+mtRKBBEURQhmw3fPClfxQ2sA3dvRwdn+z8XWN1MWARfDMlm3+oYRsL41PIAqyGgZcX0ZPm9S2lAEPoxJJuwe+mV6lQAK8OlgsvZPc+BIPSjVzbH/n4mtZG4+gg03Hzj9rEZc1qmpSdBWVP5Nc+crNyIB1lAEHrQK5vQW6lO7pbWRFNMbBxkZ3fGA0HoQa9ssrMUPrU8wSpx83J+8VxMqUYJE6N74MDNsykShrF1MFYLRljErSMn10RG3XV2KlenVttuHcfa26u6t527+M/R02s/HP3rhi2fx8WHVvSu3q7N4OZNenF77Tv005WbB+xsHRu/1t2rfBUwGj6B5Z6Flb35R1gMuoURFZIhtTVWe/nzhMjf138kl2dPHrdmxJBFMXEhv679UKHIxSKpzCYzM3X3/qUD3vliyfyLr9XvtG33N0kvYrHo/OUd5y9vf/etmVPGr/Ms53v05B9gPKSqQal3KZ5G6EG3bFIScxmJsaqaazcPyaQ2Iwcv8q7g7+MV8F6fL5/GPAi+d5orVSjkXTuOrVq5AcMwzRq9xbLs05iHuD7owrbX6nVGITk6umL9Uz2gGRgTRgLxkRQVIHSjWxsKhVLCgJFAC62yX10nJ3furUe5ip4efk/Cb2g2qFKpHrfg6OCKr5lZqSie54mR3l4v+yn7+dYGYyKRMNkZuUAQutDt20glDMsYSzeZWWmRT+9i+LjgypTUl6kzmEKnzspOVyoVdnYvI3u2tg5gTFi8CFKjPTkIkaNbNrZ2EmCN9ax1cfGsVrVR907jCq50cjLUhcfezkkikcrlL62m7JwMMDJObjTXGqEb3bJx97Z9FmOsCKyvd42rNw8E+DfWJDSLjQ+t4GkoMob1Tzn3imERt9u/nrfm3gPj5l5jFWylQONWaIR40e3b1GrsqpArwThgTFmpVP57cEVOTlb8s/B9h1cuWzkkJu6R4b0a1u9y++7JG7eP4fKJsxvCo4LBaGS9UNW0VeqQbAjd6JaNX007tOtT4oxiCGEobMbkzbY2Dt//NmLxjwNCw669986XRbr4XdqPatm0z+4Dy9Apwqqmd89PQJUPxyhzWj2LeGFjT6NuCL3oHaa2eVFkZpYksIUPWB/3T0VUrevYc4Q3EIQu9D5Tm3fzzErLBusjNwObXpWkGcIAerNy1mjsePIfiLz9vHID3dM8vUiOW7pyiM4iBzvnzGzdTew+FQImj1sNZcfsbzvrK8LbXyrV8QX9KzcYO/x7fXuF3Yx197IFgtCPoVwCj29nHNoQU6+Tv85SvCmTU3R3E0Zf39ZWd9pLiUTm7laWSZkTk6L1FeXIs21t7Aqvl0ltXV11PwsUmYr75yInLQsEgtCPoRzQgQ0cPbztHl98GthKR2I0fJB7lDN/Yr6y/Qwhl5/WauoCBGGQIuJFg2f45ebkxj22iu7AYVfiHJ2lXYbQDAVEERQdZh2/MOB5WHJ8aCpYNKGXYuTZ2cNnG3E8AmExFDcr588zHnv4ulasY5npOEIvx9jYKN//gjRDFAseOaB///wJI5XUfN0PLAhFDjw8H25nLxk9zx8Ionjwm3Fgy7LIhJhs53JOVZtYggPw6EJMVlpWYEPXniPInyF4wHuijtjQ7AN/xmSmKWwdZG7eLl7VRZZ8UCFXxoUkpcSn58oV7uXthn1RGQiCJyWcTS0uPOf0jviE2GylMm+aJomEwf9YZd7RCs7dBK/OgKZZA4VXvjLjk2oeKW4B8iePKrwlNwOU9um03kpAwjDY9q9UKPH72tnLKlSy6zuZprUhSghT2t6Q2XA9KDk2MjMjVaFglUp53mqJBDST3ubNO6jRg3pGQa2Vmr1YNm8lI2E4EeL2DIDy1Q7ZUim2t+YdXC3avA24qdsKnh2R2TC2dlJHF5l3FdsGba0lNy9hPBgjdSImCAum7GeKJgiLh2RDELwh2RAEb0g2BMEbkg1B8IZkQxC8+T8AAAD//5Rq3p0AAAAGSURBVAMAya6qX8H9oo4AAAAASUVORK5CYII=",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.prebuilt import tools_condition\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Define the nodes we will cycle between\n",
    "workflow.add_node(\"agent\", agent)  # agent\n",
    "retrieve = ToolNode([retriever_tool,retriever_tool_langchain,get_weather])\n",
    "workflow.add_node(\"retrieve\", retrieve)  # retrieval\n",
    "workflow.add_node(\"rewrite\", rewrite)  # Re-writing the question\n",
    "workflow.add_node(\n",
    "    \"generate\", generate\n",
    ")  # Generating a response after we know the documents are relevant\n",
    "# Call agent node to decide to retrieve or not\n",
    "workflow.add_edge(START, \"agent\")\n",
    "\n",
    "# Decide whether to retrieve\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    # Assess agent decision\n",
    "    tools_condition,\n",
    "    {\n",
    "        # Translate the condition outputs to nodes in our graph\n",
    "        \"tools\": \"retrieve\",\n",
    "        END: END,\n",
    "    },\n",
    ")\n",
    "\n",
    "# Edges taken after the `action` node is called.\n",
    "workflow.add_conditional_edges(\n",
    "    \"retrieve\",\n",
    "    # Assess agent decision\n",
    "    grade_documents,\n",
    ")\n",
    "workflow.add_edge(\"generate\", END)\n",
    "workflow.add_edge(\"rewrite\", \"agent\")\n",
    "\n",
    "# Compile\n",
    "graph = workflow.compile()\n",
    "from IPython.display import Image, display\n",
    "display(Image(graph.get_graph(xray=True).draw_mermaid_png()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "661decb5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T20:34:02.522859Z",
     "iopub.status.busy": "2026-02-16T20:34:02.522743Z",
     "iopub.status.idle": "2026-02-16T20:34:11.361774Z",
     "shell.execute_reply": "2026-02-16T20:34:11.361343Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---CALL AGENT---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---CHECK RELEVANCE---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---DECISION: DOCS NOT RELEVANT---\n",
      "no\n",
      "---TRANSFORM QUERY---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---CALL AGENT---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='What is LangGraph?', additional_kwargs={}, response_metadata={}, id='b5fe4c91-9ab7-49a1-ba22-bef27d9f4197'),\n",
       "  AIMessage(content='', additional_kwargs={'reasoning_content': 'We need to answer: \"What is LangGraph?\" Probably a brief explanation. Use knowledge. Could also fetch from web using function. Let\\'s search.', 'tool_calls': [{'id': 'fc_73ca3f9a-31d5-4b0e-b6a7-0b496377cba1', 'function': {'arguments': '{\"query\":\"LangGraph\"}', 'name': 'retriever_vector_db_blog'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 63, 'prompt_tokens': 195, 'total_tokens': 258, 'completion_time': 0.137176729, 'completion_tokens_details': {'reasoning_tokens': 31}, 'prompt_time': 0.007765875, 'prompt_tokens_details': None, 'queue_time': 0.044677705, 'total_time': 0.144942604}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_a09bde29de', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019c6828-fdc1-7dd3-9b07-eb9d45f16601-0', tool_calls=[{'name': 'retriever_vector_db_blog', 'args': {'query': 'LangGraph'}, 'id': 'fc_73ca3f9a-31d5-4b0e-b6a7-0b496377cba1', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 195, 'output_tokens': 63, 'total_tokens': 258, 'output_token_details': {'reasoning': 31}}),\n",
       "  ToolMessage(content='Redirecting...\\n\\n\\n\\n\\n\\n\\nRedirecting...\\n\\nRedirecting...\\n\\n\\n\\n\\n\\n\\nRedirecting...\\n\\nRedirecting...\\n\\n\\n\\n\\n\\n\\nRedirecting...', name='retriever_vector_db_blog', id='819fbc3c-d8ea-432f-a798-2d1998a03bd7', tool_call_id='fc_73ca3f9a-31d5-4b0e-b6a7-0b496377cba1'),\n",
       "  AIMessage(content='**Improved question**\\n\\n> Could you provide a clear explanation of what LangGraph is, including its purpose, core features, how it works, and typical use‑case scenarios?', additional_kwargs={'reasoning_content': 'The user wants to \"look at the input and try to reason about the underlying semantic intent / meaning. Here is the initial question: \\'What is LangGraph?\\' Formulate an improved question.\"\\n\\nSo they want to rewrite the question to be more precise, perhaps asking for definition, features, use cases, etc. Provide an improved version.\\n\\nWe should respond with an improved question. Possibly include context: \"Can you explain what LangGraph is, its purpose, core features, and typical use cases?\" Provide a refined question.\\n\\nWe need to comply.'}, response_metadata={'token_usage': {'completion_tokens': 156, 'prompt_tokens': 117, 'total_tokens': 273, 'completion_time': 0.337024019, 'completion_tokens_details': {'reasoning_tokens': 111}, 'prompt_time': 0.005417341, 'prompt_tokens_details': None, 'queue_time': 0.044519529, 'total_time': 0.34244136}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_626f3fc5e0', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019c6829-0251-7af3-a744-3a77e18948ad-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 117, 'output_tokens': 156, 'total_tokens': 273, 'output_token_details': {'reasoning': 111}}),\n",
       "  AIMessage(content='**LangGraph – at a glance**\\n\\nLangGraph is an open‑source Python library (maintained by the LangChain team) that lets developers **design, run, and manage complex, state‑driven workflows for LLM‑powered applications**.  \\nThink of it as a “graph engine” built on top of LangChain: each node in the graph is a callable (often a LangChain component such as a PromptTemplate, LLMChain, Tool, or custom function), and edges define how the **state** (a mutable dictionary) is passed from one node to the next. By treating an LLM workflow as a directed graph rather than a linear chain, LangGraph makes it easy to express loops, conditionals, branching, and parallel execution—patterns that are common in real‑world AI assistants, agents, and multi‑step reasoning tasks.\\n\\n---\\n\\n## 1. Why LangGraph exists (the problem it solves)\\n\\n| Traditional LangChain “Chains” | LangGraph “Graphs” |\\n|-------------------------------|--------------------|\\n| Linear sequence of calls (Chain → Chain → …) | Arbitrary directed graph with nodes that can be revisited |\\n| Hard to model loops, retries, or conditional branches | Native support for **conditionals, loops, and dynamic branching** |\\n| State is usually passed implicitly or via custom wrappers | **Explicit, version‑controlled state** (`state` dict) that every node can read/write |\\n| Scaling to multi‑agent or multi‑tool orchestration requires custom glue code | Built‑in **scheduling, concurrency, and checkpointing** utilities |\\n\\nIn short, LangGraph gives you a **declarative, visualizable, and testable way** to build the kind of multi‑step, decision‑making LLM agents that would otherwise require a lot of boilerplate.\\n\\n---\\n\\n## 2. Core concepts & components\\n\\n| Concept | What it is | Typical implementation |\\n|---------|------------|------------------------|\\n| **Node** | A unit of work that receives the current `state` dict and returns an updated dict (or a partial update). | `LLMChain`, `Tool`, custom `Callable`, `PromptTemplate`, etc. |\\n| **Edge** | Defines the flow from one node to the next. Edges can be unconditional, conditional, or based on a **routing function**. | `Edge(to=\"nodeB\")`, `ConditionalEdge(condition=lambda s: s[\"needs_review\"])` |\\n| **State** | A mutable dictionary that persists across the whole graph execution. It holds inputs, intermediate results, LLM outputs, flags, etc. | `state = {\"question\": \"...\", \"answer\": None, \"retries\": 0}` |\\n| **Graph** | The collection of nodes + edges + a **router** that decides the next node based on the current state. | `graph = Graph(start=\"router\", nodes={...}, edges=[...])` |\\n| **Router** | A function (or a small LLM chain) that examines `state` and returns the name of the next node. | `router = lambda s: \"search\" if \"search_query\" in s else \"final\"` |\\n| **Checkpointing & Persistence** | Ability to save the state at any point (e.g., to a DB, Redis, or a file) and resume later. | `graph.checkpoint(state, step_id)` |\\n| **Concurrency / Parallelism** | Run independent sub‑graphs in parallel and merge their results. | `graph.run_parallel([\"tool_a\", \"tool_b\"])` |\\n| **Visualization** | Auto‑generated DOT/GraphViz diagrams that show the workflow. | `graph.visualize()` |\\n\\n---\\n\\n## 3. How it works (execution flow)\\n\\n1. **Define the state schema** (optional but recommended).  \\n   ```python\\n   from pydantic import BaseModel\\n\\n   class MyState(BaseModel):\\n       user_query: str\\n       search_results: list | None = None\\n       answer: str | None = None\\n       retries: int = 0\\n   ```\\n\\n2. **Create nodes** – any callable that follows the signature `Callable[[dict], dict]`.  \\n   ```python\\n   def retrieve(state):\\n       # call a search tool, put results in state\\n       results = search_tool(state[\"user_query\"])\\n       return {\"search_results\": results}\\n\\n   def generate_answer(state):\\n       prompt = PromptTemplate(\\n           template=\"Using these results: {search_results}, answer the question: {user_query}\",\\n           input_variables=[\"search_results\", \"user_query\"]\\n       )\\n       answer = llm_chain.run(prompt.format(**state))\\n       return {\"answer\": answer}\\n   ```\\n\\n3. **Wire edges** – specify how the graph moves from node to node.  \\n   ```python\\n   from langgraph import Edge, ConditionalEdge\\n\\n   edges = [\\n       Edge(source=\"router\", target=\"search\"),\\n       ConditionalEdge(\\n           source=\"search\",\\n           target=\"answer\",\\n           condition=lambda s: s[\"search_results\"]  # go to answer only if we got results\\n       ),\\n       Edge(source=\"search\", target=\"router\")  # retry loop\\n   ]\\n   ```\\n\\n4. **Write a router** (often a tiny LLM chain or pure Python) that decides the next node based on the current state.  \\n   ```python\\n   def router(state):\\n       if state.get(\"answer\"):\\n           return \"final\"\\n       if state.get(\"search_results\"):\\n           return \"answer\"\\n       return \"search\"\\n   ```\\n\\n5. **Instantiate the graph**.  \\n   ```python\\n   from langgraph import Graph\\n\\n   graph = Graph(\\n       start_node=\"router\",\\n       nodes={\"router\": router, \"search\": retrieve, \"answer\": generate_answer},\\n       edges=edges,\\n   )\\n   ```\\n\\n6. **Run it** – optionally with checkpointing, streaming, or parallel execution.  \\n   ```python\\n   final_state = graph.run(initial_state={\"user_query\": \"Why is the sky blue?\"})\\n   print(final_state[\"answer\"])\\n   ```\\n\\nDuring execution, after each node finishes, LangGraph:\\n\\n* Merges the node’s returned dict into the global `state`.\\n* Calls the router (or follows a static edge) to pick the next node.\\n* Repeats until a node signals termination (e.g., returns a special `\"END\"` flag or the router points to a `final` node).\\n\\n---\\n\\n## 4. Key features that set LangGraph apart\\n\\n| Feature | What you get | Why it matters |\\n|---------|--------------|----------------|\\n| **Explicit state handling** | A single source of truth (`state` dict) that every node can read/write. | Eliminates hidden side‑effects and makes debugging/replaying easier. |\\n| **Conditional routing** | Dynamic decision‑making based on state, LLM output, or external signals. | Enables agents that can ask follow‑up questions, retry, or switch tools. |\\n| **Loops & retries** | Nodes can point back to earlier nodes; built‑in `max_steps` guard. | Natural support for “think‑step‑revise” patterns. |\\n| **Parallel sub‑graphs** | Run independent branches concurrently and merge results. | Useful for multi‑search, multi‑tool ensembles, or batch processing. |\\n| **Checkpoint & resume** | Persist state at any step (e.g., to a DB) and resume later. | Critical for long‑running agents, human‑in‑the‑loop workflows, or fault tolerance. |\\n| **Visualization** | Auto‑generated graph diagrams (DOT/GraphViz, Mermaid). | Great for documentation, team collaboration, and sanity‑checking flow. |\\n| **Integration with LangChain ecosystem** | Re‑use PromptTemplates, LLMChains, VectorStores, Retrievers, Tools, etc. | No need to reinvent the wheel; you get all LangChain’s connectors out‑of‑the‑box. |\\n| **Typed state (via Pydantic)** | Optional schema validation for the state dict. | Catches bugs early and provides IDE autocomplete. |\\n| **Streaming & async support** | Nodes can be async; the engine can stream partial outputs. | Enables real‑time UI updates and low‑latency agents. |\\n\\n---\\n\\n## 5. Typical use‑case scenarios\\n\\n| Scenario | How LangGraph helps |\\n|----------|---------------------|\\n| **Conversational AI with tool use** (e.g., a ChatGPT‑style assistant that can browse, run code, and query databases) | Model the conversation as a graph: `router → decide_tool → tool_call → evaluate → possibly loop back for clarification`. |\\n| **Multi‑step reasoning** (Chain‑of‑Thought, self‑critique, refinement) | Create a loop: `generate → critique → revise → check_done`. The loop continues until a confidence threshold is met. |\\n| **Human‑in‑the‑loop pipelines** (e.g., LLM drafts → human reviewer → finalizer) | Insert a `human_review` node that pauses execution; the state is checkpointed and resumed after the reviewer submits feedback. |\\n| **Batch data enrichment** (run several LLM‑driven transformations on a dataset) | Parallel sub‑graphs process different columns or rows; results are merged into a final enriched record. |\\n| **Dynamic tool orchestration** (search, calculator, code interpreter) | Conditional edges route to the appropriate tool based on the LLM’s “action” prediction. |\\n| **Self‑serving agents** (e.g., “plan a trip” agent that books flights, hotels, and creates an itinerary) | A graph where each planning step is a node; loops allow the agent to re‑plan if a booking fails. |\\n| **Safety & compliance checks** | Insert a `policy_check` node after any LLM output; if the check fails, route to a `fallback` or `escalate` node. |\\n| **Educational tutoring bots** | Nodes for “explain concept”, “ask question”, “evaluate answer”, with loops for repeated practice. |\\n\\n---\\n\\n## 6. Quick “Hello‑World” example (complete code)\\n\\n```python\\n# pip install langgraph langchain openai\\nfrom langgraph import Graph, Edge, ConditionalEdge\\nfrom langchain.llms import OpenAI\\nfrom langchain.prompts import PromptTemplate\\nfrom langchain.chains import LLMChain\\n\\n# ---- 1. LLM chain for answering ----\\nprompt = PromptTemplate(\\n    template=\"Answer the question using the following info:\\\\n{search_results}\\\\nQuestion: {user_query}\",\\n    input_variables=[\"search_results\", \"user_query\"]\\n)\\nanswer_chain = LLMChain(llm=OpenAI(model=\"gpt-4o-mini\"), prompt=prompt)\\n\\ndef generate_answer(state):\\n    # Run the LLM chain and put the answer into state\\n    answer = answer_chain.run(state)\\n    return {\"answer\": answer, \"finished\": True}\\n\\n# ---- 2. Dummy search node ----\\ndef dummy_search(state):\\n    # In a real app you\\'d call a vector store or web search\\n    fake_results = [\"The sky appears blue because molecules scatter short‑wavelength light more than long‑wavelength light.\"]\\n    return {\"search_results\": fake_results}\\n\\n# ---- 3. Router (pure Python) ----\\ndef router(state):\\n    if state.get(\"finished\"):\\n        return \"final\"\\n    if state.get(\"search_results\"):\\n        return \"answer\"\\n    return \"search\"\\n\\n# ---- 4. Build the graph ----\\ngraph = Graph(\\n    start_node=\"router\",\\n    nodes={\"router\": router, \"search\": dummy_search, \"answer\": generate_answer},\\n    edges=[\\n        Edge(source=\"router\", target=\"search\"),\\n        ConditionalEdge(source=\"search\", target=\"answer\",\\n                        condition=lambda s: bool(s.get(\"search_results\"))),\\n        Edge(source=\"answer\", target=\"router\")  # go back to router to see if we are done\\n    ]\\n)\\n\\n# ---- 5. Run it ----\\ninitial_state = {\"user_query\": \"Why is the sky blue?\"}\\nfinal_state = graph.run(initial_state)\\nprint(\"✅ Answer:\", final_state[\"answer\"])\\n```\\n\\n**What you see**\\n\\n* A **search** node runs first, populates `search_results`.\\n* The **router** sees that results exist and forwards to **answer**.\\n* After the LLM produces an answer, the router detects `finished=True` and jumps to the `final` node, ending execution.\\n* The whole flow is a **graph**, not a hard‑coded linear script, so you can easily add a “clarify” node, a “policy_check” node, or a retry loop without rewriting the core logic.\\n\\n---\\n\\n## 7. Where to learn more\\n\\n| Resource | Link |\\n|----------|------|\\n| **LangGraph documentation** (quick‑start, API reference, visual examples) | https://langchain.com/langgraph |\\n| **Tutorial notebooks** (building agents, loops, parallelism) | https://github.com/langchain-ai/langgraph-examples |\\n| **Blog post – “From Chains to Graphs: Scaling LLM Workflows”** | Search for “LangGraph blog” |\\n| **Community Discord / Slack** | LangChain community channels (often have a `#langgraph` discussion room) |\\n| **Video walkthrough (YouTube)** – “Building a self‑critiquing LLM agent with LangGraph” | Search YouTube for “LangGraph tutorial” |\\n\\n---\\n\\n## 8. TL;DR\\n\\n* **LangGraph** = a graph‑engine for LLM applications built on top of LangChain.  \\n* It gives you **explicit state**, **conditional routing**, **loops**, **parallel branches**, and **checkpointing** out of the box.  \\n* Use it whenever your LLM workflow is **more than a straight line**—e.g., agents that need to call tools, retry, ask follow‑up questions, involve humans, or run multiple sub‑tasks in parallel.  \\n\\nWith LangGraph you can describe sophisticated AI behavior as a **declarative diagram**, keep the logic testable and reproducible, and let the library handle execution, persistence, and visualization for you.', additional_kwargs={'reasoning_content': 'The user asked: \"What is LangGraph?\" The assistant responded with a suggestion to improve the question? That seems off. We need to answer the user\\'s question directly. Provide an explanation of LangGraph: a library for building stateful, multi-step LLM workflows, built on LangChain, etc. Include purpose, core features, architecture, typical use cases. Also maybe mention that it\\'s a graph-based approach to LLM orchestration, supports loops, conditionals, memory, etc. Provide examples. Should be concise but thorough.\\n\\nWe should answer directly.'}, response_metadata={'token_usage': {'completion_tokens': 3064, 'prompt_tokens': 292, 'total_tokens': 3356, 'completion_time': 6.499741933, 'completion_tokens_details': {'reasoning_tokens': 113}, 'prompt_time': 0.014907113, 'prompt_tokens_details': None, 'queue_time': 0.045557046, 'total_time': 6.514649046}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_e10890e4b9', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019c6829-0539-7d80-87a6-bd1eb4122ea9-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 292, 'output_tokens': 3064, 'total_tokens': 3356, 'output_token_details': {'reasoning': 113}})]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.invoke({\"messages\": [HumanMessage(content=\"What is LangGraph?\")]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "448a376d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T20:34:11.366391Z",
     "iopub.status.busy": "2026-02-16T20:34:11.366262Z",
     "iopub.status.idle": "2026-02-16T20:34:13.660216Z",
     "shell.execute_reply": "2026-02-16T20:34:13.659930Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---CALL AGENT---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='what is machine learning', additional_kwargs={}, response_metadata={}, id='c68382bd-ee50-4b72-9a6a-99c2c1e1b3f0'),\n",
       "  AIMessage(content='**Machine learning (ML)** is a branch of artificial intelligence that focuses on building systems that can learn from data rather than being explicitly programmed for every task. \\n\\n### Core Idea\\n- **Data\\u202f→\\u202fPattern\\u202f→\\u202fPrediction/Decision**  \\n  An ML model ingests examples (training data), discovers patterns or relationships within that data, and then uses those learned patterns to make predictions, classifications, or decisions on new, unseen inputs.\\n\\n### How It Works\\n1. **Collect Data** – Gather labeled (or unlabeled) examples relevant to the problem.\\n2. **Choose a Model** – Select an algorithm (e.g., linear regression, decision tree, neural network) that can represent the relationship you want to learn.\\n3. **Training** – Adjust the model’s internal parameters (weights, thresholds, etc.) to minimize error on the training data.\\n4. **Evaluation** – Test the trained model on separate data to gauge performance and avoid over‑fitting.\\n5. **Deployment** – Use the model in real‑world applications to make predictions on fresh data.\\n\\n### Main Types of Machine Learning\\n| Category | How It Learns | Typical Tasks |\\n|----------|---------------|----------------|\\n| **Supervised Learning** | Learns from input‑output pairs (labeled data) | Classification (spam detection), regression (price prediction) |\\n| **Unsupervised Learning** | Finds structure in unlabeled data | Clustering (customer segmentation), dimensionality reduction (PCA) |\\n| **Semi‑supervised Learning** | Uses a small amount of labeled data plus a larger unlabeled set | Improves performance when labeling is expensive |\\n| **Reinforcement Learning** | Learns via trial‑and‑error interactions with an environment, receiving rewards | Game playing (AlphaGo), robotics, recommendation systems |\\n| **Self‑Supervised / Contrastive Learning** | Generates its own supervisory signals from raw data | Language models (GPT), vision models (SimCLR) |\\n\\n### Common Algorithms & Models\\n- **Linear/Logistic Regression** – Simple, interpretable models for regression/classification.\\n- **Decision Trees & Ensembles** – Random Forests, Gradient Boosting (XGBoost, LightGBM) for tabular data.\\n- **Support Vector Machines (SVM)** – Effective for high‑dimensional spaces.\\n- **Neural Networks** – Deep learning models (CNNs for images, RNNs/Transformers for sequences).\\n- **Clustering** – K‑means, DBSCAN, hierarchical clustering.\\n- **Dimensionality Reduction** – PCA, t‑SNE, UMAP.\\n\\n### Real‑World Applications\\n- **Vision:** Image classification, object detection, medical imaging analysis.\\n- **Language:** Speech recognition, machine translation, chatbots, sentiment analysis.\\n- **Finance:** Fraud detection, algorithmic trading, credit scoring.\\n- **Healthcare:** Predictive diagnostics, drug discovery, personalized treatment plans.\\n- **Industry:** Predictive maintenance, quality control, supply‑chain optimization.\\n\\n### Key Benefits\\n- **Automation of Complex Tasks** that are hard to hand‑code.\\n- **Scalability** – Models can handle massive datasets and make rapid predictions.\\n- **Adaptability** – Systems can be retrained as new data becomes available.\\n\\n### Challenges & Considerations\\n- **Data Quality & Quantity** – Garbage in, garbage out.\\n- **Bias & Fairness** – Models can inherit biases present in training data.\\n- **Explainability** – Understanding why a model made a decision, especially for high‑stakes domains.\\n- **Computational Resources** – Training large models (e.g., deep neural nets) can be resource‑intensive.\\n- **Privacy & Security** – Protecting sensitive data and guarding against adversarial attacks.\\n\\n---\\n\\nIn summary, machine learning enables computers to **learn patterns from data** and **generalize** those patterns to new situations, powering many of the intelligent systems we interact with today.', additional_kwargs={'reasoning_content': 'The user asks: \"what is machine learning\". Need to provide a concise explanation. No need for external tools. Just answer.'}, response_metadata={'token_usage': {'completion_tokens': 844, 'prompt_tokens': 194, 'total_tokens': 1038, 'completion_time': 1.765743872, 'completion_tokens_details': {'reasoning_tokens': 27}, 'prompt_time': 0.00737713, 'prompt_tokens_details': None, 'queue_time': 0.15813894, 'total_time': 1.773121002}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_5a93aea882', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019c6829-2058-74c3-a900-62705f3a331e-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 194, 'output_tokens': 844, 'total_tokens': 1038, 'output_token_details': {'reasoning': 27}})]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.invoke({\"messages\": \"what is machine learning\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c03cb88a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T20:34:13.662965Z",
     "iopub.status.busy": "2026-02-16T20:34:13.662839Z",
     "iopub.status.idle": "2026-02-16T20:34:16.529142Z",
     "shell.execute_reply": "2026-02-16T20:34:16.528719Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---CALL AGENT---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---CHECK RELEVANCE---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---DECISION: DOCS RELEVANT---\n",
      "---GENERATE---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='weather in bengaluru', additional_kwargs={}, response_metadata={}, id='0e408caa-b291-41d5-bbf0-1414ab7bab84'),\n",
       "  AIMessage(content='', additional_kwargs={'reasoning_content': 'User asks \"weather in bengaluru\". We need to get current weather using get_weather tool.', 'tool_calls': [{'id': 'fc_16512029-5893-4e16-8349-f25a72434b22', 'function': {'arguments': '{\"city\":\"Bengaluru\"}', 'name': 'get_weather'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 51, 'prompt_tokens': 195, 'total_tokens': 246, 'completion_time': 0.113948723, 'completion_tokens_details': {'reasoning_tokens': 21}, 'prompt_time': 0.008754897, 'prompt_tokens_details': None, 'queue_time': 0.045619051, 'total_time': 0.12270362}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_a09bde29de', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019c6829-2950-7af1-9b17-c97aedfb7e3b-0', tool_calls=[{'name': 'get_weather', 'args': {'city': 'Bengaluru'}, 'id': 'fc_16512029-5893-4e16-8349-f25a72434b22', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 195, 'output_tokens': 51, 'total_tokens': 246, 'output_token_details': {'reasoning': 21}}),\n",
       "  ToolMessage(content='Bengaluru is 18.2°C', name='get_weather', id='bf010073-f7ff-47e1-a33a-aeb4e51ac3ce', tool_call_id='fc_16512029-5893-4e16-8349-f25a72434b22'),\n",
       "  HumanMessage(content='The temperature in Bengaluru is currently about\\u202f18.2\\u202f°C.', additional_kwargs={}, response_metadata={}, id='86e4897d-510e-424a-bd65-69d1b862b4a1')]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.invoke({\"messages\": \"weather in bengaluru\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "48cb2990",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T20:34:16.530376Z",
     "iopub.status.busy": "2026-02-16T20:34:16.530259Z",
     "iopub.status.idle": "2026-02-16T20:34:16.532255Z",
     "shell.execute_reply": "2026-02-16T20:34:16.531993Z"
    }
   },
   "outputs": [],
   "source": [
    "#EVAL\n",
    "\n",
    "EVAL_SET = [\n",
    "    {\n",
    "        \"question\": \"what is langchain?\",\n",
    "        \"expected_keywords\": [\"langchain\"]\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"what is langgraph?\",\n",
    "        \"expected_keywords\": [\"langgraph\"]\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is the weather in Delhi?\",\n",
    "        \"expected_keywords\": [\"Delhi\", \"°C\"]\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "33ed6616",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T20:34:16.533248Z",
     "iopub.status.busy": "2026-02-16T20:34:16.533151Z",
     "iopub.status.idle": "2026-02-16T20:34:16.535089Z",
     "shell.execute_reply": "2026-02-16T20:34:16.534833Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "def call_agent(question: str):\n",
    "    # initial state for your agent\n",
    "    state = {\n",
    "        \"messages\": [HumanMessage(content=question)]\n",
    "    }\n",
    "\n",
    "    final_state = graph.invoke(state)\n",
    "\n",
    "    # last message is model output\n",
    "    last_message = final_state[\"messages\"][-1]\n",
    "\n",
    "    return last_message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cdb4dd81",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T20:34:16.536014Z",
     "iopub.status.busy": "2026-02-16T20:34:16.535920Z",
     "iopub.status.idle": "2026-02-16T20:34:30.888569Z",
     "shell.execute_reply": "2026-02-16T20:34:30.888074Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q: what is langchain?\n",
      "---CALL AGENT---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: **LangChain** is an open‑source framework that makes it easy to build applications powered by large language models (LLMs). It provides the building blocks, abstractions, and utilities you need to connect LLMs with external data sources, APIs, and user interfaces, enabling you to create sophisticated “LLM‑first” products such as chatbots, agents, data‑augmented generators, and more.\n",
      "\n",
      "---\n",
      "\n",
      "## Core Concepts\n",
      "\n",
      "| Concept | What It Is | Why It Matters |\n",
      "|---------|------------|----------------|\n",
      "| **LLM Wrapper** | A thin abstraction around any LLM (OpenAI, Anthropic, Cohere, Hugging Face, etc.) that standardizes calls, streaming, token‑counting, and error handling. | Lets you swap models or providers without rewriting code. |\n",
      "| **Prompt Templates** | Structured ways to create prompts (simple strings, few‑shot examples, chat‑style messages) with variable interpolation. | Guarantees consistent, reusable prompting and reduces prompt‑brittleness. |\n",
      "| **Chains** | Sequential pipelines that pass the output of one component (LLM, tool, or custom function) as the input to the next. | Encapsulates multi‑step reasoning (e.g., “extract → search → summarize”). |\n",
      "| **Agents** | Dynamic executors that decide, at runtime, which tool or chain to invoke based on the LLM’s output (often using a “tool‑calling” format). | Enables autonomous behavior such as web‑search, database queries, or code execution. |\n",
      "| **Memory** | State‑keeping mechanisms (windowed, summary, conversation, vector‑store) that allow an LLM to remember prior interactions. | Makes chat‑like experiences feel coherent over long sessions. |\n",
      "| **Retrievers / Vector Stores** | Interfaces to similarity search over embeddings (FAISS, Pinecone, Chroma, Weaviate, etc.) that fetch relevant documents for a query. | Powers retrieval‑augmented generation (RAG) where the LLM answers using up‑to‑date or proprietary data. |\n",
      "| **Toolkits** | Pre‑packaged collections of tools for specific domains (e.g., SQL, CSV, PDF, APIs, web browsing). | Reduces boilerplate when building domain‑specific agents. |\n",
      "| **Callbacks & Logging** | Hooks that fire on events like LLM start/finish, chain step, error, token usage, etc. | Essential for observability, debugging, and cost monitoring. |\n",
      "| **Evaluation** | Utilities to run benchmarks, compute metrics (BLEU, ROUGE, exact match), and compare model outputs. | Helps you iterate on prompts and chain designs systematically. |\n",
      "\n",
      "---\n",
      "\n",
      "## Typical Architecture of a LangChain App\n",
      "\n",
      "```\n",
      "User Input\n",
      "   │\n",
      "   ▼\n",
      "PromptTemplate ──► LLM Wrapper ──► (LLM Output)\n",
      "   │                               │\n",
      "   ▼                               ▼\n",
      "Chain / Agent ──► Tool Calls (e.g., search, DB) ──► Retrieval / Computation\n",
      "   │                               │\n",
      "   ▼                               ▼\n",
      "Memory (optional)               Post‑processing\n",
      "   │                               │\n",
      "   └──────────────► Final Response ◄───────────────\n",
      "```\n",
      "\n",
      "1. **PromptTemplate** builds a prompt with the user’s query and any context.\n",
      "2. The **LLM Wrapper** sends it to the model.\n",
      "3. An **Agent** (or a fixed **Chain**) interprets the LLM’s response.  \n",
      "   - If the response includes a tool‑call, the appropriate tool (search, API, etc.) is executed.  \n",
      "   - The tool’s result may be fed back into the LLM for a second round.\n",
      "4. **Memory** can inject prior conversation snippets.\n",
      "5. The final answer is returned to the user.\n",
      "\n",
      "---\n",
      "\n",
      "## When to Use LangChain\n",
      "\n",
      "| Use‑Case | How LangChain Helps |\n",
      "|----------|----------------------|\n",
      "| **Chatbots with context** | Memory + chat‑style prompts keep the conversation coherent. |\n",
      "| **Retrieval‑Augmented Generation (RAG)** | Retriever → LLM chain lets you answer questions using your own documents. |\n",
      "| **Autonomous agents** | Agent + toolkits enable “think‑act‑observe” loops (e.g., “Find the cheapest flight”). |\n",
      "| **Data extraction & transformation** | Chains can combine extraction, validation, and storage steps. |\n",
      "| **Multi‑model orchestration** | Wrap different LLMs or combine them with non‑LLM services (SQL, APIs). |\n",
      "| **Rapid prototyping** | Prompt templates, pre‑built agents, and toolkits let you get a working demo in minutes. |\n",
      "\n",
      "---\n",
      "\n",
      "## Example: Simple Retrieval‑Augmented QA\n",
      "\n",
      "```python\n",
      "from langchain.llms import OpenAI\n",
      "from langchain.prompts import PromptTemplate\n",
      "from langchain.chains import RetrievalQA\n",
      "from langchain.vectorstores import FAISS\n",
      "from langchain.embeddings import OpenAIEmbeddings\n",
      "\n",
      "# 1️⃣ Load documents & create a vector store\n",
      "docs = [\"...\"]                     # your knowledge‑base texts\n",
      "embeddings = OpenAIEmbeddings()\n",
      "vectorstore = FAISS.from_texts(docs, embeddings)\n",
      "\n",
      "# 2️⃣ Define a prompt\n",
      "prompt = PromptTemplate(\n",
      "    template=\"\"\"\n",
      "    Use the following context to answer the question.\n",
      "    Context: {context}\n",
      "    Question: {question}\n",
      "    Answer:\"\"\",\n",
      "    input_variables=[\"context\", \"question\"],\n",
      ")\n",
      "\n",
      "# 3️⃣ Build the chain\n",
      "qa_chain = RetrievalQA.from_chain_type(\n",
      "    llm=OpenAI(),\n",
      "    retriever=vectorstore.as_retriever(),\n",
      "    chain_type=\"stuff\",           # simple concatenate‑then‑answer\n",
      "    return_source_documents=True,\n",
      "    chain_type_kwargs={\"prompt\": prompt},\n",
      ")\n",
      "\n",
      "# 4️⃣ Ask a question\n",
      "result = qa_chain({\"query\": \"What is LangChain?\"})\n",
      "print(result[\"answer\"])\n",
      "print(\"Sources:\", [doc.metadata[\"source\"] for doc in result[\"source_documents\"]])\n",
      "```\n",
      "\n",
      "*What happens under the hood?*  \n",
      "1. The query is embedded and used to fetch the most similar docs.  \n",
      "2. The retrieved docs are injected into the prompt template.  \n",
      "3. The LLM generates an answer using that context.  \n",
      "\n",
      "---\n",
      "\n",
      "## Ecosystem & Integrations\n",
      "\n",
      "| Category | Popular Integrations |\n",
      "|----------|----------------------|\n",
      "| **LLM Providers** | OpenAI, Anthropic, Cohere, Azure OpenAI, Hugging Face Inference, Llama‑CPP |\n",
      "| **Vector Stores** | FAISS, Pinecone, Chroma, Weaviate, Milvus, Qdrant, Supabase |\n",
      "| **Databases** | SQL (Postgres, MySQL, SQLite), MongoDB, DynamoDB, Pandas DataFrames |\n",
      "| **APIs & Tools** | SerpAPI (web search), Browserless (headless browsing), Zapier, Slack, Telegram, Twilio |\n",
      "| **Frameworks** | FastAPI, Flask, Django, Streamlit, Gradio, Next.js (via LangChain.js) |\n",
      "| **Observability** | LangSmith (LangChain’s own tracing UI), OpenTelemetry, LangChain callbacks |\n",
      "\n",
      "---\n",
      "\n",
      "## Getting Started\n",
      "\n",
      "1. **Install**  \n",
      "   ```bash\n",
      "   pip install langchain openai   # plus any vector‑store you need\n",
      "   ```\n",
      "\n",
      "2. **Set your API keys** (e.g., `export OPENAI_API_KEY=sk-…`).\n",
      "\n",
      "3. **Run a quick demo** – LangChain ships with a “Hello World” script that does a simple LLM call.\n",
      "\n",
      "   ```bash\n",
      "   python -c \"from langchain.llms import OpenAI; print(OpenAI()(\\\"Say hello in three languages\\\"))\"\n",
      "   ```\n",
      "\n",
      "4. **Explore the docs** – <https://python.langchain.com/> (or <https://js.langchain.com/> for JavaScript/TypeScript).\n",
      "\n",
      "---\n",
      "\n",
      "## Frequently Asked Questions\n",
      "\n",
      "| Question | Answer |\n",
      "|----------|--------|\n",
      "| *Do I have to use OpenAI?* | No. LangChain abstracts the LLM, so you can swap in Anthropic, Cohere, Azure, or even a self‑hosted model via Hugging Face or Llama‑CPP. |\n",
      "| *Is LangChain production‑ready?* | Yes. It’s used in many commercial products. For production you’ll typically add: <br>• LangSmith or custom callbacks for tracing <br>• Robust error handling & retries <br>• Rate‑limit and cost monitoring <br>• Secure secret management (e.g., Vault, AWS Secrets Manager). |\n",
      "| *How does LangChain differ from LangGraph?* | LangChain provides the **building blocks** (LLM wrappers, prompts, chains, agents). LangGraph, built on top of LangChain, adds a **graph‑oriented orchestration layer** that lets you define arbitrary directed‑acyclic graphs (or loops) of nodes, making complex, conditional workflows easier to visualize and run. |\n",
      "| *Can I use LangChain with non‑Python languages?* | Yes. There are official ports for **JavaScript/TypeScript** (`langchainjs`) and **Java** (`langchain4j`). The concepts (prompts, chains, agents) are the same across languages. |\n",
      "| *What about data privacy?* | You control where data lives: vector stores can be on‑premise, and you can route LLM calls through private endpoints (e.g., Azure OpenAI). LangChain itself does not send any data except what the underlying LLM provider receives. |\n",
      "\n",
      "---\n",
      "\n",
      "## TL;DR\n",
      "\n",
      "- **LangChain** = a modular toolkit for building LLM‑centric applications.  \n",
      "- It gives you **prompts, chains, agents, memory, retrievers, tools, callbacks**, and a host of integrations.  \n",
      "- Use it to **combine LLM reasoning with external knowledge or actions**, turning a raw language model into a full‑featured product.  \n",
      "\n",
      "If you’re starting a project that needs a chatbot, a knowledge‑base Q&A, an autonomous agent, or any workflow that mixes LLMs with external data/services, LangChain (or its graph‑based sibling LangGraph) is the go‑to framework to accelerate development and keep your codebase clean and extensible.\n",
      "✅ PASS\n",
      "\n",
      "Q: what is langgraph?\n",
      "---CALL AGENT---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: **LangGraph** is an open‑source Python library (maintained by the LangChain team) that makes it easy to build **state‑driven, multi‑step LLM applications** as **directed graphs** of “nodes” (functions, prompts, tools, or other agents).  \n",
      "\n",
      "---\n",
      "\n",
      "## Why it exists\n",
      "\n",
      "Large language models are great at single‑turn inference, but many real‑world tasks require:\n",
      "\n",
      "| Requirement | Traditional LLM approach | LangGraph solution |\n",
      "|-------------|--------------------------|--------------------|\n",
      "| **Multi‑step reasoning** (e.g., “search → summarize → decide”) | You have to write ad‑hoc loops or chain together many LangChain components manually. | Define each step as a node and let the graph orchestrate the flow. |\n",
      "| **Conditional branching** (e.g., “if the answer is ambiguous, ask the user”) | Hard‑coded if‑else logic scattered across code. | Graph edges can be conditional, based on the **state** that all nodes read/write. |\n",
      "| **Loops / retries** (e.g., “re‑ask until confidence > 0.9”) | You need custom while‑loops and termination checks. | Built‑in looping constructs (`while`, `for`, `repeat_until`) that automatically feed state back into earlier nodes. |\n",
      "| **State persistence & sharing** | You must pass a dict or context object manually. | LangGraph provides a **state store** (a dict‑like object) that every node can read from and write to, making data flow explicit and debuggable. |\n",
      "| **Visualization & debugging** | No native view of the execution path. | The graph can be rendered as a diagram (via `graphviz` or Mermaid) and includes step‑by‑step logging of state changes. |\n",
      "\n",
      "In short, LangGraph turns the “chain” metaphor of LangChain into a **graph** metaphor, giving you fine‑grained control over the execution order, loops, and branching while still leveraging LangChain’s LLM wrappers, memory, and tool integrations.\n",
      "\n",
      "---\n",
      "\n",
      "## Core Concepts\n",
      "\n",
      "| Concept | What it is | Typical usage |\n",
      "|---------|------------|---------------|\n",
      "| **Node** | A Python callable (function, LangChain chain, tool, or agent) that receives the current `state` and returns an updated `state` (or a partial update). | `def retrieve(state): …` or `ChatOpenAI(...)` wrapped as a node. |\n",
      "| **State** | A mutable dictionary that lives for the duration of a graph run. All nodes read/write to it. | Store `question`, `search_results`, `confidence`, etc. |\n",
      "| **Edge** | The directed connection between nodes. Edges can be unconditional or **conditional** (a predicate on the current state). | `if state[\"confidence\"] < 0.8 → retry_node`. |\n",
      "| **Graph** | The collection of nodes + edges + optional start/end nodes. Defined using the `Graph` class or via a DSL (`graph.add_edge(...)`). | `graph = Graph(start=\"router\")`. |\n",
      "| **Loop / Subgraph** | A sub‑graph that can be invoked repeatedly until a termination condition is met. | “Search → Summarize → Check → repeat until `enough_info`”. |\n",
      "| **Tool / Agent node** | Any LangChain tool (e.g., a web‑search tool, a calculator) or an autonomous agent can be wrapped as a node. | `search_tool = DuckDuckGoSearchRun(); graph.add_node(search_tool)`. |\n",
      "| **Visualization** | `graph.draw()` renders a diagram; `graph.run(..., debug=True)` prints state after each node. | Helpful for debugging complex flows. |\n",
      "\n",
      "---\n",
      "\n",
      "## Simple Example\n",
      "\n",
      "```python\n",
      "from langgraph.graph import Graph\n",
      "from langchain_community.tools import DuckDuckGoSearchRun\n",
      "from langchain_openai import ChatOpenAI\n",
      "\n",
      "# 1️⃣ Define nodes -------------------------------------------------\n",
      "def router(state):\n",
      "    \"\"\"Decide which path to take based on the user query.\"\"\"\n",
      "    query = state[\"question\"]\n",
      "    if \"weather\" in query.lower():\n",
      "        return {\"next\": \"weather_agent\"}\n",
      "    return {\"next\": \"search_and_summarize\"}\n",
      "\n",
      "def weather_agent(state):\n",
      "    # (pretend we have a weather tool)\n",
      "    city = state[\"question\"].split(\"weather in\")[-1].strip()\n",
      "    # call a weather tool here...\n",
      "    state[\"answer\"] = f\"The weather in {city} is sunny.\"\n",
      "    return {\"next\": \"end\"}\n",
      "\n",
      "def search_and_summarize(state):\n",
      "    search = DuckDuckGoSearchRun()\n",
      "    results = search.run(state[\"question\"])\n",
      "    # simple summarizer LLM\n",
      "    llm = ChatOpenAI()\n",
      "    summary = llm.invoke(f\"Summarize these results:\\n{results}\")\n",
      "    state[\"answer\"] = summary\n",
      "    return {\"next\": \"end\"}\n",
      "\n",
      "def end(state):\n",
      "    return state   # final output\n",
      "\n",
      "# 2️⃣ Build the graph ------------------------------------------------\n",
      "graph = Graph()\n",
      "graph.add_node(\"router\", router)\n",
      "graph.add_node(\"weather_agent\", weather_agent)\n",
      "graph.add_node(\"search_and_summarize\", search_and_summarize)\n",
      "graph.add_node(\"end\", end)\n",
      "\n",
      "graph.add_edge(\"router\", \"weather_agent\", condition=lambda s: s[\"next\"] == \"weather_agent\")\n",
      "graph.add_edge(\"router\", \"search_and_summarize\", condition=lambda s: s[\"next\"] == \"search_and_summarize\")\n",
      "graph.add_edge(\"weather_agent\", \"end\")\n",
      "graph.add_edge(\"search_and_summarize\", \"end\")\n",
      "\n",
      "# 3️⃣ Run ------------------------------------------------------------\n",
      "result = graph.run({\"question\": \"What’s the weather in Tokyo?\"})\n",
      "print(result[\"answer\"])\n",
      "```\n",
      "\n",
      "*What happens?*  \n",
      "\n",
      "1. `router` looks at the question → decides the next node.  \n",
      "2. The chosen node runs, writes its output into `state`.  \n",
      "3. The graph follows the edge to `end`, and the final `state` is returned.\n",
      "\n",
      "Even though this is a tiny example, the same pattern scales to dozens of nodes, nested sub‑graphs, and complex conditional logic.\n",
      "\n",
      "---\n",
      "\n",
      "## Key Features (as of 2024‑Q4)\n",
      "\n",
      "| Feature | Description |\n",
      "|---------|-------------|\n",
      "| **Declarative graph DSL** – Build graphs programmatically or via a YAML/JSON spec. |\n",
      "| **State versioning** – Each step can be logged with a snapshot of the full state for auditability. |\n",
      "| **Built‑in loop operators** – `while`, `repeat_until`, `for_each` with automatic termination safeguards. |\n",
      "| **Tool & Agent integration** – Seamlessly wrap any LangChain `Tool`, `Agent`, or custom function. |\n",
      "| **Parallel branches** – Execute independent sub‑graphs concurrently (via `asyncio` or thread pools). |\n",
      "| **Persisted state stores** – Plug in Redis, Postgres, or in‑memory dicts for long‑running workflows. |\n",
      "| **Visualization** – Export to Mermaid, Graphviz, or Streamlit UI for live debugging. |\n",
      "| **Typed state (pydantic)** – Optionally enforce schemas on the state dict to catch errors early. |\n",
      "| **Observability hooks** – Emit OpenTelemetry spans, custom callbacks, or LangChain “tracers”. |\n",
      "\n",
      "---\n",
      "\n",
      "## Typical Use‑Cases\n",
      "\n",
      "| Domain | Example workflow |\n",
      "|--------|------------------|\n",
      "| **Enterprise Q&A** | Router → Retrieval → RAG → Fact‑check → Summarize → Return |\n",
      "| **Customer Support Bot** | Intent classifier → (if “billing”) → Billing tool → (else) → Knowledge‑base search → Escalate to human |\n",
      "| **Data‑analysis assistant** | Parse user request → Generate SQL → Run query → Visualize results → Ask follow‑up |\n",
      "| **Multi‑modal agents** | Image caption → Text generation → Decision → Call external API |\n",
      "| **Iterative design** | Generate sketch → Evaluate with a critic LLM → Refine → Loop until quality threshold |\n",
      "\n",
      "---\n",
      "\n",
      "## How LangGraph Relates to LangChain\n",
      "\n",
      "| Aspect | LangChain | LangGraph |\n",
      "|--------|-----------|-----------|\n",
      "| **Abstraction level** | Chains (linear) & Agents (single‑turn) | Graphs (arbitrary DAGs with loops) |\n",
      "| **State handling** | Implicit via `Memory` objects | Explicit `state` dict shared across all nodes |\n",
      "| **Control flow** | Mostly sequential, limited branching (`if` inside an agent) | Full conditional edges, loops, sub‑graphs |\n",
      "| **Tool usage** | Tools are called from inside an agent’s prompt | Tools are first‑class nodes, can be sequenced or run in parallel |\n",
      "| **Visualization** | Minimal (just chain representation) | Built‑in graph rendering & step‑by‑step logs |\n",
      "| **Learning curve** | Low (just chain objects) | Slightly higher (graph design) but pays off for complex flows |\n",
      "\n",
      "You can think of LangGraph as **the next evolutionary layer on top of LangChain**: you still use LangChain’s LLM wrappers, memory classes, and tool utilities, but you orchestrate them with a graph instead of a simple chain.\n",
      "\n",
      "---\n",
      "\n",
      "## Getting Started (quick checklist)\n",
      "\n",
      "1. **Install**  \n",
      "   ```bash\n",
      "   pip install \"langgraph[all]\"   # pulls LangChain & optional extras\n",
      "   ```\n",
      "\n",
      "2. **Define nodes** – any callable that accepts a `state: dict` (or a pydantic model) and returns a dict of updates.\n",
      "\n",
      "3. **Create the graph** – instantiate `Graph()`, add nodes, and connect them with `add_edge`. Use `condition` for branching.\n",
      "\n",
      "4. **Run** – `graph.run(initial_state, debug=True)`; the returned dict is the final state.\n",
      "\n",
      "5. **Debug/visualize** – `graph.draw(output_path=\"my_flow.png\")` or `graph.visualize()` in a notebook.\n",
      "\n",
      "6. **Persist** – If you need long‑running sessions, pass a custom `state_store` (e.g., Redis) when constructing the graph.\n",
      "\n",
      "---\n",
      "\n",
      "## Resources\n",
      "\n",
      "| Resource | Link |\n",
      "|----------|------|\n",
      "| **Official Docs** | <https://langchain.com/langgraph> |\n",
      "| **GitHub Repo** | <https://github.com/langchain-ai/langgraph> |\n",
      "| **Tutorial Notebook** | <https://github.com/langchain-ai/langgraph/blob/main/examples/tutorial.ipynb> |\n",
      "| **Blog post (intro)** | “Building Stateful LLM Apps with LangGraph” – LangChain blog (Oct 2024) |\n",
      "| **Community Slack** | `#langgraph` channel on the LangChain community workspace |\n",
      "| **Video walkthrough** | LangChain YouTube – “LangGraph 101” (45‑min) |\n",
      "\n",
      "---\n",
      "\n",
      "### TL;DR\n",
      "\n",
      "**LangGraph** is a graph‑based orchestration layer for LLM applications. It lets you define reusable nodes, share a mutable state, and connect them with conditional edges and loops. This makes it far easier to build sophisticated, multi‑step, stateful agents (RAG pipelines, decision‑making bots, iterative design loops, etc.) while still leveraging the rich ecosystem of LangChain tools and models.\n",
      "✅ PASS\n",
      "\n",
      "Q: What is the weather in Delhi?\n",
      "---CALL AGENT---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---CHECK RELEVANCE---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---DECISION: DOCS RELEVANT---\n",
      "---GENERATE---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: The temperature in Delhi is 2.4 °C.\n",
      "✅ PASS\n",
      "\n",
      "Final accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "def run_eval(eval_set):\n",
    "    passed = 0\n",
    "    results = []\n",
    "\n",
    "    for item in eval_set:\n",
    "        q = item[\"question\"]\n",
    "        expected = item[\"expected_keywords\"]\n",
    "\n",
    "        print(f\"\\nQ: {q}\")\n",
    "        answer = call_agent(q)\n",
    "        print(f\"A: {answer}\")\n",
    "\n",
    "        score = all(word.lower() in answer.lower() for word in expected)\n",
    "\n",
    "        if score:\n",
    "            print(\"✅ PASS\")\n",
    "            passed += 1\n",
    "        else:\n",
    "            print(\"❌ FAIL\")\n",
    "\n",
    "        results.append({\n",
    "            \"question\": q,\n",
    "            \"answer\": answer,\n",
    "            \"passed\": score\n",
    "        })\n",
    "\n",
    "    accuracy = passed / len(eval_set)\n",
    "    print(f\"\\nFinal accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "    return results, accuracy\n",
    "\n",
    "results, accuracy = run_eval(EVAL_SET)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3bfb612d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T20:34:30.890310Z",
     "iopub.status.busy": "2026-02-16T20:34:30.890203Z",
     "iopub.status.idle": "2026-02-16T20:35:05.130783Z",
     "shell.execute_reply": "2026-02-16T20:35:05.130412Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q: what is langchain?\n",
      "---CALL AGENT---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: **LangChain** is an open‑source framework that makes it easier to build applications powered by large language models (LLMs). It provides a set of modular components and abstractions that let developers:\n",
      "\n",
      "| Area | What LangChain Provides |\n",
      "|------|--------------------------|\n",
      "| **LLM Integration** | Simple wrappers for many LLM providers (OpenAI, Anthropic, Cohere, Hugging Face, Azure, etc.) so you can call a model with a single line of code. |\n",
      "| **Prompt Management** | Prompt templates, chaining, and dynamic prompt generation (e.g., few‑shot, few‑shot‑with‑examples, chat history). |\n",
      "| **Memory** | Built‑in “memory” objects that let a chain remember past interactions, enabling conversational agents that keep context across turns. |\n",
      "| **Chains** | A **Chain** is a sequence of calls (LLM → prompt → post‑processing) that can be composed, nested, or conditionally executed. |\n",
      "| **Agents** | Agents combine an LLM with a **tool‑use** loop: the model decides which tool (e.g., search, calculator, database query) to call, runs it, and incorporates the result back into its reasoning. |\n",
      "| **Tools & Integrations** | Connectors for external resources: search APIs, vector stores (FAISS, Pinecone, Weaviate, Chroma, etc.), databases, APIs, file systems, and more. |\n",
      "| **Retrieval‑Augmented Generation (RAG)** | Utilities to fetch relevant documents from a vector store, combine them with prompts, and generate answers grounded in external knowledge. |\n",
      "| **Evaluation** | Helpers to benchmark LLM outputs, compute metrics, and run systematic evaluations. |\n",
      "| **Deployment** | Support for LangServe (HTTP API server), LangGraph (state‑machine / workflow orchestration), and easy packaging for cloud functions or serverless environments. |\n",
      "| **Community & Ecosystem** | A growing ecosystem of community‑contributed integrations, tutorials, and example notebooks. |\n",
      "\n",
      "### Why Use LangChain?\n",
      "\n",
      "1. **Productivity** – It abstracts repetitive boilerplate (prompt formatting, token limits, response parsing) so you can focus on the core logic of your app.\n",
      "2. **Modularity** – Components are interchangeable; you can swap out an LLM, a vector store, or a memory implementation without rewriting the whole pipeline.\n",
      "3. **Scalability** – The framework is designed to work from a quick prototype in a notebook to a production‑grade microservice with monitoring, caching, and parallel execution.\n",
      "4. **Complex Workflows** – With LangChain (and its newer companion **LangGraph**), you can define sophisticated stateful workflows (e.g., multi‑step reasoning, branching, retries) that go beyond a single “prompt‑and‑response” call.\n",
      "\n",
      "### Typical Use Cases\n",
      "\n",
      "| Use Case | How LangChain Helps |\n",
      "|----------|---------------------|\n",
      "| **Chatbots / Conversational Agents** | Memory + agents + tool use → a bot that can remember user context, browse the web, or run calculations. |\n",
      "| **Question‑Answering over Private Docs** | RAG pipeline: embed documents → vector store → retrieve relevant chunks → LLM generates answer. |\n",
      "| **Data Extraction / Summarization** | Chains that call an LLM to extract fields, then post‑process (e.g., JSON validation) before storing results. |\n",
      "| **Code Generation / Debugging Assistants** | Agents that can run a linter, execute code, or query a repository, feeding results back to the LLM. |\n",
      "| **Decision Support** | Multi‑step reasoning chains that gather data, evaluate alternatives, and produce a recommendation. |\n",
      "\n",
      "### Core Concepts in a Nutshell\n",
      "\n",
      "| Concept | Description |\n",
      "|---------|-------------|\n",
      "| **PromptTemplate** | A reusable string with placeholders that are filled at runtime. |\n",
      "| **LLM** | Wrapper around a language model; you call `llm.invoke(prompt)` to get a response. |\n",
      "| **Chain** | An ordered collection of steps (LLM calls, parsers, utilities). |\n",
      "| **Agent** | An LLM that can decide which **Tool** to invoke; the loop continues until a final answer is produced. |\n",
      "| **Tool** | Any callable function (search API, calculator, database query) exposed to an agent. |\n",
      "| **Memory** | Stores past inputs/outputs; examples: `ConversationBufferMemory`, `ConversationSummaryMemory`. |\n",
      "| **Retriever** | Component that fetches relevant documents from a vector store based on a query. |\n",
      "| **Document** | A simple data object (`page_content`, `metadata`) used throughout RAG pipelines. |\n",
      "| **LangGraph** | A newer library built on top of LangChain that lets you define state‑machine‑style workflows (nodes, edges, conditions) for even more complex orchestrations. |\n",
      "\n",
      "### Minimal Example (Python)\n",
      "\n",
      "```python\n",
      "from langchain.llms import OpenAI\n",
      "from langchain.prompts import PromptTemplate\n",
      "from langchain.chains import LLMChain\n",
      "\n",
      "# 1. Define a prompt template\n",
      "prompt = PromptTemplate(\n",
      "    input_variables=[\"question\"],\n",
      "    template=\"You are a helpful assistant. Answer the question concisely:\\n{question}\"\n",
      ")\n",
      "\n",
      "# 2. Instantiate an LLM (requires OPENAI_API_KEY env var)\n",
      "llm = OpenAI(model=\"gpt-4o-mini\")\n",
      "\n",
      "# 3. Build a chain that ties the prompt to the LLM\n",
      "chain = LLMChain(prompt=prompt, llm=llm)\n",
      "\n",
      "# 4. Run the chain\n",
      "answer = chain.invoke({\"question\": \"What are the main benefits of electric vehicles?\"})\n",
      "print(answer[\"text\"])\n",
      "```\n",
      "\n",
      "### Getting Started\n",
      "\n",
      "1. **Install**  \n",
      "   ```bash\n",
      "   pip install langchain openai   # plus any vector‑store libs you need\n",
      "   ```\n",
      "2. **Read the Docs** – <https://python.langchain.com/> (or the TypeScript/JS docs if you’re on Node).  \n",
      "3. **Explore the Examples** – The repo includes notebooks for chatbots, RAG, agents, and more.  \n",
      "4. **Pick a Starter Template** – LangChain’s “templates” directory has ready‑made configurations for common patterns (e.g., `qa_with_sources`, `agent_with_tools`).  \n",
      "\n",
      "---\n",
      "\n",
      "**In short:** LangChain is a toolkit that stitches together large language models, prompts, memory, external tools, and data stores into reusable, composable building blocks, enabling developers to create sophisticated LLM‑driven applications with far less boilerplate and greater flexibility.\n",
      "✅ PASS\n",
      "\n",
      "Q: what is langgraph?\n",
      "---CALL AGENT---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---CHECK RELEVANCE---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---DECISION: DOCS NOT RELEVANT---\n",
      "no\n",
      "---TRANSFORM QUERY---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---CALL AGENT---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---CHECK RELEVANCE---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---DECISION: DOCS NOT RELEVANT---\n",
      "no\n",
      "---TRANSFORM QUERY---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---CALL AGENT---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: ## LangGraph – at a glance  \n",
      "\n",
      "| Aspect | What it is | Why it matters |\n",
      "|--------|------------|----------------|\n",
      "| **Definition** | An open‑source Python library (maintained by the LangChain team) for **building, visualising, and executing graph‑structured workflows** that orchestrate Large Language Models (LLMs), tools, and other services. | It turns the often‑linear “prompt‑‑>completion” pattern into a **state‑machine / directed‑graph** where each node can be a prompt, a tool call, a conditional branch, a loop, or any custom Python function. |\n",
      "| **Core goal** | Make **complex, multi‑turn, stateful LLM applications** (chat assistants, agents, data pipelines, decision‑making bots, etc.) **easy to design, test, debug, and maintain**. | As LLM‑driven products get richer (retrieval‑augmented generation, tool‑use, self‑reflection, multi‑agent coordination), a graph abstraction gives you a clear mental model and reusable components. |\n",
      "| **Relationship to LangChain** | LangGraph is built on top of LangChain’s primitives (LLM wrappers, prompts, memory, tools). It re‑uses the same `BaseLanguageModel`, `Tool`, `PromptTemplate`, etc., but adds a **graph execution engine** and a **visual DSL**. | You can start with plain LangChain code and “upgrade” to LangGraph when you need branching, loops, or explicit state handling without rewriting your LLM calls. |\n",
      "\n",
      "---\n",
      "\n",
      "## 1. Why a graph‑based abstraction?\n",
      "\n",
      "Traditional LangChain chains are linear: `Prompt → LLM → Tool → LLM → …`. Real‑world agents often need:\n",
      "\n",
      "* **Conditional routing** – “If the user asks for a calendar event, call the calendar tool; otherwise, search the web.”\n",
      "* **Loops / retries** – “Keep asking the LLM to refine a summary until a quality metric is met.”\n",
      "* **Parallel branches** – “Generate a draft and a list of references at the same time.”\n",
      "* **Explicit state** – “Remember the current step, the last tool output, and any user‑provided context.”\n",
      "\n",
      "A **directed graph** captures all of these patterns naturally:\n",
      "\n",
      "```\n",
      "          ┌───────────────┐\n",
      "          │   StartNode   │\n",
      "          └───────┬───────┘\n",
      "                  │\n",
      "          ┌───────▼───────┐\n",
      "          │   PromptNode │\n",
      "          └───────┬───────┘\n",
      "   ┌─────────────┼─────────────┐\n",
      "   │             │             │\n",
      "┌──▼───┐     ┌───▼───┐     ┌───▼───┐\n",
      "│ToolA│     │ToolB │     │LLM‑2 │\n",
      "└──┬───┘     └───┬───┘     └───┬───┘\n",
      "   │             │             │\n",
      "   └───────►─────┘──────►───────┘\n",
      "          (edges encode routing logic)\n",
      "```\n",
      "\n",
      "The graph engine walks the nodes, evaluates edge conditions, updates a **state dictionary**, and can pause/resume (useful for async APIs, human‑in‑the‑loop, or streaming).  \n",
      "\n",
      "---\n",
      "\n",
      "## 2. Core concepts & building blocks  \n",
      "\n",
      "| Concept | Description | Typical implementation |\n",
      "|---------|-------------|------------------------|\n",
      "| **Node** | A unit of work. Can be a `PromptNode`, `ToolNode`, `LLMNode`, `FunctionNode`, or a custom subclass. | `PromptNode(prompt_template, llm)` – renders a prompt, calls the LLM, stores the response in state. |\n",
      "| **Edge** | Directed connection that optionally carries a **condition** (a Python callable that inspects the current state) and a **label**. | `Edge(source, target, condition=lambda s: s[\"needs_tool\"])`. |\n",
      "| **State** | A mutable dict passed from node to node. Holds LLM outputs, tool results, user messages, and any derived variables. | `state[\"summary\"] = \"…\"`, `state[\"loop_counter\"] = 3`. |\n",
      "| **Graph** | The container (`Graph` or `StateGraph`) that holds nodes, edges, and the execution engine. | `graph = StateGraph(start_node=\"init\")`. |\n",
      "| **Loop / Retry** | Implemented by an edge that points back to an earlier node, often guarded by a condition (e.g., `while not state[\"valid\"]`). | `Edge(\"check_quality\", \"refine\", condition=lambda s: not s[\"quality_ok\"])`. |\n",
      "| **Memory integration** | You can attach any LangChain memory (ConversationBuffer, VectorStoreRetriever, etc.) to nodes, or store the whole memory in the graph state. | `PromptNode(..., memory=ConversationBufferMemory())`. |\n",
      "| **Visualization** | `graph.draw_png()` or `graph.get_dot()` produces a GraphViz diagram, useful for debugging and documentation. | `graph.get_dot().render(\"my_workflow\", format=\"png\")`. |\n",
      "| **Async / Streaming** | Nodes can be async functions; the engine can yield intermediate states for UI streaming. | `async def call_llm(state): …`. |\n",
      "| **Human‑in‑the‑loop** | A node can pause execution (`await pause(state)`) and resume when a human supplies input. | `HumanInputNode(prompt=\"Please clarify…\")`. |\n",
      "\n",
      "---\n",
      "\n",
      "## 3. Typical use‑cases  \n",
      "\n",
      "| Use‑case | How LangGraph shines |\n",
      "|----------|----------------------|\n",
      "| **Retrieval‑augmented generation (RAG) with feedback** | 1️⃣ Retrieve docs → 2️⃣ LLM drafts answer → 3️⃣ LLM evaluates answer quality → 4️⃣ If not good, loop back to retrieve more or re‑prompt. |\n",
      "| **Tool‑using agents** (e.g., calendar, calculator, web search) | Conditional edges decide which tool to invoke; loop edges retry on tool failures; state holds tool results for subsequent prompts. |\n",
      "| **Multi‑agent coordination** | Each agent is a sub‑graph; a master graph routes messages between them, merges results, and decides final output. |\n",
      "| **Complex UI assistants** (chat + form filling + file upload) | Nodes handle UI events, store partial form data, call LLM for suggestions, and finally submit. |\n",
      "| **Data pipelines with LLM transformations** | Use LLM nodes to clean/normalize data, tool nodes to call external APIs, and loop nodes for iterative improvement. |\n",
      "| **Testing & debugging of LLM logic** | Visual graph + deterministic state snapshots make it easy to unit‑test each node and edge condition. |\n",
      "| **Human‑in‑the‑loop review** | Pause after a critical step, let a reviewer edit `state[\"draft\"]`, then resume downstream nodes. |\n",
      "\n",
      "---\n",
      "\n",
      "## 4. Minimal example – a “summarize‑and‑verify” workflow  \n",
      "\n",
      "```python\n",
      "from langchain.llms import OpenAI\n",
      "from langgraph.graph import StateGraph, PromptNode, FunctionNode, Edge\n",
      "\n",
      "# 1️⃣ Define the LLM we’ll use\n",
      "llm = OpenAI(model=\"gpt-4o-mini\", temperature=0.0)\n",
      "\n",
      "# 2️⃣ Prompt node: generate a summary of a document\n",
      "summary_prompt = \"\"\"Summarize the following text in 3 bullet points:\\n\\n{doc}\"\"\"\n",
      "summarize = PromptNode(\n",
      "    prompt_template=summary_prompt,\n",
      "    llm=llm,\n",
      "    input_keys=[\"doc\"],          # pulled from state\n",
      "    output_key=\"summary\",        # stored back in state\n",
      ")\n",
      "\n",
      "# 3️⃣ Function node: simple quality check (e.g., at least 3 lines)\n",
      "def check_quality(state):\n",
      "    ok = len(state[\"summary\"].splitlines()) >= 3\n",
      "    state[\"quality_ok\"] = ok\n",
      "    return state\n",
      "\n",
      "quality_check = FunctionNode(func=check_quality)\n",
      "\n",
      "# 4️⃣ Prompt node: ask LLM to improve if not ok\n",
      "improve_prompt = \"\"\"The previous summary was too short. Rewrite it to contain at least 3 bullet points.\\n\\n{summary}\"\"\"\n",
      "improve = PromptNode(\n",
      "    prompt_template=improve_prompt,\n",
      "    llm=llm,\n",
      "    input_keys=[\"summary\"],\n",
      "    output_key=\"summary\",\n",
      ")\n",
      "\n",
      "# 5️⃣ Build the graph\n",
      "graph = StateGraph(start_key=\"summarize\")\n",
      "graph.add_node(\"summarize\", summarize)\n",
      "graph.add_node(\"quality_check\", quality_check)\n",
      "graph.add_node(\"improve\", improve)\n",
      "\n",
      "# Edges\n",
      "graph.add_edge(\"summarize\", \"quality_check\")\n",
      "graph.add_edge(\"quality_check\", \"improve\",\n",
      "               condition=lambda s: not s[\"quality_ok\"])   # loop if bad\n",
      "graph.add_edge(\"quality_check\", \"end\",\n",
      "               condition=lambda s: s[\"quality_ok\"])      # finish if good\n",
      "\n",
      "graph.set_end(\"end\")\n",
      "\n",
      "# 6️⃣ Run it\n",
      "state = {\"doc\": \"Your long document text goes here...\"}\n",
      "final_state = graph.run(state)\n",
      "\n",
      "print(\"✅ Final summary:\")\n",
      "print(final_state[\"summary\"])\n",
      "```\n",
      "\n",
      "**What happens?**  \n",
      "\n",
      "1. `summarize` creates a first draft.  \n",
      "2. `quality_check` stores `quality_ok`.  \n",
      "3. If the draft is too short, the edge to `improve` fires, looping back to generate a better version.  \n",
      "4. When the condition is satisfied, the graph terminates at `end`.  \n",
      "\n",
      "The whole flow is visible as a GraphViz diagram with `graph.get_dot().render(\"summarize_flow\", format=\"png\")`.\n",
      "\n",
      "---\n",
      "\n",
      "## 5. How to get started  \n",
      "\n",
      "| Step | Action |\n",
      "|------|--------|\n",
      "| **1. Install** | `pip install \"langgraph[all]\"` (includes optional visualisation deps). |\n",
      "| **2. Learn the DSL** | The official docs have a *“Getting started with StateGraph”* tutorial; copy the example above and modify it. |\n",
      "| **3. Pick a memory/tool** | Decide if you need a vector store, a calculator tool, etc., and import the corresponding LangChain component. |\n",
      "| **4. Sketch the graph** | Draw a quick flowchart on paper or use `graph.get_dot()` to prototype. |\n",
      "| **5. Write nodes** | Start with simple `PromptNode`s; later replace with `FunctionNode`s for custom logic. |\n",
      "| **6. Test edges** | Write unit tests for each edge condition (`assert edge.condition(state) == True`). |\n",
      "| **7. Iterate** | Add loops, parallel branches, or sub‑graphs as the product matures. |\n",
      "| **8. Deploy** | The graph can be run synchronously (`graph.run(state)`) or async (`await graph.arun(state)`). Deploy as a FastAPI endpoint, a Lambda, or inside a LangChain “AgentExecutor”. |\n",
      "\n",
      "---\n",
      "\n",
      "## 6. Frequently asked questions  \n",
      "\n",
      "| Question | Answer |\n",
      "|----------|--------|\n",
      "| **Is LangGraph only for LLMs?** | No. Any callable (API request, DB query, pure Python function) can be a node. LLMs are the most common use‑case, but the graph works for arbitrary pipelines. |\n",
      "| **Does it replace LangChain?** | It *extends* LangChain. You still use LangChain’s prompts, tools, memory, retrievers, etc. LangGraph just adds a higher‑level orchestration layer. |\n",
      "| **Can I visualise a running graph?** | Yes. `graph.get_dot(state=state)` can colour nodes based on whether they have been visited, making debugging interactive. |\n",
      "| **Is state persisted across runs?** | By default the state lives only in memory for a single execution. You can serialize it (e.g., to a DB or Redis) and pass it back in `graph.run(state)` to resume later. |\n",
      "| **How does error handling work?** | A node can raise an exception; you can attach an `Edge` with `on_error=True` that routes to a fallback node (e.g., “retry” or “human review”). |\n",
      "| **Is it thread‑safe?** | The engine itself is stateless; the only mutable object is the `state` dict you provide. If you share that dict across threads you must protect it. Running independent graph instances in parallel is safe. |\n",
      "| **Can I embed a LangGraph in a LangChain `AgentExecutor`?** | Yes. You can wrap a graph as a custom `Tool` and let an existing LangChain agent call it, or you can replace the agent’s internal chain with a graph. |\n",
      "| **What about cost control?** | Because each node is a separate LLM call, you can attach per‑node cost‑tracking logic (e.g., a `FunctionNode` that updates `state[\"cost\"]`). This makes budgeting explicit. |\n",
      "\n",
      "---\n",
      "\n",
      "## 7. TL;DR summary  \n",
      "\n",
      "* **LangGraph** is a **graph‑based orchestration library** for LLM‑centric (and general) workflows, built on top of LangChain.  \n",
      "* It lets you **declare nodes (prompts, tools, functions) and edges (conditional routing, loops)**, run them with a mutable **state** object, and **visualise** the whole pipeline.  \n",
      "* Ideal for **agents, RAG pipelines, multi‑step assistants, data‑processing chains, and any situation where you need branching, retries, or human‑in‑the‑loop**.  \n",
      "* You get **reusable, testable components**, explicit cost/quality tracking, and a clean mental model that scales from a simple two‑step prompt to a full‑blown multi‑agent system.  \n",
      "\n",
      "If you’re already using LangChain and find yourself writing a lot of `if … else …` and manual loops to manage LLM calls, switching to LangGraph can dramatically reduce boilerplate, improve readability, and give you powerful debugging visualisations out of the box.\n",
      "✅ PASS\n",
      "\n",
      "Q: What is the weather in Delhi?\n",
      "---CALL AGENT---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---CHECK RELEVANCE---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---DECISION: DOCS RELEVANT---\n",
      "---GENERATE---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: The temperature in Delhi is 2.4 °C.\n",
      "✅ PASS\n",
      "\n",
      "Final accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "results, accuracy = run_eval(EVAL_SET)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7c2427a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T20:35:05.135903Z",
     "iopub.status.busy": "2026-02-16T20:35:05.135779Z",
     "iopub.status.idle": "2026-02-16T20:35:31.024728Z",
     "shell.execute_reply": "2026-02-16T20:35:31.024377Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q: what is langchain?\n",
      "---CALL AGENT---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: **LangChain** is an open‑source framework that makes it easier to build **applications powered by large language models (LLMs)**—especially those that need to go beyond a single prompt‑completion cycle.  \n",
      "\n",
      "### Core ideas\n",
      "\n",
      "| Concept | What it means |\n",
      "|---------|---------------|\n",
      "| **Chains** | A *chain* is a sequence (or graph) of operations that combine LLM calls, prompts, data look‑ups, and other logic. You can think of it as a “pipeline” that takes an input, runs one or more steps, and returns a result. |\n",
      "| **Agents** | An *agent* is a more autonomous component that decides **what** actions to take next (e.g., “search the web”, “call a calculator”, “write a summary”) based on the LLM’s reasoning. Agents use tools and can loop until a goal is reached. |\n",
      "| **Memory** | For multi‑turn conversations, *memory* stores past interactions (or extracted facts) so the LLM can refer back to them. LangChain provides simple in‑memory buffers, vector‑store‑backed memory, and custom implementations. |\n",
      "| **Data‑aware prompting** | LangChain encourages you to retrieve relevant external information (documents, database rows, APIs) and inject it into the prompt, turning a “knowledge‑only” LLM into a **knowledge‑augmented** system. |\n",
      "| **Tool integration** | The framework ships adapters for many common services: vector stores (FAISS, Pinecone, Chroma, Weaviate, etc.), APIs (OpenAI, Anthropic, Cohere, Azure, HuggingFace), web‑search, SQL databases, spreadsheets, and more. |\n",
      "| **Composable building blocks** | Prompt templates, output parsers, callbacks, logging, tracing, and evaluation utilities are all modular, allowing you to mix‑and‑match them. |\n",
      "\n",
      "### Typical use‑cases\n",
      "\n",
      "| Use‑case | How LangChain helps |\n",
      "|----------|---------------------|\n",
      "| **Chatbots with context** | Memory + retrieval‑augmented generation (RAG) lets the bot remember prior turns and answer based on private docs. |\n",
      "| **Question‑answering over private data** | A chain that (1) embeds the query, (2) searches a vector store, (3) formats the top docs into a prompt, (4) calls the LLM, (5) parses the answer. |\n",
      "| **Automation / agents** | An agent can decide to call a calculator, fetch a web page, or write to a spreadsheet, orchestrated by LangChain’s tool‑calling API. |\n",
      "| **Data extraction / transformation** | Prompt templates + output parsers turn free‑form LLM output into structured JSON, CSV, or database rows. |\n",
      "| **LLM‑powered code generation** | Chains can feed code snippets to a compiler or test runner, capture results, and loop until the code passes tests. |\n",
      "\n",
      "### Architecture at a glance\n",
      "\n",
      "```\n",
      "User Input ──► PromptTemplate ──► LLM (OpenAI, Anthropic, …)\n",
      "                     │\n",
      "                     ▼\n",
      "                (Optional) Retrieval ──► VectorStore / DB\n",
      "                     │\n",
      "                     ▼\n",
      "                (Optional) Memory ──► Store / Retrieve past context\n",
      "                     │\n",
      "                     ▼\n",
      "                (Optional) Tools ──► Search, Calculator, API calls\n",
      "                     │\n",
      "                     ▼\n",
      "                OutputParser ──► Structured result\n",
      "```\n",
      "\n",
      "All of these pieces are **Python classes** (there’s also a TypeScript/JS version) that can be instantiated and wired together. The library also provides higher‑level helpers like `ConversationalRetrievalChain`, `SQLDatabaseChain`, `AgentExecutor`, etc., that bundle common patterns.\n",
      "\n",
      "### Why use LangChain?\n",
      "\n",
      "1. **Productivity** – You don’t have to reinvent the wheel for common patterns (RAG, agents, memory).  \n",
      "2. **Flexibility** – You can swap any component (e.g., switch from OpenAI gpt‑4 to Anthropic Claude) with minimal code changes.  \n",
      "3. **Observability** – Built‑in callbacks let you log prompts, responses, token usage, and timings, which is essential for debugging and cost monitoring.  \n",
      "4. **Community & Ecosystem** – A large open‑source community contributes integrations, examples, and templates; the official docs include “cookbooks” for dozens of real‑world scenarios.  \n",
      "\n",
      "### Getting started (Python)\n",
      "\n",
      "```python\n",
      "# Install\n",
      "pip install langchain openai chromadb\n",
      "\n",
      "# Simple RAG chain\n",
      "from langchain.llms import OpenAI\n",
      "from langchain.embeddings import OpenAIEmbeddings\n",
      "from langchain.vectorstores import Chroma\n",
      "from langchain.chains import RetrievalQA\n",
      "from langchain.prompts import PromptTemplate\n",
      "\n",
      "# 1. Load docs & create vector store\n",
      "docs = [\"LangChain is a framework ...\", \"It helps build LLM apps ...\"]\n",
      "embeddings = OpenAIEmbeddings()\n",
      "vectorstore = Chroma.from_texts(docs, embeddings)\n",
      "\n",
      "# 2. Define LLM and QA chain\n",
      "llm = OpenAI(model=\"gpt-4\")\n",
      "qa_prompt = PromptTemplate(\n",
      "    template=\"Answer the question based only on the following context:\\n{context}\\n\\nQuestion: {question}\",\n",
      "    input_variables=[\"context\", \"question\"]\n",
      ")\n",
      "qa_chain = RetrievalQA.from_chain_type(\n",
      "    llm=llm,\n",
      "    retriever=vectorstore.as_retriever(),\n",
      "    chain_type_kwargs={\"prompt\": qa_prompt}\n",
      ")\n",
      "\n",
      "# 3. Ask a question\n",
      "answer = qa_chain.run(\"What does LangChain provide for memory?\")\n",
      "print(answer)\n",
      "```\n",
      "\n",
      "### Ecosystem highlights\n",
      "\n",
      "| Category | Popular integrations |\n",
      "|----------|----------------------|\n",
      "| **Vector stores** | FAISS, Chroma, Pinecone, Weaviate, Milvus, Qdrant |\n",
      "| **LLM providers** | OpenAI, Anthropic, Cohere, Azure OpenAI, HuggingFace Inference, Llama‑CPP |\n",
      "| **Agents / tools** | SerpAPI (web search), Python REPL, SQL DB, CSV/Excel, Zapier, Slack, Github, custom HTTP APIs |\n",
      "| **Evaluation** | `langchain.evaluation` utilities for answer grading, truthfulness, and cost tracking |\n",
      "| **Tracing** | LangSmith (official SaaS), LangChain‑Hub, OpenTelemetry support |\n",
      "\n",
      "---\n",
      "\n",
      "**In short:** LangChain is a modular toolkit that lets developers stitch together prompts, external data sources, memory, and tool‑calling logic into robust, production‑ready LLM applications. It abstracts the plumbing so you can focus on the *behaviour* you want the AI to exhibit.\n",
      "✅ PASS\n",
      "\n",
      "Q: what is langgraph?\n",
      "---CALL AGENT---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---CHECK RELEVANCE---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---DECISION: DOCS NOT RELEVANT---\n",
      "no\n",
      "---TRANSFORM QUERY---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---CALL AGENT---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: ## LangGraph – a quick‑look\n",
      "\n",
      "**LangGraph** is an open‑source Python library (maintained by the LangChain team) that lets you **compose, run, and debug complex LLM‑driven workflows as directed graphs**.  \n",
      "Think of it as the “state‑machine / workflow engine” that sits on top of LangChain’s building blocks (LLMs, prompts, tools, memory, etc.) and gives you a clean way to describe multi‑step, branching, looping, and reusable logic.\n",
      "\n",
      "| Aspect | What it means |\n",
      "|--------|----------------|\n",
      "| **Core idea** | Model an LLM application as a **graph** where each node is a piece of logic (LLM call, tool, custom Python function, memory update, etc.) and edges define the next step based on the node’s output. |\n",
      "| **Primary goal** | Make *complex* LLM agents **deterministic, testable, debuggable, and reusable** without scattering control flow across ad‑hoc `if/else` blocks. |\n",
      "| **Relation to LangChain** | LangChain provides the *components* (LLM wrappers, prompts, tools, memory). LangGraph provides the *orchestration* layer that wires those components together in a graph‑based DSL. |\n",
      "| **Target audience** | Engineers building production‑grade agents, chat‑bots, data‑processing pipelines, or any multi‑turn LLM workflow that needs branching, loops, or sub‑workflows. |\n",
      "\n",
      "---\n",
      "\n",
      "## Key Features\n",
      "\n",
      "| Feature | Why it matters | Example |\n",
      "|---------|----------------|---------|\n",
      "| **Declarative graph DSL** | You describe the workflow once; the engine handles execution, state passing, and routing. | ```python\\nfrom langgraph.graph import StateGraph\\n\\ng = StateGraph(state_schema=MyState)\\n``` |\n",
      "| **Typed state & automatic propagation** | A single `state` dict (or Pydantic model) is passed from node to node, making data flow explicit and type‑checked. | `state[\"question\"]`, `state[\"answer\"]` |\n",
      "| **Conditional edges** | Branches are defined by functions that inspect the current state and return the name of the next node. | `g.add_conditional_edges(\"router\", decide_next, [\"search\", \"fallback\"])` |\n",
      "| **Loops & recursion** | Nodes can point back to earlier nodes; the runtime tracks iteration count and can enforce limits. | Re‑ask the user until a valid answer is obtained. |\n",
      "| **Sub‑graphs (nested graphs)** | Reuse a whole graph as a single node, enabling modular, composable components. | A “retrieval‑augmented generation” sub‑graph used in many agents. |\n",
      "| **Built‑in tool‑calling & function calling** | Seamlessly integrate LangChain tools, OpenAI function calling, or arbitrary Python functions as graph nodes. | `g.add_node(\"search\", tool_node(search_tool))` |\n",
      "| **Memory integration** | Attach LangChain memory objects (ConversationBuffer, VectorStoreRetriever, etc.) that automatically update the shared state. | `state[\"history\"] = memory.load_memory_variables(state)` |\n",
      "| **Persistable runs** | Execution can be logged to a DB (SQLite, Postgres, etc.) so you can replay, audit, or resume a run. | `graph.run(state, config={\"recorder\": SqliteRecorder(...)})` |\n",
      "| **Visualization** | Auto‑generate a Mermaid/GraphViz diagram of the graph for docs or debugging. | `graph.get_graph().draw_mermaid()` |\n",
      "| **Parallel execution** | Nodes that don’t depend on each other can be run concurrently (via `asyncio` or thread pools). | Simultaneous calls to multiple search APIs. |\n",
      "| **Error handling & retries** | Define fallback nodes or retry policies per node. | If the LLM times out, go to `\"fallback\"` node. |\n",
      "| **Streaming & callbacks** | Hook into token‑level streaming, logging, tracing (LangChain callbacks, OpenTelemetry, etc.). | Show partial LLM output in a UI while the graph runs. |\n",
      "\n",
      "---\n",
      "\n",
      "## Typical Use‑Cases\n",
      "\n",
      "| Use‑Case | How LangGraph helps |\n",
      "|----------|---------------------|\n",
      "| **Multi‑turn conversational agents** | Keep the conversation history in state, route between “question understanding”, “tool selection”, “answer generation”, and “clarification” nodes. |\n",
      "| **Tool‑augmented RAG pipelines** | A sub‑graph that first retrieves relevant docs, then does a synthesis step, then optionally calls a calculator tool. |\n",
      "| **Decision‑tree bots** | Encode a decision tree as a graph where each node asks a question and branches based on the answer. |\n",
      "| **Workflow automation** | Combine LLM‑generated instructions with external APIs (e.g., schedule a meeting, send email) in a deterministic pipeline. |\n",
      "| **Iterative refinement** | Loop a “draft → critique → rewrite” cycle until a quality metric is met. |\n",
      "| **Complex data extraction** | Parse a document, call a validator, then branch to “re‑ask” or “store” nodes. |\n",
      "| **Testing & debugging** | Because each node is isolated, you can unit‑test them, mock LLM calls, and replay a run from any checkpoint. |\n",
      "| **Production monitoring** | Persisted run logs let you monitor latency, error rates, and token usage per node. |\n",
      "\n",
      "---\n",
      "\n",
      "## Minimal Example\n",
      "\n",
      "Below is a **self‑contained** example that shows the main concepts:\n",
      "\n",
      "```python\n",
      "# -------------------------------------------------\n",
      "# 1️⃣ Install (once)\n",
      "# -------------------------------------------------\n",
      "# pip install \"langgraph[all]\" \"langchain-openai\"\n",
      "\n",
      "# -------------------------------------------------\n",
      "# 2️⃣ Imports & state definition\n",
      "# -------------------------------------------------\n",
      "from typing import TypedDict, Literal\n",
      "from langgraph.graph import StateGraph, END\n",
      "from langchain_openai import ChatOpenAI\n",
      "from langchain.prompts import PromptTemplate\n",
      "\n",
      "class AgentState(TypedDict):\n",
      "    \"\"\"State that travels through the graph.\"\"\"\n",
      "    user_input: str          # what the user said\n",
      "    intent: str | None       # detected intent (or None)\n",
      "    answer: str | None       # final answer to return\n",
      "    step: int                # for demo loops\n",
      "\n",
      "# -------------------------------------------------\n",
      "# 3️⃣ Define the nodes (functions that receive & return state)\n",
      "# -------------------------------------------------\n",
      "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
      "\n",
      "def classify_intent(state: AgentState) -> AgentState:\n",
      "    \"\"\"LLM decides if the user is asking a math question or a general chat.\"\"\"\n",
      "    prompt = PromptTemplate.from_template(\n",
      "        \"Classify the user request as either 'math' or 'chat'.\\nUser: {user_input}\"\n",
      "    )\n",
      "    resp = llm.invoke(prompt.format_messages(user_input=state[\"user_input\"]))\n",
      "    state[\"intent\"] = resp.content.strip().lower()\n",
      "    return state\n",
      "\n",
      "def math_answer(state: AgentState) -> AgentState:\n",
      "    \"\"\"Call the LLM as a calculator.\"\"\"\n",
      "    prompt = PromptTemplate.from_template(\n",
      "        \"You are a calculator. Solve the problem and give only the numeric answer.\\n{user_input}\"\n",
      "    )\n",
      "    resp = llm.invoke(prompt.format_messages(user_input=state[\"user_input\"]))\n",
      "    state[\"answer\"] = resp.content.strip()\n",
      "    return state\n",
      "\n",
      "def chat_answer(state: AgentState) -> AgentState:\n",
      "    \"\"\"General chat response.\"\"\"\n",
      "    prompt = PromptTemplate.from_template(\n",
      "        \"You are a friendly assistant. Respond to the user politely.\\nUser: {user_input}\"\n",
      "    )\n",
      "    resp = llm.invoke(prompt.format_messages(user_input=state[\"user_input\"]))\n",
      "    state[\"answer\"] = resp.content.strip()\n",
      "    return state\n",
      "\n",
      "def route(state: AgentState) -> Literal[\"math\"] | Literal[\"chat\"]:\n",
      "    \"\"\"Conditional edge selector based on intent.\"\"\"\n",
      "    return \"math\" if state[\"intent\"] == \"math\" else \"chat\"\n",
      "\n",
      "# -------------------------------------------------\n",
      "# 4️⃣ Build the graph\n",
      "# -------------------------------------------------\n",
      "graph = StateGraph(AgentState)\n",
      "\n",
      "graph.add_node(\"classify\", classify_intent)   # first step\n",
      "graph.add_node(\"math\", math_answer)          # math branch\n",
      "graph.add_node(\"chat\", chat_answer)          # chat branch\n",
      "\n",
      "# Conditional routing after classification\n",
      "graph.add_conditional_edges(\n",
      "    \"classify\",\n",
      "    route,                # function that decides the next node\n",
      "    [\"math\", \"chat\"],     # possible destinations\n",
      ")\n",
      "\n",
      "# Both leaf nodes terminate the run\n",
      "graph.add_edge(\"math\", END)\n",
      "graph.add_edge(\"chat\", END)\n",
      "\n",
      "# Compile the graph into a runnable object\n",
      "app = graph.compile()\n",
      "\n",
      "# -------------------------------------------------\n",
      "# 5️⃣ Run it\n",
      "# -------------------------------------------------\n",
      "input_state = AgentState(user_input=\"What is 12 * 7?\", intent=None, answer=None, step=0)\n",
      "result = app.invoke(input_state)\n",
      "\n",
      "print(\"🗨️ Final answer:\", result[\"answer\"])\n",
      "# -------------------------------------------------\n",
      "```\n",
      "\n",
      "**What this shows**\n",
      "\n",
      "1. **Typed state** (`AgentState`) flows through every node.  \n",
      "2. **Nodes** are ordinary Python callables that can call any LangChain component.  \n",
      "3. **Conditional edges** (`add_conditional_edges`) let the graph decide the next node based on runtime data.  \n",
      "4. The **graph is compiled** once and then can be invoked many times (or served via an API).  \n",
      "\n",
      "---\n",
      "\n",
      "## How to Get Started\n",
      "\n",
      "| Step | Action |\n",
      "|------|--------|\n",
      "| **1. Install** | `pip install \"langgraph[all]\"` (the `[all]` extra pulls in LangChain, OpenAI, and optional async tools). |\n",
      "| **2. Read the docs** | <https://langchain-ai.github.io/langgraph/> – especially the *Getting Started* and *Graph DSL* sections. |\n",
      "| **3. Look at examples** | The repo’s `examples/` folder contains ready‑made graphs for RAG, tool‑calling, and multi‑agent orchestration. |\n",
      "| **4. Pick a storage backend** (optional) | For production runs you’ll likely want a recorder: `from langgraph.checkpoint.memory import MemorySaver` or `SqliteCheckpointSaver`. |\n",
      "| **5. Visualize** | `graph.get_graph().draw_mermaid()` gives you a Mermaid diagram you can embed in docs or notebooks. |\n",
      "| **6. Deploy** | Wrap the compiled graph in a FastAPI/Starlette endpoint, or use LangServe (`langserve run app:app`) for a ready‑to‑use HTTP server. |\n",
      "\n",
      "---\n",
      "\n",
      "## Frequently Asked Questions\n",
      "\n",
      "| Question | Answer |\n",
      "|----------|--------|\n",
      "| **Do I have to use LangChain?** | No, but LangGraph’s node helpers (`tool_node`, `llm_node`, etc.) are thin wrappers around LangChain objects. You can call any Python function or external service directly. |\n",
      "| **Can I run graphs asynchronously?** | Yes. Define `async def` nodes and compile with `graph.compile(checkpointer=..., executor=AsyncExecutor())`. The engine will `await` each async node. |\n",
      "| **Is state persisted across runs?** | By default the state lives only for the duration of a single run. If you need persistence (e.g., to resume after a crash), attach a checkpoint store (`SqliteCheckpointSaver`, `RedisSaver`, etc.). |\n",
      "| **How does LangGraph differ from LangChain’s “Agent” classes?** | Agents in LangChain are a *single* LLM‑driven loop with tool‑calling logic baked in. LangGraph lets you **explicitly model** that loop as a node, and you can add *any* number of additional branches, loops, or sub‑graphs beyond the classic “agent → tool → observation → repeat” pattern. |\n",
      "| **Is there support for non‑LLM nodes?** | Absolutely. Anything that takes a state dict and returns a (possibly mutated) state dict can be a node—database queries, HTTP calls, file I/O, etc. |\n",
      "| **Can I combine multiple graphs?** | Yes. A graph can be added as a node inside another graph (`graph.add_node(\"sub\", subgraph)`). This encourages modular design. |\n",
      "| **What about monitoring & observability?** | LangGraph integrates with LangChain’s callback system, so you can plug in OpenTelemetry, LangChain’s built‑in `ConsoleCallbackHandler`, or custom loggers to capture start/end timestamps, token usage, and errors per node. |\n",
      "\n",
      "---\n",
      "\n",
      "## TL;DR Summary\n",
      "\n",
      "- **LangGraph** = **graph‑oriented workflow engine** built for LLM‑centric applications.  \n",
      "- It **orchestrates** LangChain components (LLMs, tools, memory) via **typed state** and **conditional/looping edges**.  \n",
      "- Ideal for **multi‑step agents, RAG pipelines, decision trees, and any production‑grade LLM app** that needs clear control flow, debuggability, and reusability.  \n",
      "- You define a graph once, compile it, and then invoke it many times—just like calling a function, but with built‑in support for branching, loops, persistence, and visualization.  \n",
      "\n",
      "If you’re already using LangChain and find your agent logic getting tangled in `if/else` blocks, give LangGraph a try; it will make the flow explicit, testable, and far easier to scale. Happy graph‑building!\n",
      "✅ PASS\n",
      "\n",
      "Q: What is the weather in Delhi?\n",
      "---CALL AGENT---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---CHECK RELEVANCE---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---DECISION: DOCS RELEVANT---\n",
      "---GENERATE---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: The temperature in Delhi is 2.4 °C.\n",
      "✅ PASS\n",
      "\n",
      "Final accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# ===== CI ENTRYPOINT =====\n",
    "\n",
    "results, accuracy = run_eval(EVAL_SET)\n",
    "\n",
    "# Fail CI if accuracy is too low\n",
    "if accuracy < 0.7:\n",
    "    raise Exception(\"Evaluation accuracy below threshold\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agenticaiworkspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
