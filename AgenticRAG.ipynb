{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19ab913a",
   "metadata": {},
   "source": [
    "### Agentic RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da2985b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"]=os.getenv(\"GROQ_API_KEY\")\n",
    "os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c96f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitterss\n",
    "from langchain_huggingface import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "706e2335",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Document(metadata={'source': 'https://langchain-ai.github.io/langgraph/tutorials/introduction/', 'title': 'Redirecting...', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nRedirecting...\\n\\n\\n\\n\\n\\n\\nRedirecting...\\n\\n\\n')],\n",
       " [Document(metadata={'source': 'https://langchain-ai.github.io/langgraph/tutorials/workflows/', 'title': 'Redirecting...', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nRedirecting...\\n\\n\\n\\n\\n\\n\\nRedirecting...\\n\\n\\n')],\n",
       " [Document(metadata={'source': 'https://langchain-ai.github.io/langgraph/how-tos/map-reduce/', 'title': 'Redirecting...', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nRedirecting...\\n\\n\\n\\n\\n\\n\\nRedirecting...\\n\\n\\n')]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls=[\n",
    "    \"https://langchain-ai.github.io/langgraph/tutorials/introduction/\",\n",
    "    \"https://langchain-ai.github.io/langgraph/tutorials/workflows/\",\n",
    "    \"https://langchain-ai.github.io/langgraph/how-tos/map-reduce/\"\n",
    "]\n",
    "\n",
    "docs=[WebBaseLoader(url).load() for url in urls]\n",
    "docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a08aee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=100\n",
    ")\n",
    "\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "## Add alll these text to vectordb\n",
    "\n",
    "vectorstore=FAISS.from_documents(\n",
    "    documents=doc_splits,\n",
    "    embedding=HuggingFaceEmbeddings()\n",
    ")\n",
    "\n",
    "\n",
    "retriever=vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ea4cf11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='039dbdae-a349-4a8a-8c13-52b618adcde4', metadata={'source': 'https://langchain-ai.github.io/langgraph/tutorials/introduction/', 'title': 'Redirecting...', 'language': 'en'}, page_content='Redirecting...\\n\\n\\n\\n\\n\\n\\nRedirecting...'),\n",
       " Document(id='9b9200d4-68e9-4561-9fde-029229d9993b', metadata={'source': 'https://langchain-ai.github.io/langgraph/tutorials/workflows/', 'title': 'Redirecting...', 'language': 'en'}, page_content='Redirecting...\\n\\n\\n\\n\\n\\n\\nRedirecting...'),\n",
       " Document(id='013efafc-5928-4d61-b868-c060dfa6b4b4', metadata={'source': 'https://langchain-ai.github.io/langgraph/how-tos/map-reduce/', 'title': 'Redirecting...', 'language': 'en'}, page_content='Redirecting...\\n\\n\\n\\n\\n\\n\\nRedirecting...')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"what is langgraph\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e0a7a99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Retriever To Retriever Tools\n",
    "from langchain_core.tools import create_retriever_tool\n",
    "\n",
    "\n",
    "\n",
    "retriever_tool=create_retriever_tool(\n",
    "    retriever,\n",
    "    \"retriever_vector_db_blog\",\n",
    "    \"Search and run information about Langgraph\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "abcc6a53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructuredTool(name='retriever_vector_db_blog', description='Search and run information about Langgraph', args_schema=<class 'langchain_core.tools.retriever.RetrieverInput'>, func=<function create_retriever_tool.<locals>.func at 0x000002485840B560>, coroutine=<function create_retriever_tool.<locals>.afunc at 0x00000248198E1E40>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever_tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ac6e08",
   "metadata": {},
   "source": [
    "### Langchain Blogs- Seperate Vector Store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c2ed2c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/', 'title': 'LangChain overview - Docs by LangChain', 'description': 'LangChain is an open source framework with a pre-built agent architecture and integrations for any model or tool — so you can build agents that adapt as fast as the ecosystem evolves', 'language': 'en'}, page_content='LangChain overview - Docs by LangChainSkip to main contentDocs by LangChain home pageOpen sourceSearch...⌘KAsk AIGitHubTry LangSmithTry LangSmithSearch...NavigationLangChain overviewDeep AgentsLangChainLangGraphIntegrationsLearnReferenceContributePythonOverviewGet startedInstallQuickstartChangelogPhilosophyCore componentsAgentsModelsMessagesToolsShort-term memoryStreamingStructured outputMiddlewareOverviewPrebuilt middlewareCustom middlewareAdvanced usageGuardrailsRuntimeContext engineeringModel Context Protocol (MCP)Human-in-the-loopMulti-agentRetrievalLong-term memoryAgent developmentLangSmith StudioTestAgent Chat UIDeploy with LangSmithDeploymentObservabilityOn this page Create an agent Core benefitsLangChain overviewCopy pageLangChain is an open source framework with a pre-built agent architecture and integrations for any model or tool — so you can build agents that adapt as fast as the ecosystem evolvesCopy pageLangChain is the easy way to start building completely custom agents and applications powered by LLMs.\\nWith under 10 lines of code, you can connect to OpenAI, Anthropic, Google, and more.\\nLangChain provides a pre-built agent architecture and model integrations to help you get started quickly and seamlessly incorporate LLMs into your agents and applications.\\nLangChain vs. LangGraph vs. Deep AgentsIf you are looking to build an agent, we recommend you start with Deep Agents which comes “batteries-included”, with modern features like automatic compression of long conversations, a virtual filesystem, and subagent-spawning for managing and isolating context.Deep Agents are implementations of LangChain agents. If you don’t need these capabilities or would like to customize your own for your agents and autonomous applications, start with LangChain.Use LangGraph, our low-level agent orchestration framework and runtime, when you have more advanced needs that require a combination of deterministic and agentic workflows and heavy customization.\\nLangChain agents are built on top of LangGraph in order to provide durable execution, streaming, human-in-the-loop, persistence, and more. You do not need to know LangGraph for basic LangChain agent usage.\\nWe recommend you use LangChain if you want to quickly build agents and autonomous applications.\\n\\u200b Create an agent\\nCopy# pip install -qU langchain \"langchain[anthropic]\"\\nfrom langchain.agents import create_agent\\n\\ndef get_weather(city: str) -> str:\\n    \"\"\"Get weather for a given city.\"\"\"\\n    return f\"It\\'s always sunny in {city}!\"\\n\\nagent = create_agent(\\n    model=\"claude-sonnet-4-5-20250929\",\\n    tools=[get_weather],\\n    system_prompt=\"You are a helpful assistant\",\\n)\\n\\n# Run the agent\\nagent.invoke(\\n    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}\\n)\\n\\nSee the Installation instructions and Quickstart guide to get started building your own agents and applications with LangChain.\\n\\u200b Core benefits\\nStandard model interfaceDifferent providers have unique APIs for interacting with models, including the format of responses. LangChain standardizes how you interact with models so that you can seamlessly swap providers and avoid lock-in.Learn moreEasy to use, highly flexible agentLangChain’s agent abstraction is designed to be easy to get started with, letting you build a simple agent in under 10 lines of code. But it also provides enough flexibility to allow you to do all the context engineering your heart desires.Learn moreBuilt on top of LangGraphLangChain’s agents are built on top of LangGraph. This allows us to take advantage of LangGraph’s durable execution, human-in-the-loop support, persistence, and more.Learn moreDebug with LangSmithGain deep visibility into complex agent behavior with visualization tools that trace execution paths, capture state transitions, and provide detailed runtime metrics.Learn more\\n\\nEdit this page on GitHub or file an issue.\\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.Was this page helpful?YesNoInstall LangChainNext⌘IDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyHomeAboutCareersBloggithubxlinkedinyoutubePowered by\\n')],\n",
       " [Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/chatbot/', 'title': 'Build a RAG agent with LangChain - Docs by LangChain', 'language': 'en'}, page_content='Build a RAG agent with LangChain - Docs by LangChainSkip to main contentDocs by LangChain home pageOpen sourceSearch...‚åòKAsk AIGitHubTry LangSmithTry LangSmithSearch...NavigationLangChainBuild a RAG agent with LangChainDeep AgentsLangChainLangGraphIntegrationsLearnReferenceContributePythonLearnTutorialsDeep AgentsLangChainSemantic searchRAG agentSQL agentVoice agentMulti-agentLangGraphConceptual overviewsLangChain vs. LangGraph vs. Deep AgentsComponent architectureMemoryContextGraph APIFunctional APIAdditional resourcesLangChain AcademyCase studiesGet helpOn this pageOverviewConceptsPreviewSetupInstallationLangSmithComponents1. IndexingLoading documentsSplitting documentsStoring documents2. Retrieval and generationRAG agentsRAG chainsNext stepsTutorialsLangChainBuild a RAG agent with LangChainCopy pageCopy page‚ÄãOverview\\nOne of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.\\nThis tutorial will show how to build a simple Q&A application over an unstructured text data source. We will demonstrate:\\n\\nA RAG agent that executes searches with a simple tool. This is a good general-purpose implementation.\\nA two-step RAG chain that uses just a single LLM call per query. This is a fast and effective method for simple queries.\\n\\n‚ÄãConcepts\\nWe will cover the following concepts:\\n\\n\\nIndexing: a pipeline for ingesting data from a source and indexing it. This usually happens in a separate process.\\n\\n\\nRetrieval and generation: the actual RAG process, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.\\n\\n\\nOnce we‚Äôve indexed our data, we will use an agent as our orchestration framework to implement the retrieval and generation steps.\\nThe indexing portion of this tutorial will largely follow the semantic search tutorial.If your data is already available for search (i.e., you have a function to execute a search), or you‚Äôre comfortable with the content from that tutorial, feel free to skip to the section on retrieval and generation\\n‚ÄãPreview\\nIn this guide we‚Äôll build an app that answers questions about the website‚Äôs content. The specific website we will use is the LLM Powered Autonomous Agents blog post by Lilian Weng, which allows us to ask questions about the contents of the post.\\nWe can create a simple indexing pipeline and RAG chain to do this in ~40 lines of code. See below for the full code snippet:\\nExpand for full code snippetCopyimport bs4\\nfrom langchain.agents import AgentState, create_agent\\nfrom langchain_community.document_loaders import WebBaseLoader\\nfrom langchain.messages import MessageLikeRepresentation\\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\n\\n# Load and chunk contents of the blog\\nloader = WebBaseLoader(\\n    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\\n    bs_kwargs=dict(\\n        parse_only=bs4.SoupStrainer(\\n            class_=(\"post-content\", \"post-title\", \"post-header\")\\n        )\\n    ),\\n)\\ndocs = loader.load()\\n\\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\\nall_splits = text_splitter.split_documents(docs)\\n\\n# Index chunks\\n_ = vector_store.add_documents(documents=all_splits)\\n\\n# Construct a tool for retrieving context\\n@tool(response_format=\"content_and_artifact\")\\ndef retrieve_context(query: str):\\n    \"\"\"Retrieve information to help answer a query.\"\"\"\\n    retrieved_docs = vector_store.similarity_search(query, k=2)\\n    serialized = \"\\\\n\\\\n\".join(\\n        (f\"Source: {doc.metadata}\\\\nContent: {doc.page_content}\")\\n        for doc in retrieved_docs\\n    )\\n    return serialized, retrieved_docs\\n\\ntools = [retrieve_context]\\n# If desired, specify custom instructions\\nprompt = (\\n    \"You have access to a tool that retrieves context from a blog post. \"\\n    \"Use the tool to help answer user queries.\"\\n)\\nagent = create_agent(model, tools, system_prompt=prompt)\\nCopyquery = \"What is task decomposition?\"\\nfor step in agent.stream(\\n    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\\n    stream_mode=\"values\",\\n):\\n    step[\"messages\"][-1].pretty_print()\\nCopy================================ Human Message =================================\\n\\nWhat is task decomposition?\\n================================== Ai Message ==================================\\nTool Calls:\\n  retrieve_context (call_xTkJr8njRY0geNz43ZvGkX0R)\\n Call ID: call_xTkJr8njRY0geNz43ZvGkX0R\\n  Args:\\n    query: task decomposition\\n================================= Tool Message =================================\\nName: retrieve_context\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Task decomposition can be done by...\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Component One: Planning...\\n================================== Ai Message ==================================\\n\\nTask decomposition refers to...\\nCheck out the LangSmith trace.\\n‚ÄãSetup\\n‚ÄãInstallation\\nThis tutorial requires these langchain dependencies:\\npipuvCopypip install langchain langchain-text-splitters langchain-community bs4\\n\\nFor more details, see our Installation guide.\\n‚ÄãLangSmith\\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with LangSmith.\\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\\nCopyexport LANGSMITH_TRACING=\"true\"\\nexport LANGSMITH_API_KEY=\"...\"\\n\\nOr, set them in Python:\\nCopyimport getpass\\nimport os\\n\\nos.environ[\"LANGSMITH_TRACING\"] = \"true\"\\nos.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\\n\\n‚ÄãComponents\\nWe will need to select three components from LangChain‚Äôs suite of integrations.\\nSelect a chat model:\\n OpenAI Anthropic Azure Google Gemini AWS Bedrock HuggingFace\\uf8ffüëâ Read the OpenAI chat model integration docsCopypip install -U \"langchain[openai]\"\\ninit_chat_modelModel ClassCopyimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\\n\\nmodel = init_chat_model(\"gpt-4.1\")\\n\\uf8ffüëâ Read the Anthropic chat model integration docsCopypip install -U \"langchain[anthropic]\"\\ninit_chat_modelModel ClassCopyimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"ANTHROPIC_API_KEY\"] = \"sk-...\"\\n\\nmodel = init_chat_model(\"claude-sonnet-4-5-20250929\")\\n\\uf8ffüëâ Read the Azure chat model integration docsCopypip install -U \"langchain[openai]\"\\ninit_chat_modelModel ClassCopyimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"AZURE_OPENAI_API_KEY\"] = \"...\"\\nos.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"...\"\\nos.environ[\"OPENAI_API_VERSION\"] = \"2025-03-01-preview\"\\n\\nmodel = init_chat_model(\\n    \"azure_openai:gpt-4.1\",\\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\\n)\\n\\uf8ffüëâ Read the Google GenAI chat model integration docsCopypip install -U \"langchain[google-genai]\"\\ninit_chat_modelModel ClassCopyimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"GOOGLE_API_KEY\"] = \"...\"\\n\\nmodel = init_chat_model(\"google_genai:gemini-2.5-flash-lite\")\\n\\uf8ffüëâ Read the AWS Bedrock chat model integration docsCopypip install -U \"langchain[aws]\"\\ninit_chat_modelModel ClassCopyfrom langchain.chat_models import init_chat_model\\n\\n# Follow the steps here to configure your credentials:\\n# https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html\\n\\nmodel = init_chat_model(\\n    \"anthropic.claude-3-5-sonnet-20240620-v1:0\",\\n    model_provider=\"bedrock_converse\",\\n)\\n\\uf8ffüëâ Read the HuggingFace chat model integration docsCopypip install -U \"langchain[huggingface]\"\\ninit_chat_modelModel ClassCopyimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"hf_...\"\\n\\nmodel = init_chat_model(\\n    \"microsoft/Phi-3-mini-4k-instruct\",\\n    model_provider=\"huggingface\",\\n    temperature=0.7,\\n    max_tokens=1024,\\n)\\n\\nSelect an embeddings model:\\n OpenAI Azure Google Gemini Google Vertex AWS HuggingFace Ollama Cohere MistralAI Nomic NVIDIA Voyage AI IBM watsonx Fake IsaacusCopypip install -U \"langchain-openai\"\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"OPENAI_API_KEY\"):\\n    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\\n\\nfrom langchain_openai import OpenAIEmbeddings\\n\\nembeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\\nCopypip install -U \"langchain-openai\"\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"AZURE_OPENAI_API_KEY\"):\\n    os.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for Azure: \")\\n\\nfrom langchain_openai import AzureOpenAIEmbeddings\\n\\nembeddings = AzureOpenAIEmbeddings(\\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\\n)\\nCopypip install -qU langchain-google-genai\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"GOOGLE_API_KEY\"):\\n    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")\\n\\nfrom langchain_google_genai import GoogleGenerativeAIEmbeddings\\n\\nembeddings = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\")\\nCopypip install -qU langchain-google-vertexai\\nCopyfrom langchain_google_vertexai import VertexAIEmbeddings\\n\\nembeddings = VertexAIEmbeddings(model=\"text-embedding-005\")\\nCopypip install -qU langchain-aws\\nCopyfrom langchain_aws import BedrockEmbeddings\\n\\nembeddings = BedrockEmbeddings(model_id=\"amazon.titan-embed-text-v2:0\")\\nCopypip install -qU langchain-huggingface\\nCopyfrom langchain_huggingface import HuggingFaceEmbeddings\\n\\nembeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\\nCopypip install -qU langchain-ollama\\nCopyfrom langchain_ollama import OllamaEmbeddings\\n\\nembeddings = OllamaEmbeddings(model=\"llama3\")\\nCopypip install -qU langchain-cohere\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"COHERE_API_KEY\"):\\n    os.environ[\"COHERE_API_KEY\"] = getpass.getpass(\"Enter API key for Cohere: \")\\n\\nfrom langchain_cohere import CohereEmbeddings\\n\\nembeddings = CohereEmbeddings(model=\"embed-english-v3.0\")\\nCopypip install -qU langchain-mistralai\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"MISTRALAI_API_KEY\"):\\n    os.environ[\"MISTRALAI_API_KEY\"] = getpass.getpass(\"Enter API key for MistralAI: \")\\n\\nfrom langchain_mistralai import MistralAIEmbeddings\\n\\nembeddings = MistralAIEmbeddings(model=\"mistral-embed\")\\nCopypip install -qU langchain-nomic\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"NOMIC_API_KEY\"):\\n    os.environ[\"NOMIC_API_KEY\"] = getpass.getpass(\"Enter API key for Nomic: \")\\n\\nfrom langchain_nomic import NomicEmbeddings\\n\\nembeddings = NomicEmbeddings(model=\"nomic-embed-text-v1.5\")\\nCopypip install -qU langchain-nvidia-ai-endpoints\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"NVIDIA_API_KEY\"):\\n    os.environ[\"NVIDIA_API_KEY\"] = getpass.getpass(\"Enter API key for NVIDIA: \")\\n\\nfrom langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\\n\\nembeddings = NVIDIAEmbeddings(model=\"NV-Embed-QA\")\\nCopypip install -qU langchain-voyageai\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"VOYAGE_API_KEY\"):\\n    os.environ[\"VOYAGE_API_KEY\"] = getpass.getpass(\"Enter API key for Voyage AI: \")\\n\\nfrom langchain-voyageai import VoyageAIEmbeddings\\n\\nembeddings = VoyageAIEmbeddings(model=\"voyage-3\")\\nCopypip install -qU langchain-ibm\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"WATSONX_APIKEY\"):\\n    os.environ[\"WATSONX_APIKEY\"] = getpass.getpass(\"Enter API key for IBM watsonx: \")\\n\\nfrom langchain_ibm import WatsonxEmbeddings\\n\\nembeddings = WatsonxEmbeddings(\\n    model_id=\"ibm/slate-125m-english-rtrvr\",\\n    url=\"https://us-south.ml.cloud.ibm.com\",\\n    project_id=\"<WATSONX PROJECT_ID>\",\\n)\\nCopypip install -qU langchain-core\\nCopyfrom langchain_core.embeddings import DeterministicFakeEmbedding\\n\\nembeddings = DeterministicFakeEmbedding(size=4096)\\nCopypip install -qU langchain-isaacus\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"ISAACUS_API_KEY\"):\\nos.environ[\"ISAACUS_API_KEY\"] = getpass.getpass(\"Enter API key for Isaacus: \")\\n\\nfrom langchain_isaacus import IsaacusEmbeddings\\n\\nembeddings = IsaacusEmbeddings(model=\"kanon-2-embedder\")\\n\\nSelect a vector store:\\n In-memory Amazon OpenSearch AstraDB Chroma FAISS Milvus MongoDB PGVector PGVectorStore Pinecone QdrantCopypip install -U \"langchain-core\"\\nCopyfrom langchain_core.vectorstores import InMemoryVectorStore\\n\\nvector_store = InMemoryVectorStore(embeddings)\\nCopypip install -qU  boto3\\nCopyfrom opensearchpy import RequestsHttpConnection\\n\\nservice = \"es\"  # must set the service as \\'es\\'\\nregion = \"us-east-2\"\\ncredentials = boto3.Session(\\n    aws_access_key_id=\"xxxxxx\", aws_secret_access_key=\"xxxxx\"\\n).get_credentials()\\nawsauth = AWS4Auth(\"xxxxx\", \"xxxxxx\", region, service, session_token=credentials.token)\\n\\nvector_store = OpenSearchVectorSearch.from_documents(\\n    docs,\\n    embeddings,\\n    opensearch_url=\"host url\",\\n    http_auth=awsauth,\\n    timeout=300,\\n    use_ssl=True,\\n    verify_certs=True,\\n    connection_class=RequestsHttpConnection,\\n    index_name=\"test-index\",\\n)\\nCopypip install -U \"langchain-astradb\"\\nCopyfrom langchain_astradb import AstraDBVectorStore\\n\\nvector_store = AstraDBVectorStore(\\n    embedding=embeddings,\\n    api_endpoint=ASTRA_DB_API_ENDPOINT,\\n    collection_name=\"astra_vector_langchain\",\\n    token=ASTRA_DB_APPLICATION_TOKEN,\\n    namespace=ASTRA_DB_NAMESPACE,\\n)\\nCopypip install -qU langchain-chroma\\nCopyfrom langchain_chroma import Chroma\\n\\nvector_store = Chroma(\\n    collection_name=\"example_collection\",\\n    embedding_function=embeddings,\\n    persist_directory=\"./chroma_langchain_db\",  # Where to save data locally, remove if not necessary\\n)\\nCopypip install -qU langchain-community faiss-cpu\\nCopyimport faiss\\nfrom langchain_community.docstore.in_memory import InMemoryDocstore\\nfrom langchain_community.vectorstores import FAISS\\n\\nembedding_dim = len(embeddings.embed_query(\"hello world\"))\\nindex = faiss.IndexFlatL2(embedding_dim)\\n\\nvector_store = FAISS(\\n    embedding_function=embeddings,\\n    index=index,\\n    docstore=InMemoryDocstore(),\\n    index_to_docstore_id={},\\n)\\nCopypip install -qU langchain-milvus\\nCopyfrom langchain_milvus import Milvus\\n\\nURI = \"./milvus_example.db\"\\n\\nvector_store = Milvus(\\n    embedding_function=embeddings,\\n    connection_args={\"uri\": URI},\\n    index_params={\"index_type\": \"FLAT\", \"metric_type\": \"L2\"},\\n)\\nCopypip install -qU langchain-mongodb\\nCopyfrom langchain_mongodb import MongoDBAtlasVectorSearch\\n\\nvector_store = MongoDBAtlasVectorSearch(\\n    embedding=embeddings,\\n    collection=MONGODB_COLLECTION,\\n    index_name=ATLAS_VECTOR_SEARCH_INDEX_NAME,\\n    relevance_score_fn=\"cosine\",\\n)\\nCopypip install -qU langchain-postgres\\nCopyfrom langchain_postgres import PGVector\\n\\nvector_store = PGVector(\\n    embeddings=embeddings,\\n    collection_name=\"my_docs\",\\n    connection=\"postgresql+psycopg://...\",\\n)\\nCopypip install -qU langchain-postgres\\nCopyfrom langchain_postgres import PGEngine, PGVectorStore\\n\\npg_engine = PGEngine.from_connection_string(\\n    url=\"postgresql+psycopg://...\"\\n)\\n\\nvector_store = PGVectorStore.create_sync(\\n    engine=pg_engine,\\n    table_name=\\'test_table\\',\\n    embedding_service=embeddings\\n)\\nCopypip install -qU langchain-pinecone\\nCopyfrom langchain_pinecone import PineconeVectorStore\\nfrom pinecone import Pinecone\\n\\npc = Pinecone(api_key=...)\\nindex = pc.Index(index_name)\\n\\nvector_store = PineconeVectorStore(embedding=embeddings, index=index)\\nCopypip install -qU langchain-qdrant\\nCopyfrom qdrant_client.models import Distance, VectorParams\\nfrom langchain_qdrant import QdrantVectorStore\\nfrom qdrant_client import QdrantClient\\n\\nclient = QdrantClient(\":memory:\")\\n\\nvector_size = len(embeddings.embed_query(\"sample text\"))\\n\\nif not client.collection_exists(\"test\"):\\n    client.create_collection(\\n        collection_name=\"test\",\\n        vectors_config=VectorParams(size=vector_size, distance=Distance.COSINE)\\n    )\\nvector_store = QdrantVectorStore(\\n    client=client,\\n    collection_name=\"test\",\\n    embedding=embeddings,\\n)\\n\\n‚Äã1. Indexing\\nThis section is an abbreviated version of the content in the semantic search tutorial.If your data is already indexed and available for search (i.e., you have a function to execute a search), or if you‚Äôre comfortable with document loaders, embeddings, and vector stores, feel free to skip to the next section on retrieval and generation.\\nIndexing commonly works as follows:\\n\\nLoad: First we need to load our data. This is done with Document Loaders.\\nSplit: Text splitters break large Documents into smaller chunks. This is useful both for indexing data and passing it into a model, as large chunks are harder to search over and won‚Äôt fit in a model‚Äôs finite context window.\\nStore: We need somewhere to store and index our splits, so that they can be searched over later. This is often done using a VectorStore and Embeddings model.\\n\\n\\n‚ÄãLoading documents\\nWe need to first load the blog post contents. We can use DocumentLoaders for this, which are objects that load in data from a source and return a list of Document objects.\\nIn this case we‚Äôll use the WebBaseLoader, which uses urllib to load HTML from web URLs and BeautifulSoup to parse it to text. We can customize the HTML -> text parsing by passing in parameters into the BeautifulSoup parser via bs_kwargs (see BeautifulSoup docs). In this case only HTML tags with class ‚Äúpost-content‚Äù, ‚Äúpost-title‚Äù, or ‚Äúpost-header‚Äù are relevant, so we‚Äôll remove all others.\\nCopyimport bs4\\nfrom langchain_community.document_loaders import WebBaseLoader\\n\\n# Only keep post title, headers, and content from the full HTML.\\nbs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))\\nloader = WebBaseLoader(\\n    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\\n    bs_kwargs={\"parse_only\": bs4_strainer},\\n)\\ndocs = loader.load()\\n\\nassert len(docs) == 1\\nprint(f\"Total characters: {len(docs[0].page_content)}\")\\n\\nCopyTotal characters: 43131\\n\\nCopyprint(docs[0].page_content[:500])\\n\\nCopy      LLM Powered Autonomous Agents\\n\\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn\\n\\nGo deeper\\nDocumentLoader: Object that loads data from a source as list of Documents.\\n\\nIntegrations: 160+ integrations to choose from.\\nBaseLoader: API reference for the base interface.\\n\\n‚ÄãSplitting documents\\nOur loaded document is over 42k characters which is too long to fit into the context window of many models. Even for those models that could fit the full post in their context window, models can struggle to find information in very long inputs.\\nTo handle this we‚Äôll split the Document into chunks for embedding and vector storage. This should help us retrieve only the most relevant parts of the blog post at run time.\\nAs in the semantic search tutorial, we use a RecursiveCharacterTextSplitter, which will recursively split the document using common separators like new lines until each chunk is the appropriate size. This is the recommended text splitter for generic text use cases.\\nCopyfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\n\\ntext_splitter = RecursiveCharacterTextSplitter(\\n    chunk_size=1000,  # chunk size (characters)\\n    chunk_overlap=200,  # chunk overlap (characters)\\n    add_start_index=True,  # track index in original document\\n)\\nall_splits = text_splitter.split_documents(docs)\\n\\nprint(f\"Split blog post into {len(all_splits)} sub-documents.\")\\n\\nCopySplit blog post into 66 sub-documents.\\n\\nGo deeper\\nTextSplitter: Object that splits a list of Document objects into smaller\\nchunks for storage and retrieval.\\n\\nIntegrations\\nInterface: API reference for the base interface.\\n\\n‚ÄãStoring documents\\nNow we need to index our 66 text chunks so that we can search over them at runtime. Following the semantic search tutorial, our approach is to embed the contents of each document split and insert these embeddings into a vector store. Given an input query, we can then use vector search to retrieve relevant documents.\\nWe can embed and store all of our document splits in a single command using the vector store and embeddings model selected at the start of the tutorial.\\nCopydocument_ids = vector_store.add_documents(documents=all_splits)\\n\\nprint(document_ids[:3])\\n\\nCopy[\\'07c18af6-ad58-479a-bfb1-d508033f9c64\\', \\'9000bf8e-1993-446f-8d4d-f4e507ba4b8f\\', \\'ba3b5d14-bed9-4f5f-88be-44c88aedc2e6\\']\\n\\nGo deeper\\nEmbeddings: Wrapper around a text embedding model, used for converting text to embeddings.\\n\\nIntegrations: 30+ integrations to choose from.\\nInterface: API reference for the base interface.\\n\\nVectorStore: Wrapper around a vector database, used for storing and querying embeddings.\\n\\nIntegrations: 40+ integrations to choose from.\\nInterface: API reference for the base interface.\\n\\nThis completes the Indexing portion of the pipeline. At this point we have a query-able vector store containing the chunked contents of our blog post. Given a user question, we should ideally be able to return the snippets of the blog post that answer the question.\\n‚Äã2. Retrieval and generation\\nRAG applications commonly work as follows:\\n\\nRetrieve: Given a user input, relevant splits are retrieved from storage using a Retriever.\\nGenerate: A model produces an answer using a prompt that includes both the question with the retrieved data\\n\\n\\nNow let‚Äôs write the actual application logic. We want to create a simple application that takes a user question, searches for documents relevant to that question, passes the retrieved documents and initial question to a model, and returns an answer.\\nWe will demonstrate:\\n\\nA RAG agent that executes searches with a simple tool. This is a good general-purpose implementation.\\nA two-step RAG chain that uses just a single LLM call per query. This is a fast and effective method for simple queries.\\n\\n‚ÄãRAG agents\\nOne formulation of a RAG application is as a simple agent with a tool that retrieves information. We can assemble a minimal RAG agent by implementing a tool that wraps our vector store:\\nCopyfrom langchain.tools import tool\\n\\n@tool(response_format=\"content_and_artifact\")\\ndef retrieve_context(query: str):\\n    \"\"\"Retrieve information to help answer a query.\"\"\"\\n    retrieved_docs = vector_store.similarity_search(query, k=2)\\n    serialized = \"\\\\n\\\\n\".join(\\n        (f\"Source: {doc.metadata}\\\\nContent: {doc.page_content}\")\\n        for doc in retrieved_docs\\n    )\\n    return serialized, retrieved_docs\\n\\nHere we use the tool decorator to configure the tool to attach raw documents as artifacts to each ToolMessage. This will let us access document metadata in our application, separate from the stringified representation that is sent to the model.\\nRetrieval tools are not limited to a single string query argument, as in the above example. You can\\nforce the LLM to specify additional search parameters by adding arguments‚Äî for example, a category:Copyfrom typing import Literal\\n\\ndef retrieve_context(query: str, section: Literal[\"beginning\", \"middle\", \"end\"]):\\n\\nGiven our tool, we can construct the agent:\\nCopyfrom langchain.agents import create_agent\\n\\n\\ntools = [retrieve_context]\\n# If desired, specify custom instructions\\nprompt = (\\n    \"You have access to a tool that retrieves context from a blog post. \"\\n    \"Use the tool to help answer user queries.\"\\n)\\nagent = create_agent(model, tools, system_prompt=prompt)\\n\\nLet‚Äôs test this out. We construct a question that would typically require an iterative sequence of retrieval steps to answer:\\nCopyquery = (\\n    \"What is the standard method for Task Decomposition?\\\\n\\\\n\"\\n    \"Once you get the answer, look up common extensions of that method.\"\\n)\\n\\nfor event in agent.stream(\\n    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\\n    stream_mode=\"values\",\\n):\\n    event[\"messages\"][-1].pretty_print()\\n\\nCopy================================ Human Message =================================\\n\\nWhat is the standard method for Task Decomposition?\\n\\nOnce you get the answer, look up common extensions of that method.\\n================================== Ai Message ==================================\\nTool Calls:\\n  retrieve_context (call_d6AVxICMPQYwAKj9lgH4E337)\\n Call ID: call_d6AVxICMPQYwAKj9lgH4E337\\n  Args:\\n    query: standard method for Task Decomposition\\n================================= Tool Message =================================\\nName: retrieve_context\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Task decomposition can be done...\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Component One: Planning...\\n================================== Ai Message ==================================\\nTool Calls:\\n  retrieve_context (call_0dbMOw7266jvETbXWn4JqWpR)\\n Call ID: call_0dbMOw7266jvETbXWn4JqWpR\\n  Args:\\n    query: common extensions of the standard method for Task Decomposition\\n================================= Tool Message =================================\\nName: retrieve_context\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Task decomposition can be done...\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Component One: Planning...\\n================================== Ai Message ==================================\\n\\nThe standard method for Task Decomposition often used is the Chain of Thought (CoT)...\\n\\nNote that the agent:\\n\\nGenerates a query to search for a standard method for task decomposition;\\nReceiving the answer, generates a second query to search for common extensions of it;\\nHaving received all necessary context, answers the question.\\n\\nWe can see the full sequence of steps, along with latency and other metadata, in the LangSmith trace.\\nYou can add a deeper level of control and customization using the LangGraph framework directly‚Äî for example, you can add steps to grade document relevance and rewrite search queries. Check out LangGraph‚Äôs Agentic RAG tutorial for more advanced formulations.\\n‚ÄãRAG chains\\nIn the above agentic RAG formulation we allow the LLM to use its discretion in generating a tool call to help answer user queries. This is a good general-purpose solution, but comes with some trade-offs:\\n‚úÖ Benefits‚ö†Ô∏è DrawbacksSearch only when needed ‚Äì The LLM can handle greetings, follow-ups, and simple queries without triggering unnecessary searches.Two inference calls ‚Äì When a search is performed, it requires one call to generate the query and another to produce the final response.Contextual search queries ‚Äì By treating search as a tool with a query input, the LLM crafts its own queries that incorporate conversational context.Reduced control ‚Äì The LLM may skip searches when they are actually needed, or issue extra searches when unnecessary.Multiple searches allowed ‚Äì The LLM can execute several searches in support of a single user query.\\nAnother common approach is a two-step chain, in which we always run a search (potentially using the raw user query) and incorporate the result as context for a single LLM query. This results in a single inference call per query, buying reduced latency at the expense of flexibility.\\nIn this approach we no longer call the model in a loop, but instead make a single pass.\\nWe can implement this chain by removing tools from the agent and instead incorporating the retrieval step into a custom prompt:\\nCopyfrom langchain.agents.middleware import dynamic_prompt, ModelRequest\\n\\n@dynamic_prompt\\ndef prompt_with_context(request: ModelRequest) -> str:\\n    \"\"\"Inject context into state messages.\"\"\"\\n    last_query = request.state[\"messages\"][-1].text\\n    retrieved_docs = vector_store.similarity_search(last_query)\\n\\n    docs_content = \"\\\\n\\\\n\".join(doc.page_content for doc in retrieved_docs)\\n\\n    system_message = (\\n        \"You are a helpful assistant. Use the following context in your response:\"\\n        f\"\\\\n\\\\n{docs_content}\"\\n    )\\n\\n    return system_message\\n\\n\\nagent = create_agent(model, tools=[], middleware=[prompt_with_context])\\n\\nLet‚Äôs try this out:\\nCopyquery = \"What is task decomposition?\"\\nfor step in agent.stream(\\n    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\\n    stream_mode=\"values\",\\n):\\n    step[\"messages\"][-1].pretty_print()\\n\\nCopy================================ Human Message =================================\\n\\nWhat is task decomposition?\\n================================== Ai Message ==================================\\n\\nTask decomposition is...\\n\\nIn the LangSmith trace we can see the retrieved context incorporated into the model prompt.\\nThis is a fast and effective method for simple queries in constrained settings, when we typically do want to run user queries through semantic search to pull additional context.\\nReturning source documentsThe above RAG chain incorporates retrieved context into a single system message for that run.As in the agentic RAG formulation, we sometimes want to include raw source documents in the application state to have access to document metadata. We can do this for the two-step chain case by:\\nAdding a key to the state to store the retrieved documents\\nAdding a new node via a pre-model hook to populate that key (as well as inject the context).\\nCopyfrom typing import Any\\nfrom langchain_core.documents import Document\\nfrom langchain.agents.middleware import AgentMiddleware, AgentState\\n\\n\\nclass State(AgentState):\\n    context: list[Document]\\n\\n\\nclass RetrieveDocumentsMiddleware(AgentMiddleware[State]):\\n    state_schema = State\\n\\n    def before_model(self, state: AgentState) -> dict[str, Any] | None:\\n        last_message = state[\"messages\"][-1]\\n        retrieved_docs = vector_store.similarity_search(last_message.text)\\n\\n        docs_content = \"\\\\n\\\\n\".join(doc.page_content for doc in retrieved_docs)\\n\\n        augmented_message_content = (\\n            f\"{last_message.text}\\\\n\\\\n\"\\n            \"Use the following context to answer the query:\\\\n\"\\n            f\"{docs_content}\"\\n        )\\n        return {\\n            \"messages\": [last_message.model_copy(update={\"content\": augmented_message_content})],\\n            \"context\": retrieved_docs,\\n        }\\n\\n\\nagent = create_agent(\\n    model,\\n    tools=[],\\n    middleware=[RetrieveDocumentsMiddleware()],\\n)\\n\\n‚ÄãNext steps\\nNow that we‚Äôve implemented a simple RAG application via create_agent, we can easily incorporate new features and go deeper:\\n\\nStream tokens and other information for responsive user experiences\\nAdd conversational memory to support multi-turn interactions\\nAdd long-term memory to support memory across conversational threads\\nAdd structured responses\\nDeploy your application with LangSmith Deployment\\n\\n\\nEdit this page on GitHub or file an issue.\\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.Was this page helpful?YesNoBuild a semantic search engine with LangChainPreviousBuild a SQL agentNext‚åòIDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyHomeAboutCareersBloggithubxlinkedinyoutubePowered by\\n')],\n",
       " [Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/qa_chat_history/', 'title': 'Build a RAG agent with LangChain - Docs by LangChain', 'language': 'en'}, page_content='Build a RAG agent with LangChain - Docs by LangChainSkip to main contentDocs by LangChain home pageOpen sourceSearch...‚åòKAsk AIGitHubTry LangSmithTry LangSmithSearch...NavigationLangChainBuild a RAG agent with LangChainDeep AgentsLangChainLangGraphIntegrationsLearnReferenceContributePythonLearnTutorialsDeep AgentsLangChainSemantic searchRAG agentSQL agentVoice agentMulti-agentLangGraphConceptual overviewsLangChain vs. LangGraph vs. Deep AgentsComponent architectureMemoryContextGraph APIFunctional APIAdditional resourcesLangChain AcademyCase studiesGet helpOn this pageOverviewConceptsPreviewSetupInstallationLangSmithComponents1. IndexingLoading documentsSplitting documentsStoring documents2. Retrieval and generationRAG agentsRAG chainsNext stepsTutorialsLangChainBuild a RAG agent with LangChainCopy pageCopy page‚ÄãOverview\\nOne of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.\\nThis tutorial will show how to build a simple Q&A application over an unstructured text data source. We will demonstrate:\\n\\nA RAG agent that executes searches with a simple tool. This is a good general-purpose implementation.\\nA two-step RAG chain that uses just a single LLM call per query. This is a fast and effective method for simple queries.\\n\\n‚ÄãConcepts\\nWe will cover the following concepts:\\n\\n\\nIndexing: a pipeline for ingesting data from a source and indexing it. This usually happens in a separate process.\\n\\n\\nRetrieval and generation: the actual RAG process, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.\\n\\n\\nOnce we‚Äôve indexed our data, we will use an agent as our orchestration framework to implement the retrieval and generation steps.\\nThe indexing portion of this tutorial will largely follow the semantic search tutorial.If your data is already available for search (i.e., you have a function to execute a search), or you‚Äôre comfortable with the content from that tutorial, feel free to skip to the section on retrieval and generation\\n‚ÄãPreview\\nIn this guide we‚Äôll build an app that answers questions about the website‚Äôs content. The specific website we will use is the LLM Powered Autonomous Agents blog post by Lilian Weng, which allows us to ask questions about the contents of the post.\\nWe can create a simple indexing pipeline and RAG chain to do this in ~40 lines of code. See below for the full code snippet:\\nExpand for full code snippetCopyimport bs4\\nfrom langchain.agents import AgentState, create_agent\\nfrom langchain_community.document_loaders import WebBaseLoader\\nfrom langchain.messages import MessageLikeRepresentation\\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\n\\n# Load and chunk contents of the blog\\nloader = WebBaseLoader(\\n    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\\n    bs_kwargs=dict(\\n        parse_only=bs4.SoupStrainer(\\n            class_=(\"post-content\", \"post-title\", \"post-header\")\\n        )\\n    ),\\n)\\ndocs = loader.load()\\n\\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\\nall_splits = text_splitter.split_documents(docs)\\n\\n# Index chunks\\n_ = vector_store.add_documents(documents=all_splits)\\n\\n# Construct a tool for retrieving context\\n@tool(response_format=\"content_and_artifact\")\\ndef retrieve_context(query: str):\\n    \"\"\"Retrieve information to help answer a query.\"\"\"\\n    retrieved_docs = vector_store.similarity_search(query, k=2)\\n    serialized = \"\\\\n\\\\n\".join(\\n        (f\"Source: {doc.metadata}\\\\nContent: {doc.page_content}\")\\n        for doc in retrieved_docs\\n    )\\n    return serialized, retrieved_docs\\n\\ntools = [retrieve_context]\\n# If desired, specify custom instructions\\nprompt = (\\n    \"You have access to a tool that retrieves context from a blog post. \"\\n    \"Use the tool to help answer user queries.\"\\n)\\nagent = create_agent(model, tools, system_prompt=prompt)\\nCopyquery = \"What is task decomposition?\"\\nfor step in agent.stream(\\n    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\\n    stream_mode=\"values\",\\n):\\n    step[\"messages\"][-1].pretty_print()\\nCopy================================ Human Message =================================\\n\\nWhat is task decomposition?\\n================================== Ai Message ==================================\\nTool Calls:\\n  retrieve_context (call_xTkJr8njRY0geNz43ZvGkX0R)\\n Call ID: call_xTkJr8njRY0geNz43ZvGkX0R\\n  Args:\\n    query: task decomposition\\n================================= Tool Message =================================\\nName: retrieve_context\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Task decomposition can be done by...\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Component One: Planning...\\n================================== Ai Message ==================================\\n\\nTask decomposition refers to...\\nCheck out the LangSmith trace.\\n‚ÄãSetup\\n‚ÄãInstallation\\nThis tutorial requires these langchain dependencies:\\npipuvCopypip install langchain langchain-text-splitters langchain-community bs4\\n\\nFor more details, see our Installation guide.\\n‚ÄãLangSmith\\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with LangSmith.\\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\\nCopyexport LANGSMITH_TRACING=\"true\"\\nexport LANGSMITH_API_KEY=\"...\"\\n\\nOr, set them in Python:\\nCopyimport getpass\\nimport os\\n\\nos.environ[\"LANGSMITH_TRACING\"] = \"true\"\\nos.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\\n\\n‚ÄãComponents\\nWe will need to select three components from LangChain‚Äôs suite of integrations.\\nSelect a chat model:\\n OpenAI Anthropic Azure Google Gemini AWS Bedrock HuggingFace\\uf8ffüëâ Read the OpenAI chat model integration docsCopypip install -U \"langchain[openai]\"\\ninit_chat_modelModel ClassCopyimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\\n\\nmodel = init_chat_model(\"gpt-4.1\")\\n\\uf8ffüëâ Read the Anthropic chat model integration docsCopypip install -U \"langchain[anthropic]\"\\ninit_chat_modelModel ClassCopyimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"ANTHROPIC_API_KEY\"] = \"sk-...\"\\n\\nmodel = init_chat_model(\"claude-sonnet-4-5-20250929\")\\n\\uf8ffüëâ Read the Azure chat model integration docsCopypip install -U \"langchain[openai]\"\\ninit_chat_modelModel ClassCopyimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"AZURE_OPENAI_API_KEY\"] = \"...\"\\nos.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"...\"\\nos.environ[\"OPENAI_API_VERSION\"] = \"2025-03-01-preview\"\\n\\nmodel = init_chat_model(\\n    \"azure_openai:gpt-4.1\",\\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\\n)\\n\\uf8ffüëâ Read the Google GenAI chat model integration docsCopypip install -U \"langchain[google-genai]\"\\ninit_chat_modelModel ClassCopyimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"GOOGLE_API_KEY\"] = \"...\"\\n\\nmodel = init_chat_model(\"google_genai:gemini-2.5-flash-lite\")\\n\\uf8ffüëâ Read the AWS Bedrock chat model integration docsCopypip install -U \"langchain[aws]\"\\ninit_chat_modelModel ClassCopyfrom langchain.chat_models import init_chat_model\\n\\n# Follow the steps here to configure your credentials:\\n# https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html\\n\\nmodel = init_chat_model(\\n    \"anthropic.claude-3-5-sonnet-20240620-v1:0\",\\n    model_provider=\"bedrock_converse\",\\n)\\n\\uf8ffüëâ Read the HuggingFace chat model integration docsCopypip install -U \"langchain[huggingface]\"\\ninit_chat_modelModel ClassCopyimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"hf_...\"\\n\\nmodel = init_chat_model(\\n    \"microsoft/Phi-3-mini-4k-instruct\",\\n    model_provider=\"huggingface\",\\n    temperature=0.7,\\n    max_tokens=1024,\\n)\\n\\nSelect an embeddings model:\\n OpenAI Azure Google Gemini Google Vertex AWS HuggingFace Ollama Cohere MistralAI Nomic NVIDIA Voyage AI IBM watsonx Fake IsaacusCopypip install -U \"langchain-openai\"\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"OPENAI_API_KEY\"):\\n    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\\n\\nfrom langchain_openai import OpenAIEmbeddings\\n\\nembeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\\nCopypip install -U \"langchain-openai\"\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"AZURE_OPENAI_API_KEY\"):\\n    os.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for Azure: \")\\n\\nfrom langchain_openai import AzureOpenAIEmbeddings\\n\\nembeddings = AzureOpenAIEmbeddings(\\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\\n)\\nCopypip install -qU langchain-google-genai\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"GOOGLE_API_KEY\"):\\n    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")\\n\\nfrom langchain_google_genai import GoogleGenerativeAIEmbeddings\\n\\nembeddings = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\")\\nCopypip install -qU langchain-google-vertexai\\nCopyfrom langchain_google_vertexai import VertexAIEmbeddings\\n\\nembeddings = VertexAIEmbeddings(model=\"text-embedding-005\")\\nCopypip install -qU langchain-aws\\nCopyfrom langchain_aws import BedrockEmbeddings\\n\\nembeddings = BedrockEmbeddings(model_id=\"amazon.titan-embed-text-v2:0\")\\nCopypip install -qU langchain-huggingface\\nCopyfrom langchain_huggingface import HuggingFaceEmbeddings\\n\\nembeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\\nCopypip install -qU langchain-ollama\\nCopyfrom langchain_ollama import OllamaEmbeddings\\n\\nembeddings = OllamaEmbeddings(model=\"llama3\")\\nCopypip install -qU langchain-cohere\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"COHERE_API_KEY\"):\\n    os.environ[\"COHERE_API_KEY\"] = getpass.getpass(\"Enter API key for Cohere: \")\\n\\nfrom langchain_cohere import CohereEmbeddings\\n\\nembeddings = CohereEmbeddings(model=\"embed-english-v3.0\")\\nCopypip install -qU langchain-mistralai\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"MISTRALAI_API_KEY\"):\\n    os.environ[\"MISTRALAI_API_KEY\"] = getpass.getpass(\"Enter API key for MistralAI: \")\\n\\nfrom langchain_mistralai import MistralAIEmbeddings\\n\\nembeddings = MistralAIEmbeddings(model=\"mistral-embed\")\\nCopypip install -qU langchain-nomic\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"NOMIC_API_KEY\"):\\n    os.environ[\"NOMIC_API_KEY\"] = getpass.getpass(\"Enter API key for Nomic: \")\\n\\nfrom langchain_nomic import NomicEmbeddings\\n\\nembeddings = NomicEmbeddings(model=\"nomic-embed-text-v1.5\")\\nCopypip install -qU langchain-nvidia-ai-endpoints\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"NVIDIA_API_KEY\"):\\n    os.environ[\"NVIDIA_API_KEY\"] = getpass.getpass(\"Enter API key for NVIDIA: \")\\n\\nfrom langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\\n\\nembeddings = NVIDIAEmbeddings(model=\"NV-Embed-QA\")\\nCopypip install -qU langchain-voyageai\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"VOYAGE_API_KEY\"):\\n    os.environ[\"VOYAGE_API_KEY\"] = getpass.getpass(\"Enter API key for Voyage AI: \")\\n\\nfrom langchain-voyageai import VoyageAIEmbeddings\\n\\nembeddings = VoyageAIEmbeddings(model=\"voyage-3\")\\nCopypip install -qU langchain-ibm\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"WATSONX_APIKEY\"):\\n    os.environ[\"WATSONX_APIKEY\"] = getpass.getpass(\"Enter API key for IBM watsonx: \")\\n\\nfrom langchain_ibm import WatsonxEmbeddings\\n\\nembeddings = WatsonxEmbeddings(\\n    model_id=\"ibm/slate-125m-english-rtrvr\",\\n    url=\"https://us-south.ml.cloud.ibm.com\",\\n    project_id=\"<WATSONX PROJECT_ID>\",\\n)\\nCopypip install -qU langchain-core\\nCopyfrom langchain_core.embeddings import DeterministicFakeEmbedding\\n\\nembeddings = DeterministicFakeEmbedding(size=4096)\\nCopypip install -qU langchain-isaacus\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"ISAACUS_API_KEY\"):\\nos.environ[\"ISAACUS_API_KEY\"] = getpass.getpass(\"Enter API key for Isaacus: \")\\n\\nfrom langchain_isaacus import IsaacusEmbeddings\\n\\nembeddings = IsaacusEmbeddings(model=\"kanon-2-embedder\")\\n\\nSelect a vector store:\\n In-memory Amazon OpenSearch AstraDB Chroma FAISS Milvus MongoDB PGVector PGVectorStore Pinecone QdrantCopypip install -U \"langchain-core\"\\nCopyfrom langchain_core.vectorstores import InMemoryVectorStore\\n\\nvector_store = InMemoryVectorStore(embeddings)\\nCopypip install -qU  boto3\\nCopyfrom opensearchpy import RequestsHttpConnection\\n\\nservice = \"es\"  # must set the service as \\'es\\'\\nregion = \"us-east-2\"\\ncredentials = boto3.Session(\\n    aws_access_key_id=\"xxxxxx\", aws_secret_access_key=\"xxxxx\"\\n).get_credentials()\\nawsauth = AWS4Auth(\"xxxxx\", \"xxxxxx\", region, service, session_token=credentials.token)\\n\\nvector_store = OpenSearchVectorSearch.from_documents(\\n    docs,\\n    embeddings,\\n    opensearch_url=\"host url\",\\n    http_auth=awsauth,\\n    timeout=300,\\n    use_ssl=True,\\n    verify_certs=True,\\n    connection_class=RequestsHttpConnection,\\n    index_name=\"test-index\",\\n)\\nCopypip install -U \"langchain-astradb\"\\nCopyfrom langchain_astradb import AstraDBVectorStore\\n\\nvector_store = AstraDBVectorStore(\\n    embedding=embeddings,\\n    api_endpoint=ASTRA_DB_API_ENDPOINT,\\n    collection_name=\"astra_vector_langchain\",\\n    token=ASTRA_DB_APPLICATION_TOKEN,\\n    namespace=ASTRA_DB_NAMESPACE,\\n)\\nCopypip install -qU langchain-chroma\\nCopyfrom langchain_chroma import Chroma\\n\\nvector_store = Chroma(\\n    collection_name=\"example_collection\",\\n    embedding_function=embeddings,\\n    persist_directory=\"./chroma_langchain_db\",  # Where to save data locally, remove if not necessary\\n)\\nCopypip install -qU langchain-community faiss-cpu\\nCopyimport faiss\\nfrom langchain_community.docstore.in_memory import InMemoryDocstore\\nfrom langchain_community.vectorstores import FAISS\\n\\nembedding_dim = len(embeddings.embed_query(\"hello world\"))\\nindex = faiss.IndexFlatL2(embedding_dim)\\n\\nvector_store = FAISS(\\n    embedding_function=embeddings,\\n    index=index,\\n    docstore=InMemoryDocstore(),\\n    index_to_docstore_id={},\\n)\\nCopypip install -qU langchain-milvus\\nCopyfrom langchain_milvus import Milvus\\n\\nURI = \"./milvus_example.db\"\\n\\nvector_store = Milvus(\\n    embedding_function=embeddings,\\n    connection_args={\"uri\": URI},\\n    index_params={\"index_type\": \"FLAT\", \"metric_type\": \"L2\"},\\n)\\nCopypip install -qU langchain-mongodb\\nCopyfrom langchain_mongodb import MongoDBAtlasVectorSearch\\n\\nvector_store = MongoDBAtlasVectorSearch(\\n    embedding=embeddings,\\n    collection=MONGODB_COLLECTION,\\n    index_name=ATLAS_VECTOR_SEARCH_INDEX_NAME,\\n    relevance_score_fn=\"cosine\",\\n)\\nCopypip install -qU langchain-postgres\\nCopyfrom langchain_postgres import PGVector\\n\\nvector_store = PGVector(\\n    embeddings=embeddings,\\n    collection_name=\"my_docs\",\\n    connection=\"postgresql+psycopg://...\",\\n)\\nCopypip install -qU langchain-postgres\\nCopyfrom langchain_postgres import PGEngine, PGVectorStore\\n\\npg_engine = PGEngine.from_connection_string(\\n    url=\"postgresql+psycopg://...\"\\n)\\n\\nvector_store = PGVectorStore.create_sync(\\n    engine=pg_engine,\\n    table_name=\\'test_table\\',\\n    embedding_service=embeddings\\n)\\nCopypip install -qU langchain-pinecone\\nCopyfrom langchain_pinecone import PineconeVectorStore\\nfrom pinecone import Pinecone\\n\\npc = Pinecone(api_key=...)\\nindex = pc.Index(index_name)\\n\\nvector_store = PineconeVectorStore(embedding=embeddings, index=index)\\nCopypip install -qU langchain-qdrant\\nCopyfrom qdrant_client.models import Distance, VectorParams\\nfrom langchain_qdrant import QdrantVectorStore\\nfrom qdrant_client import QdrantClient\\n\\nclient = QdrantClient(\":memory:\")\\n\\nvector_size = len(embeddings.embed_query(\"sample text\"))\\n\\nif not client.collection_exists(\"test\"):\\n    client.create_collection(\\n        collection_name=\"test\",\\n        vectors_config=VectorParams(size=vector_size, distance=Distance.COSINE)\\n    )\\nvector_store = QdrantVectorStore(\\n    client=client,\\n    collection_name=\"test\",\\n    embedding=embeddings,\\n)\\n\\n‚Äã1. Indexing\\nThis section is an abbreviated version of the content in the semantic search tutorial.If your data is already indexed and available for search (i.e., you have a function to execute a search), or if you‚Äôre comfortable with document loaders, embeddings, and vector stores, feel free to skip to the next section on retrieval and generation.\\nIndexing commonly works as follows:\\n\\nLoad: First we need to load our data. This is done with Document Loaders.\\nSplit: Text splitters break large Documents into smaller chunks. This is useful both for indexing data and passing it into a model, as large chunks are harder to search over and won‚Äôt fit in a model‚Äôs finite context window.\\nStore: We need somewhere to store and index our splits, so that they can be searched over later. This is often done using a VectorStore and Embeddings model.\\n\\n\\n‚ÄãLoading documents\\nWe need to first load the blog post contents. We can use DocumentLoaders for this, which are objects that load in data from a source and return a list of Document objects.\\nIn this case we‚Äôll use the WebBaseLoader, which uses urllib to load HTML from web URLs and BeautifulSoup to parse it to text. We can customize the HTML -> text parsing by passing in parameters into the BeautifulSoup parser via bs_kwargs (see BeautifulSoup docs). In this case only HTML tags with class ‚Äúpost-content‚Äù, ‚Äúpost-title‚Äù, or ‚Äúpost-header‚Äù are relevant, so we‚Äôll remove all others.\\nCopyimport bs4\\nfrom langchain_community.document_loaders import WebBaseLoader\\n\\n# Only keep post title, headers, and content from the full HTML.\\nbs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))\\nloader = WebBaseLoader(\\n    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\\n    bs_kwargs={\"parse_only\": bs4_strainer},\\n)\\ndocs = loader.load()\\n\\nassert len(docs) == 1\\nprint(f\"Total characters: {len(docs[0].page_content)}\")\\n\\nCopyTotal characters: 43131\\n\\nCopyprint(docs[0].page_content[:500])\\n\\nCopy      LLM Powered Autonomous Agents\\n\\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn\\n\\nGo deeper\\nDocumentLoader: Object that loads data from a source as list of Documents.\\n\\nIntegrations: 160+ integrations to choose from.\\nBaseLoader: API reference for the base interface.\\n\\n‚ÄãSplitting documents\\nOur loaded document is over 42k characters which is too long to fit into the context window of many models. Even for those models that could fit the full post in their context window, models can struggle to find information in very long inputs.\\nTo handle this we‚Äôll split the Document into chunks for embedding and vector storage. This should help us retrieve only the most relevant parts of the blog post at run time.\\nAs in the semantic search tutorial, we use a RecursiveCharacterTextSplitter, which will recursively split the document using common separators like new lines until each chunk is the appropriate size. This is the recommended text splitter for generic text use cases.\\nCopyfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\n\\ntext_splitter = RecursiveCharacterTextSplitter(\\n    chunk_size=1000,  # chunk size (characters)\\n    chunk_overlap=200,  # chunk overlap (characters)\\n    add_start_index=True,  # track index in original document\\n)\\nall_splits = text_splitter.split_documents(docs)\\n\\nprint(f\"Split blog post into {len(all_splits)} sub-documents.\")\\n\\nCopySplit blog post into 66 sub-documents.\\n\\nGo deeper\\nTextSplitter: Object that splits a list of Document objects into smaller\\nchunks for storage and retrieval.\\n\\nIntegrations\\nInterface: API reference for the base interface.\\n\\n‚ÄãStoring documents\\nNow we need to index our 66 text chunks so that we can search over them at runtime. Following the semantic search tutorial, our approach is to embed the contents of each document split and insert these embeddings into a vector store. Given an input query, we can then use vector search to retrieve relevant documents.\\nWe can embed and store all of our document splits in a single command using the vector store and embeddings model selected at the start of the tutorial.\\nCopydocument_ids = vector_store.add_documents(documents=all_splits)\\n\\nprint(document_ids[:3])\\n\\nCopy[\\'07c18af6-ad58-479a-bfb1-d508033f9c64\\', \\'9000bf8e-1993-446f-8d4d-f4e507ba4b8f\\', \\'ba3b5d14-bed9-4f5f-88be-44c88aedc2e6\\']\\n\\nGo deeper\\nEmbeddings: Wrapper around a text embedding model, used for converting text to embeddings.\\n\\nIntegrations: 30+ integrations to choose from.\\nInterface: API reference for the base interface.\\n\\nVectorStore: Wrapper around a vector database, used for storing and querying embeddings.\\n\\nIntegrations: 40+ integrations to choose from.\\nInterface: API reference for the base interface.\\n\\nThis completes the Indexing portion of the pipeline. At this point we have a query-able vector store containing the chunked contents of our blog post. Given a user question, we should ideally be able to return the snippets of the blog post that answer the question.\\n‚Äã2. Retrieval and generation\\nRAG applications commonly work as follows:\\n\\nRetrieve: Given a user input, relevant splits are retrieved from storage using a Retriever.\\nGenerate: A model produces an answer using a prompt that includes both the question with the retrieved data\\n\\n\\nNow let‚Äôs write the actual application logic. We want to create a simple application that takes a user question, searches for documents relevant to that question, passes the retrieved documents and initial question to a model, and returns an answer.\\nWe will demonstrate:\\n\\nA RAG agent that executes searches with a simple tool. This is a good general-purpose implementation.\\nA two-step RAG chain that uses just a single LLM call per query. This is a fast and effective method for simple queries.\\n\\n‚ÄãRAG agents\\nOne formulation of a RAG application is as a simple agent with a tool that retrieves information. We can assemble a minimal RAG agent by implementing a tool that wraps our vector store:\\nCopyfrom langchain.tools import tool\\n\\n@tool(response_format=\"content_and_artifact\")\\ndef retrieve_context(query: str):\\n    \"\"\"Retrieve information to help answer a query.\"\"\"\\n    retrieved_docs = vector_store.similarity_search(query, k=2)\\n    serialized = \"\\\\n\\\\n\".join(\\n        (f\"Source: {doc.metadata}\\\\nContent: {doc.page_content}\")\\n        for doc in retrieved_docs\\n    )\\n    return serialized, retrieved_docs\\n\\nHere we use the tool decorator to configure the tool to attach raw documents as artifacts to each ToolMessage. This will let us access document metadata in our application, separate from the stringified representation that is sent to the model.\\nRetrieval tools are not limited to a single string query argument, as in the above example. You can\\nforce the LLM to specify additional search parameters by adding arguments‚Äî for example, a category:Copyfrom typing import Literal\\n\\ndef retrieve_context(query: str, section: Literal[\"beginning\", \"middle\", \"end\"]):\\n\\nGiven our tool, we can construct the agent:\\nCopyfrom langchain.agents import create_agent\\n\\n\\ntools = [retrieve_context]\\n# If desired, specify custom instructions\\nprompt = (\\n    \"You have access to a tool that retrieves context from a blog post. \"\\n    \"Use the tool to help answer user queries.\"\\n)\\nagent = create_agent(model, tools, system_prompt=prompt)\\n\\nLet‚Äôs test this out. We construct a question that would typically require an iterative sequence of retrieval steps to answer:\\nCopyquery = (\\n    \"What is the standard method for Task Decomposition?\\\\n\\\\n\"\\n    \"Once you get the answer, look up common extensions of that method.\"\\n)\\n\\nfor event in agent.stream(\\n    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\\n    stream_mode=\"values\",\\n):\\n    event[\"messages\"][-1].pretty_print()\\n\\nCopy================================ Human Message =================================\\n\\nWhat is the standard method for Task Decomposition?\\n\\nOnce you get the answer, look up common extensions of that method.\\n================================== Ai Message ==================================\\nTool Calls:\\n  retrieve_context (call_d6AVxICMPQYwAKj9lgH4E337)\\n Call ID: call_d6AVxICMPQYwAKj9lgH4E337\\n  Args:\\n    query: standard method for Task Decomposition\\n================================= Tool Message =================================\\nName: retrieve_context\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Task decomposition can be done...\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Component One: Planning...\\n================================== Ai Message ==================================\\nTool Calls:\\n  retrieve_context (call_0dbMOw7266jvETbXWn4JqWpR)\\n Call ID: call_0dbMOw7266jvETbXWn4JqWpR\\n  Args:\\n    query: common extensions of the standard method for Task Decomposition\\n================================= Tool Message =================================\\nName: retrieve_context\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Task decomposition can be done...\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Component One: Planning...\\n================================== Ai Message ==================================\\n\\nThe standard method for Task Decomposition often used is the Chain of Thought (CoT)...\\n\\nNote that the agent:\\n\\nGenerates a query to search for a standard method for task decomposition;\\nReceiving the answer, generates a second query to search for common extensions of it;\\nHaving received all necessary context, answers the question.\\n\\nWe can see the full sequence of steps, along with latency and other metadata, in the LangSmith trace.\\nYou can add a deeper level of control and customization using the LangGraph framework directly‚Äî for example, you can add steps to grade document relevance and rewrite search queries. Check out LangGraph‚Äôs Agentic RAG tutorial for more advanced formulations.\\n‚ÄãRAG chains\\nIn the above agentic RAG formulation we allow the LLM to use its discretion in generating a tool call to help answer user queries. This is a good general-purpose solution, but comes with some trade-offs:\\n‚úÖ Benefits‚ö†Ô∏è DrawbacksSearch only when needed ‚Äì The LLM can handle greetings, follow-ups, and simple queries without triggering unnecessary searches.Two inference calls ‚Äì When a search is performed, it requires one call to generate the query and another to produce the final response.Contextual search queries ‚Äì By treating search as a tool with a query input, the LLM crafts its own queries that incorporate conversational context.Reduced control ‚Äì The LLM may skip searches when they are actually needed, or issue extra searches when unnecessary.Multiple searches allowed ‚Äì The LLM can execute several searches in support of a single user query.\\nAnother common approach is a two-step chain, in which we always run a search (potentially using the raw user query) and incorporate the result as context for a single LLM query. This results in a single inference call per query, buying reduced latency at the expense of flexibility.\\nIn this approach we no longer call the model in a loop, but instead make a single pass.\\nWe can implement this chain by removing tools from the agent and instead incorporating the retrieval step into a custom prompt:\\nCopyfrom langchain.agents.middleware import dynamic_prompt, ModelRequest\\n\\n@dynamic_prompt\\ndef prompt_with_context(request: ModelRequest) -> str:\\n    \"\"\"Inject context into state messages.\"\"\"\\n    last_query = request.state[\"messages\"][-1].text\\n    retrieved_docs = vector_store.similarity_search(last_query)\\n\\n    docs_content = \"\\\\n\\\\n\".join(doc.page_content for doc in retrieved_docs)\\n\\n    system_message = (\\n        \"You are a helpful assistant. Use the following context in your response:\"\\n        f\"\\\\n\\\\n{docs_content}\"\\n    )\\n\\n    return system_message\\n\\n\\nagent = create_agent(model, tools=[], middleware=[prompt_with_context])\\n\\nLet‚Äôs try this out:\\nCopyquery = \"What is task decomposition?\"\\nfor step in agent.stream(\\n    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\\n    stream_mode=\"values\",\\n):\\n    step[\"messages\"][-1].pretty_print()\\n\\nCopy================================ Human Message =================================\\n\\nWhat is task decomposition?\\n================================== Ai Message ==================================\\n\\nTask decomposition is...\\n\\nIn the LangSmith trace we can see the retrieved context incorporated into the model prompt.\\nThis is a fast and effective method for simple queries in constrained settings, when we typically do want to run user queries through semantic search to pull additional context.\\nReturning source documentsThe above RAG chain incorporates retrieved context into a single system message for that run.As in the agentic RAG formulation, we sometimes want to include raw source documents in the application state to have access to document metadata. We can do this for the two-step chain case by:\\nAdding a key to the state to store the retrieved documents\\nAdding a new node via a pre-model hook to populate that key (as well as inject the context).\\nCopyfrom typing import Any\\nfrom langchain_core.documents import Document\\nfrom langchain.agents.middleware import AgentMiddleware, AgentState\\n\\n\\nclass State(AgentState):\\n    context: list[Document]\\n\\n\\nclass RetrieveDocumentsMiddleware(AgentMiddleware[State]):\\n    state_schema = State\\n\\n    def before_model(self, state: AgentState) -> dict[str, Any] | None:\\n        last_message = state[\"messages\"][-1]\\n        retrieved_docs = vector_store.similarity_search(last_message.text)\\n\\n        docs_content = \"\\\\n\\\\n\".join(doc.page_content for doc in retrieved_docs)\\n\\n        augmented_message_content = (\\n            f\"{last_message.text}\\\\n\\\\n\"\\n            \"Use the following context to answer the query:\\\\n\"\\n            f\"{docs_content}\"\\n        )\\n        return {\\n            \"messages\": [last_message.model_copy(update={\"content\": augmented_message_content})],\\n            \"context\": retrieved_docs,\\n        }\\n\\n\\nagent = create_agent(\\n    model,\\n    tools=[],\\n    middleware=[RetrieveDocumentsMiddleware()],\\n)\\n\\n‚ÄãNext steps\\nNow that we‚Äôve implemented a simple RAG application via create_agent, we can easily incorporate new features and go deeper:\\n\\nStream tokens and other information for responsive user experiences\\nAdd conversational memory to support multi-turn interactions\\nAdd long-term memory to support memory across conversational threads\\nAdd structured responses\\nDeploy your application with LangSmith Deployment\\n\\n\\nEdit this page on GitHub or file an issue.\\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.Was this page helpful?YesNoBuild a semantic search engine with LangChainPreviousBuild a SQL agentNext‚åòIDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyHomeAboutCareersBloggithubxlinkedinyoutubePowered by\\n')]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "langchain_urls=[\n",
    "    \"https://python.langchain.com/docs/tutorials/\",\n",
    "    \"https://python.langchain.com/docs/tutorials/chatbot/\",\n",
    "    \"https://python.langchain.com/docs/tutorials/qa_chat_history/\"\n",
    "]\n",
    "\n",
    "docs=[WebBaseLoader(url).load() for url in langchain_urls]\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f834d0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=100\n",
    ")\n",
    "\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "## Add alll these text to vectordb\n",
    "\n",
    "vectorstorelangchain=FAISS.from_documents(\n",
    "    documents=doc_splits,\n",
    "    embedding=HuggingFaceEmbeddings()\n",
    ")\n",
    "\n",
    "\n",
    "retrieverlangchain=vectorstorelangchain.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cf70c2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import create_retriever_tool\n",
    "\n",
    "retriever_tool_langchain=create_retriever_tool(\n",
    "    retrieverlangchain,\n",
    "    \"retriever_vector_langchain_blog\",\n",
    "    \"Search and run information about Langchain\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ad6b196c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bengaluru is 19.3°C'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from langchain.tools import tool\n",
    "\n",
    "@tool\n",
    "def get_weather(city: str) -> str:\n",
    "    \"\"\"Get current weather for a given city.\"\"\"\n",
    "    \n",
    "    api_key = os.getenv(\"WEATHER_API_KEY\")\n",
    "    if not api_key:\n",
    "        return \"Weather API key not set\"\n",
    "\n",
    "    url = f\"https://api.weatherapi.com/v1/current.json?key={api_key}&q={city}\"\n",
    "    resp = requests.get(url)\n",
    "    data = resp.json()\n",
    "\n",
    "    if \"error\" in data:\n",
    "        return f\"Weather API error: {data['error']['message']}\"\n",
    "\n",
    "    return f\"{data['location']['name']} is {data['current']['temp_c']}°C\"\n",
    "\n",
    "get_weather.invoke({\"city\": \"bengaluru\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "edbc7887",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools=[retriever_tool,retriever_tool_langchain,get_weather]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bab96e38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StructuredTool(name='retriever_vector_db_blog', description='Search and run information about Langgraph', args_schema=<class 'langchain_core.tools.retriever.RetrieverInput'>, func=<function create_retriever_tool.<locals>.func at 0x000002485840B560>, coroutine=<function create_retriever_tool.<locals>.afunc at 0x00000248198E1E40>),\n",
       " StructuredTool(name='retriever_vector_langchain_blog', description='Search and run information about Langchain', args_schema=<class 'langchain_core.tools.retriever.RetrieverInput'>, func=<function create_retriever_tool.<locals>.func at 0x00000248585B0860>, coroutine=<function create_retriever_tool.<locals>.afunc at 0x00000248585B09A0>),\n",
       " StructuredTool(name='get_weather', description='Get current weather for a given city.', args_schema=<class 'langchain_core.utils.pydantic.get_weather'>, func=<function get_weather at 0x0000024835074C20>)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760a65ca",
   "metadata": {},
   "source": [
    "### LangGraph Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "127c309b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Sequence\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage],add_messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9b581a0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I help you today?', additional_kwargs={'reasoning_content': 'The user says \"hi\". We just respond politely. Probably a greeting.'}, response_metadata={'token_usage': {'completion_tokens': 34, 'prompt_tokens': 72, 'total_tokens': 106, 'completion_time': 0.071503218, 'completion_tokens_details': {'reasoning_tokens': 16}, 'prompt_time': 0.002912243, 'prompt_tokens_details': None, 'queue_time': 0.046104484, 'total_time': 0.074415461}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_a09bde29de', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019c681e-9157-7e90-b831-eafba986fd80-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 72, 'output_tokens': 34, 'total_tokens': 106, 'output_token_details': {'reasoning': 16}})"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(model=\"openai/gpt-oss-120b\")\n",
    "llm.invoke(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f5176f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent(state):\n",
    "    \"\"\"\n",
    "    Invokes the agent model to generate a response based on current state. Given the question, it will decide to retrieve using \n",
    "    the retriever tools, for weather related questions use get_weather tool.\n",
    "    \n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "\n",
    "    Returns:\n",
    "        dict: The updated state with the response added to the messages\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"---CALL AGENT---\")\n",
    "    messages = state[\"messages\"]\n",
    "    model = ChatGroq(model=\"openai/gpt-oss-120b\")\n",
    "    model = model.bind_tools(tools)\n",
    "    response = model.invoke(messages)\n",
    "    return {\n",
    "        \"messages\": messages + [response]\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "67749188",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Literal, Sequence\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langchain_classic import hub\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ed80d86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Edges\n",
    "def grade_documents(state) -> Literal[\"generate\", \"rewrite\"]:\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question.\n",
    "\n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "\n",
    "    Returns:\n",
    "        str: A decision for whether the documents are relevant or not\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK RELEVANCE---\")\n",
    "\n",
    "    # Data model\n",
    "    class grade(BaseModel):\n",
    "        \"\"\"Binary score for relevance check.\"\"\"\n",
    "\n",
    "        binary_score: str = Field(description=\"Relevance score 'yes' or 'no'\")\n",
    "\n",
    "    # LLM\n",
    "    model = ChatGroq(model=\"openai/gpt-oss-120b\")\n",
    "\n",
    "    # LLM with tool and validation\n",
    "    llm_with_tool = model.with_structured_output(grade)\n",
    "\n",
    "    # Prompt\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "        Here is the retrieved document: \\n\\n {context} \\n\\n\n",
    "        Here is the user question: {question} \\n\n",
    "        If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n",
    "        Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\",\n",
    "        input_variables=[\"context\", \"question\"],\n",
    "    )\n",
    "\n",
    "    # Chain\n",
    "    chain = prompt | llm_with_tool\n",
    "\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "\n",
    "    question = messages[0].content\n",
    "    docs = last_message.content\n",
    "\n",
    "    scored_result = chain.invoke({\"question\": question, \"context\": docs})\n",
    "\n",
    "    score = scored_result.binary_score\n",
    "\n",
    "    if score == \"yes\":\n",
    "        print(\"---DECISION: DOCS RELEVANT---\")\n",
    "        return \"generate\"\n",
    "\n",
    "    else:\n",
    "        print(\"---DECISION: DOCS NOT RELEVANT---\")\n",
    "        print(score)\n",
    "        return \"rewrite\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "884aa9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer\n",
    "\n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "\n",
    "    Returns:\n",
    "         dict: The updated message\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE---\")\n",
    "    messages = state[\"messages\"]\n",
    "    question = messages[0].content\n",
    "    last_message = messages[-1]\n",
    "\n",
    "    docs = last_message.content\n",
    "\n",
    "    # Prompt\n",
    "    prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "    # LLM\n",
    "    llm = ChatGroq(model=\"openai/gpt-oss-120b\")\n",
    "\n",
    "    # Post-processing\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "    # Chain\n",
    "    rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "    # Run\n",
    "    response = rag_chain.invoke({\"context\": docs, \"question\": question})\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "aa2a563b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite(state):\n",
    "    \"\"\"\n",
    "    Transform the query to produce a better question.\n",
    "\n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "\n",
    "    Returns:\n",
    "        dict: The updated state with re-phrased question\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---TRANSFORM QUERY---\")\n",
    "    messages = state[\"messages\"]\n",
    "    question = messages[0].content\n",
    "\n",
    "    msg = [\n",
    "        HumanMessage(\n",
    "            content=f\"\"\" \\n \n",
    "    Look at the input and try to reason about the underlying semantic intent / meaning. \\n \n",
    "    Here is the initial question:\n",
    "    \\n ------- \\n\n",
    "    {question} \n",
    "    \\n ------- \\n\n",
    "    Formulate an improved question: \"\"\",\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Grader\n",
    "    model = ChatGroq(model=\"openai/gpt-oss-120b\")\n",
    "    response = model.invoke(msg)\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "26bbd7fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.prebuilt import tools_condition\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Define the nodes we will cycle between\n",
    "workflow.add_node(\"agent\", agent)  # agent\n",
    "retrieve = ToolNode([retriever_tool,retriever_tool_langchain,get_weather])\n",
    "workflow.add_node(\"retrieve\", retrieve)  # retrieval\n",
    "workflow.add_node(\"rewrite\", rewrite)  # Re-writing the question\n",
    "workflow.add_node(\n",
    "    \"generate\", generate\n",
    ")  # Generating a response after we know the documents are relevant\n",
    "# Call agent node to decide to retrieve or not\n",
    "workflow.add_edge(START, \"agent\")\n",
    "\n",
    "# Decide whether to retrieve\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    # Assess agent decision\n",
    "    tools_condition,\n",
    "    {\n",
    "        # Translate the condition outputs to nodes in our graph\n",
    "        \"tools\": \"retrieve\",\n",
    "        END: END,\n",
    "    },\n",
    ")\n",
    "\n",
    "# Edges taken after the `action` node is called.\n",
    "workflow.add_conditional_edges(\n",
    "    \"retrieve\",\n",
    "    # Assess agent decision\n",
    "    grade_documents,\n",
    ")\n",
    "workflow.add_edge(\"generate\", END)\n",
    "workflow.add_edge(\"rewrite\", \"agent\")\n",
    "\n",
    "# Compile\n",
    "graph = workflow.compile()\n",
    "from IPython.display import Image, display\n",
    "display(Image(graph.get_graph(xray=True).draw_mermaid_png()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "661decb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---CALL AGENT---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='What is LangGraph?', additional_kwargs={}, response_metadata={}, id='f9dec876-e7cd-44ee-ae75-bd0f3ac034fb'),\n",
       "  AIMessage(content='**LangGraph** is an open‑source Python library (maintained by the LangChain team) that lets you build **graph‑structured, stateful workflows** for large‑language‑model (LLM) applications.  \\n\\nIn short, it gives you a **“programming model”** for orchestrating LLM calls, tool usage, memory, branching, loops, and conditional logic—everything you need to turn a prompt into a full‑featured, production‑ready application.\\n\\n---\\n\\n## Why LangGraph Exists\\n\\n| Problem in LLM Apps | How LangGraph Helps |\\n|----------------------|----------------------|\\n| **Linear prompt‑to‑output pipelines** feel brittle when you need retries, branching, or multiple tool calls. | You model the workflow as a **graph** of nodes (steps) and edges (transitions), so the execution can naturally branch, loop, or backtrack. |\\n| **State management** (e.g., remembering past user inputs, tool results, or partial progress) is often ad‑hoc. | LangGraph provides a **typed `State` object** that is automatically passed between nodes, making persistence and inspection trivial. |\\n| **Complex agent logic** (e.g., “if the user asks about weather, call the weather API; otherwise, ask the knowledge base”) tends to be hard‑coded. | Nodes can contain **conditional logic** that decides the next edge based on the current state, letting you declaratively express “if‑then‑else” flows. |\\n| **Testing & debugging** of multi‑step LLM pipelines is cumbersome. | Because each node is a pure function of `state → new_state`, you can unit‑test nodes in isolation and visualize the graph execution trace. |\\n| **Scalability & deployment**: you often need to run steps on different workers, queue them, or checkpoint progress. | LangGraph’s graph can be executed **synchronously, asynchronously, or via a task‑queue (e.g., Celery, Prefect, or LangChain’s built‑in `Runnable` infrastructure)**, and you can persist the state to a DB or KV store. |\\n\\n---\\n\\n## Core Concepts\\n\\n| Concept | What It Is | Typical Implementation |\\n|---------|------------|------------------------|\\n| **Node (or `Runnable`)** | A single unit of work – usually a prompt, a tool call, or any Python function. | `def my_node(state: dict) -> dict: …` or a `langchain.chains` object wrapped with `Runnable`. |\\n| **Edge** | The directed connection that tells the graph which node to run next. Can be unconditional or conditional on the state. | `(\"node_a\", \"node_b\")` or `ConditionalEdge(source=\"node_a\", target=\"node_b\", condition=lambda s: s[\"needs_tool\"])`. |\\n| **State** | A typed dict (often a `pydantic.BaseModel`) that holds everything the workflow cares about: user messages, LLM outputs, tool results, flags, etc. | `class GraphState(BaseModel): user_input: str; llm_output: str | None; tool_result: Any | None`. |\\n| **Graph** | The collection of nodes + edges that defines the execution order. LangGraph builds it with a declarative DSL (`GraphBuilder`). | `builder = GraphBuilder(); builder.add_node(\"router\", router_node); builder.add_conditional_edge(\"router\", \"weather\", lambda s: s.intent == \"weather\")`. |\\n| **Executor** | The runtime that walks the graph, feeding the current state into each node, handling retries, async calls, and persistence. | `graph = builder.compile(); result = graph.invoke(initial_state)`. |\\n| **Checkpointing** | Automatic snapshots of the state after each node, enabling resumable runs, debugging, and observability. | `graph.checkpoint(state)`. |\\n| **Observability** | Built‑in logging, tracing (OpenTelemetry), and visual graph rendering (via `graph.draw()`). | `graph.visualize()`. |\\n\\n---\\n\\n## How It Fits With LangChain\\n\\n- **LangChain** provides the *building blocks*: prompt templates, LLM wrappers, tool integrations, memory, retrievers, etc.\\n- **LangGraph** provides the *orchestration layer* on top of those blocks, turning a collection of LangChain components into a **stateful, programmable graph**.\\n\\nYou can think of LangGraph as the “workflow engine” while LangChain remains the “LLM toolbox”.\\n\\n---\\n\\n## Typical Use Cases\\n\\n| Use Case | Why a Graph Helps |\\n|----------|-------------------|\\n| **Multi‑turn agents** that need to decide whether to call a search tool, a calculator, or a knowledge base on each turn. | Conditional edges let you route based on the agent’s “thoughts”. |\\n| **Complex data pipelines** (e.g., ingest → summarise → classify → store) where each step may need to be retried or run in parallel. | Nodes can be executed asynchronously and checkpointed. |\\n| **Human‑in‑the‑loop workflows** (e.g., LLM drafts a response, a reviewer edits, then the LLM refines). | State captures both LLM and human edits; the graph can loop until a termination condition is met. |\\n| **Chatbots with fallback strategies** (primary model → fallback model → canned response). | Edges allow graceful degradation without hard‑coding multiple `if` statements. |\\n| **Tool‑driven decision trees** (e.g., travel planner: get location → get weather → suggest activities). | Each tool call is a node; the next step depends on the tool’s output. |\\n\\n---\\n\\n## Minimal Example\\n\\nBelow is a tiny, self‑contained example that demonstrates the main pieces. It builds a chatbot that can either **answer a factual question** using a LangChain `RetrievalQA` chain **or** **fetch the current weather** via a mock tool.\\n\\n```python\\n# Install once:\\n# pip install langgraph langchain openai\\n\\nfrom typing import Literal, TypedDict, Optional\\nfrom pydantic import BaseModel\\nfrom langchain_openai import ChatOpenAI\\nfrom langgraph.graph import GraphBuilder, StateGraph\\nfrom langchain.prompts import PromptTemplate\\nfrom langchain.chains import RetrievalQA\\nfrom langchain.vectorstores import FAISS\\nfrom langchain.embeddings import OpenAIEmbeddings\\n\\n# ---------- 1️⃣ Define the shared state ----------\\nclass GraphState(BaseModel):\\n    user_input: str\\n    intent: Literal[\"weather\", \"qa\", \"unknown\"] = \"unknown\"\\n    llm_output: Optional[str] = None\\n    tool_result: Optional[str] = None\\n\\n# ---------- 2️⃣ Helper functions ----------\\ndef simple_intent_classifier(text: str) -> Literal[\"weather\", \"qa\", \"unknown\"]:\\n    \"\"\"Very naive classifier – in real life you\\'d use an LLM or a trained model.\"\"\"\\n    lowered = text.lower()\\n    if \"weather\" in lowered or \"temperature\" in lowered:\\n        return \"weather\"\\n    elif \"who\" in lowered or \"what\" in lowered or \"when\" in lowered:\\n        return \"qa\"\\n    else:\\n        return \"unknown\"\\n\\n# ---------- 3️⃣ Define the nodes ----------\\ndef router_node(state: GraphState) -> GraphState:\\n    state.intent = simple_intent_classifier(state.user_input)\\n    return state\\n\\ndef weather_tool(state: GraphState) -> GraphState:\\n    # Mock weather; replace with a real API call or LangChain tool\\n    city = state.user_input.split(\"in\")[-1].strip().capitalize()\\n    state.tool_result = f\"The weather in {city} is sunny, 24°C.\"\\n    return state\\n\\ndef qa_node(state: GraphState) -> GraphState:\\n    # Very small example knowledge base\\n    docs = [\"Python is a programming language created by Guido van Rossum in 1991.\"]\\n    embeddings = OpenAIEmbeddings()\\n    vectordb = FAISS.from_texts(docs, embeddings)\\n    retriever = vectordb.as_retriever()\\n    qa_chain = RetrievalQA.from_chain_type(\\n        llm=ChatOpenAI(model=\"gpt-4o-mini\"),\\n        retriever=retriever,\\n        return_source_documents=False,\\n    )\\n    answer = qa_chain.invoke({\"question\": state.user_input})\\n    state.llm_output = answer[\"result\"]\\n    return state\\n\\ndef final_output(state: GraphState) -> str:\\n    \"\"\"Produce the user‑facing answer.\"\"\"\\n    if state.intent == \"weather\":\\n        return state.tool_result or \"Sorry, I couldn\\'t get the weather.\"\\n    elif state.intent == \"qa\":\\n        return state.llm_output or \"I couldn\\'t find an answer.\"\\n    else:\\n        return \"I\\'m not sure how to help with that.\"\\n\\n# ---------- 4️⃣ Build the graph ----------\\nbuilder = GraphBuilder()\\nbuilder.add_node(\"router\", router_node)\\nbuilder.add_node(\"weather\", weather_tool)\\nbuilder.add_node(\"qa\", qa_node)\\nbuilder.add_node(\"output\", final_output)\\n\\n# Unconditional edge from router to output (fallback)\\nbuilder.add_edge(\"router\", \"output\")\\n\\n# Conditional edges based on intent\\nbuilder.add_conditional_edge(\\n    source=\"router\",\\n    target=\"weather\",\\n    condition=lambda s: s.intent == \"weather\",\\n)\\nbuilder.add_conditional_edge(\\n    source=\"router\",\\n    target=\"qa\",\\n    condition=lambda s: s.intent == \"qa\",\\n)\\n\\n# After a tool/qa node finishes, go to the output node\\nbuilder.add_edge(\"weather\", \"output\")\\nbuilder.add_edge(\"qa\", \"output\")\\n\\ngraph: StateGraph[GraphState] = builder.compile()\\n\\n# ---------- 5️⃣ Run the graph ----------\\ndef run_chat(message: str) -> str:\\n    init_state = GraphState(user_input=message)\\n    result_state = graph.invoke(init_state)   # runs the whole workflow\\n    # `final_output` returns a plain string, so the invoke result is that string.\\n    return result_state\\n\\n# Example usage\\nprint(run_chat(\"What\\'s the weather in Berlin?\"))\\nprint(run_chat(\"Who created Python?\"))\\nprint(run_chat(\"Tell me a joke.\"))\\n```\\n\\n**What you see in the code**\\n\\n1. **State model** (`GraphState`) that travels through the graph.\\n2. **Router node** decides the intent.\\n3. **Conditional edges** send the flow to either a weather‑tool node or a QA node.\\n4. **Final output node** assembles the response.\\n5. The **graph is compiled** once and can be invoked many times, with automatic checkpointing and traceability.\\n\\n---\\n\\n## Advanced Features You Might Want Later\\n\\n| Feature | What It Gives You | Example API |\\n|---------|-------------------|-------------|\\n| **Async execution** | Run long‑running tool calls in parallel, or interleave with external queues. | `await graph.ainvoke(state)` |\\n| **Looping / Re‑tries** | Keep looping until a termination condition (e.g., “LLM says it’s done”). | `builder.add_loop_edge(\"agent\", \"agent\", condition=…)` |\\n| **Sub‑graphs (nested graphs)** | Compose complex workflows from reusable sub‑graphs (e.g., a “search‑and‑summarize” sub‑graph). | `subgraph = builder.compile(); builder.add_node(\"search\", subgraph)` |\\n| **Persistence** | Store checkpoints in Redis, DynamoDB, PostgreSQL, or a simple file. | `graph = builder.compile(checkpoint_store=RedisCheckpointStore(...))` |\\n| **Observability & Tracing** | Auto‑emit OpenTelemetry spans, log each node’s inputs/outputs, visualize execution path. | `graph.enable_tracing()` |\\n| **Human‑in‑the‑loop** | Pause execution at a node, wait for a human to edit the state, then resume. | `builder.add_human_input_node(\"review\")` |\\n| **Tool integration via LangChain “tool” interface** | Use any LangChain tool (search, calculator, database) as a node without extra boilerplate. | `from langchain.tools import TavilySearchResults; builder.add_node(\"search\", TavilySearchResults())` |\\n\\n---\\n\\n## When to Use LangGraph (and When Not To)\\n\\n| Situation | Recommended |\\n|-----------|--------------|\\n| **Simple, one‑shot prompts** (e.g., “Summarize this text”) | No need for a graph; a plain LangChain chain or `Runnable` is enough. |\\n| **Multi‑step, conditional, or looping logic** (agents, tool orchestration, retries) | LangGraph shines. |\\n| **Need fine‑grained observability or resumable runs** | Use LangGraph’s checkpointing and tracing. |\\n| **You already have a DAG engine (Airflow, Prefect) for batch jobs** | You might embed LangChain calls inside those, but for *LLM‑centric* branching LangGraph is lighter. |\\n| **Very low‑latency, single‑function inference** | Adding graph overhead could be unnecessary; stick to direct model calls. |\\n\\n---\\n\\n## Ecosystem & Community\\n\\n- **GitHub**: https://github.com/langchain-ai/langgraph (active development, issue tracker, examples)\\n- **Documentation**: https://langchain-ai.github.io/langgraph/\\n- **Tutorials**: The repo includes a `examples/` folder with real‑world patterns (agentic loops, tool calling, multi‑modal pipelines).\\n- **Integrations**: Works seamlessly with all LangChain providers (OpenAI, Anthropic, Cohere, Llama‑cpp, etc.) and with LangChain’s `Runnable`, `Tool`, `Memory`, `Retriever`, and `VectorStore` abstractions.\\n- **Community**: LangChain Discord, LangGraph channel, weekly office‑hours, and a growing collection of blog posts (e.g., “Building a ReAct‑style agent with LangGraph”).\\n\\n---\\n\\n## TL;DR Summary\\n\\n- **LangGraph** = a **graph‑oriented workflow engine** built on top of LangChain.\\n- It lets you define **nodes** (LLM calls, tools, Python functions) and **edges** (conditional or unconditional transitions) that operate on a shared, typed **state**.\\n- The result is a **deterministic, testable, observable, and resumable** LLM application, perfect for agents, multi‑turn chats, data pipelines, and any scenario where you need more than a single prompt‑to‑output step.\\n- The library is lightweight, fully compatible with existing LangChain components, and supports async execution, checkpointing, sub‑graphs, and rich observability.\\n\\nIf you’re building anything beyond a “single prompt”—especially when you need to call tools, loop, or handle branching—LangGraph is the recommended way to structure your code.', additional_kwargs={'reasoning_content': 'The user asks: \"What is LangGraph?\" We need to answer. We can provide explanation: LangGraph is a library for building LLM-powered applications with stateful workflows, graph-based execution, built on LangChain, supports agents, loops, memory, etc. Provide overview, features, use cases, example code. No need to call external functions. Just answer.'}, response_metadata={'token_usage': {'completion_tokens': 3159, 'prompt_tokens': 195, 'total_tokens': 3354, 'completion_time': 6.731782979, 'completion_tokens_details': {'reasoning_tokens': 76}, 'prompt_time': 0.00799753, 'prompt_tokens_details': None, 'queue_time': 0.04587622, 'total_time': 6.739780509}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_d29d1d1418', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019c6820-8435-76a1-bc9b-44a735b839fa-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 195, 'output_tokens': 3159, 'total_tokens': 3354, 'output_token_details': {'reasoning': 76}})]}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.invoke({\"messages\": [HumanMessage(content=\"What is LangGraph?\")]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "448a376d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---CALL AGENT---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='what is machine learning', additional_kwargs={}, response_metadata={}, id='9255a691-a86c-439d-95a9-cd2dfcc0bf01'),\n",
       "  AIMessage(content='**Machine learning (ML)** is a subfield of artificial intelligence (AI) that focuses on building systems that can **learn from data** and improve their performance on a task without being explicitly programmed for every possible situation.\\n\\n### Core Idea\\n- **Traditional programming**: You write explicit rules (if‑else statements) that tell a computer exactly what to do.\\n- **Machine learning**: You give the computer **examples** (data) and a **goal** (e.g., predict a label, generate text, control a robot). The computer automatically discovers patterns in the data and creates a model that can make predictions or decisions on new, unseen data.\\n\\n### How It Works (Simplified)\\n1. **Collect Data** – Gather a dataset that represents the problem (e.g., images of cats and dogs, historical stock prices, user click logs).\\n2. **Choose a Model** – Select a mathematical structure that can capture patterns (e.g., linear regression, decision tree, neural network).\\n3. **Training** – Feed the data to the model and adjust its internal parameters (weights) to minimize error on a **training objective** (like predicting the correct label).\\n4. **Evaluation** – Test the trained model on separate data it hasn’t seen to estimate how well it will perform in the real world.\\n5. **Deployment** – Use the model to make predictions on new inputs (e.g., classify a new photo, recommend a product).\\n\\n### Main Types of Machine Learning\\n\\n| Category | What It Does | Typical Algorithms |\\n|----------|--------------|--------------------|\\n| **Supervised Learning** | Learns a mapping from inputs to known outputs (labels). | Linear regression, logistic regression, support vector machines, random forests, deep neural networks. |\\n| **Unsupervised Learning** | Finds structure in data without explicit labels. | K‑means clustering, hierarchical clustering, principal component analysis (PCA), autoencoders. |\\n| **Semi‑Supervised Learning** | Uses a small amount of labeled data plus a large amount of unlabeled data. | Variants of supervised methods that incorporate unlabeled data (e.g., label propagation). |\\n| **Reinforcement Learning** | Learns by interacting with an environment and receiving rewards/punishments. | Q‑learning, Deep Q‑Networks (DQN), policy gradient methods, AlphaGo‑style algorithms. |\\n| **Self‑Supervised Learning** | Generates its own supervisory signal from raw data (common in modern NLP and vision). | Masked language modeling (BERT), contrastive learning (SimCLR). |\\n| **Transfer Learning** | Reuses knowledge from one task/domain to improve learning on another. | Fine‑tuning pretrained models like BERT, GPT, ResNet. |\\n\\n### Common Applications\\n\\n- **Computer Vision** – Image classification, object detection, facial recognition.\\n- **Natural Language Processing (NLP)** – Sentiment analysis, machine translation, chatbots, text summarization.\\n- **Recommendation Systems** – Suggest movies, products, music based on user behavior.\\n- **Healthcare** – Diagnose diseases from medical images, predict patient outcomes.\\n- **Finance** – Fraud detection, algorithmic trading, credit scoring.\\n- **Robotics & Autonomous Vehicles** – Perception, path planning, control.\\n- **Speech & Audio** – Speech recognition, voice assistants, music recommendation.\\n\\n### Key Concepts & Terminology\\n\\n- **Feature**: An individual measurable property or characteristic used as input (e.g., pixel intensity, word count, temperature).\\n- **Label/Target**: The desired output the model should predict (e.g., “cat” vs. “dog”, house price).\\n- **Model**: The mathematical function that maps features to predictions.\\n- **Loss Function**: Quantifies how far the model’s predictions are from the true labels; training aims to minimize this loss.\\n- **Optimization Algorithm**: Method for adjusting model parameters to reduce loss (e.g., gradient descent, Adam).\\n- **Overfitting**: When a model learns the training data too well, including noise, and performs poorly on new data.\\n- **Underfitting**: When a model is too simple to capture the underlying pattern, leading to poor performance even on training data.\\n- **Regularization**: Techniques (L1/L2 penalties, dropout, early stopping) that help prevent overfitting.\\n- **Cross‑Validation**: Splitting data into multiple train/validation folds to assess model robustness.\\n\\n### A Simple Example (Linear Regression)\\n\\nSuppose you have data on house sizes (square feet) and their sale prices.  \\n- **Goal**: Predict price from size.  \\n- **Model**: `price = w * size + b` (a straight line).  \\n- **Training**: Adjust `w` (slope) and `b` (intercept) to minimize the squared difference between predicted and actual prices across the training set.  \\n- **Result**: After training, the model can estimate the price of a new house given its size.\\n\\n### Getting Started\\n\\n1. **Learn the Basics**: Statistics, linear algebra, and probability fundamentals.\\n2. **Pick a Language/Framework**: Python is dominant; popular libraries include **scikit‑learn** (classic ML), **TensorFlow** and **PyTorch** (deep learning).\\n3. **Hands‑On Projects**: Start with tutorials like “Iris classification”, “MNIST digit recognition”, or “sentiment analysis on movie reviews”.\\n4. **Explore Data**: Practice data cleaning, visualization, and feature engineering—often the most time‑consuming part.\\n5. **Iterate**: Experiment with different models, hyperparameters, and evaluation metrics.\\n\\n### Resources\\n\\n- **Books**: *Pattern Recognition and Machine Learning* (Bishop), *Hands‑On Machine Learning with Scikit‑Learn, Keras, and TensorFlow* (Geron).\\n- **Online Courses**: Coursera’s *Machine Learning* (Andrew Ng), fast.ai’s *Practical Deep Learning for Coders*, Udacity’s *Intro to Machine Learning*.\\n- **Communities**: Stack Overflow, r/MachineLearning on Reddit, Kaggle competitions & notebooks.\\n\\n---\\n\\n**In a nutshell:** Machine learning is about giving computers data and a goal, letting them automatically discover the underlying patterns, and then using those patterns to make predictions or decisions on new data. It powers many of the intelligent services we use every day, from voice assistants to personalized recommendations.', additional_kwargs={'reasoning_content': 'The user asks: \"what is machine learning\". Need to provide a clear explanation. No tool needed. Provide definition, types, applications, basics.'}, response_metadata={'token_usage': {'completion_tokens': 1370, 'prompt_tokens': 194, 'total_tokens': 1564, 'completion_time': 2.935990456, 'completion_tokens_details': {'reasoning_tokens': 31}, 'prompt_time': 0.026461989, 'prompt_tokens_details': None, 'queue_time': 0.056318263, 'total_time': 2.9624524450000003}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_626f3fc5e0', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019c6820-a027-7941-8e59-83eb8cf81665-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 194, 'output_tokens': 1370, 'total_tokens': 1564, 'output_token_details': {'reasoning': 31}})]}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.invoke({\"messages\": \"what is machine learning\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c03cb88a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---CALL AGENT---\n",
      "---CHECK RELEVANCE---\n",
      "---DECISION: DOCS RELEVANT---\n",
      "---GENERATE---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='weather in bengaluru', additional_kwargs={}, response_metadata={}, id='edcb3e71-c94a-48e4-a173-c0cfb294447d'),\n",
       "  AIMessage(content='', additional_kwargs={'reasoning_content': 'We need to get weather for Bengaluru using get_weather tool.', 'tool_calls': [{'id': 'fc_8a806ba7-6db3-4d9d-8864-c112f48f3c80', 'function': {'arguments': '{\"city\":\"Bengaluru\"}', 'name': 'get_weather'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 43, 'prompt_tokens': 195, 'total_tokens': 238, 'completion_time': 0.097119056, 'completion_tokens_details': {'reasoning_tokens': 13}, 'prompt_time': 0.041282642, 'prompt_tokens_details': None, 'queue_time': 0.049806287, 'total_time': 0.138401698}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_d29d1d1418', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019c6820-ad64-7043-a8f4-f87a19613129-0', tool_calls=[{'name': 'get_weather', 'args': {'city': 'Bengaluru'}, 'id': 'fc_8a806ba7-6db3-4d9d-8864-c112f48f3c80', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 195, 'output_tokens': 43, 'total_tokens': 238, 'output_token_details': {'reasoning': 13}}),\n",
       "  ToolMessage(content='Bengaluru is 19.3°C', name='get_weather', id='be327d36-58b0-4050-9ede-0fcfaefc20c0', tool_call_id='fc_8a806ba7-6db3-4d9d-8864-c112f48f3c80'),\n",
       "  HumanMessage(content='The temperature in Bengaluru is currently about 19.3\\u202f°C.', additional_kwargs={}, response_metadata={}, id='62eae9ad-c623-4d84-81eb-08d3e44c8339')]}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.invoke({\"messages\": \"weather in bengaluru\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "48cb2990",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EVAL\n",
    "\n",
    "EVAL_SET = [\n",
    "    {\n",
    "        \"question\": \"what is langchain?\",\n",
    "        \"expected_keywords\": [\"langchain\"]\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"what is langgraph?\",\n",
    "        \"expected_keywords\": [\"langgraph\"]\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is the weather in Delhi?\",\n",
    "        \"expected_keywords\": [\"Delhi\", \"°C\"]\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "33ed6616",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "def call_agent(question: str):\n",
    "    # initial state for your agent\n",
    "    state = {\n",
    "        \"messages\": [HumanMessage(content=question)]\n",
    "    }\n",
    "\n",
    "    final_state = graph.invoke(state)\n",
    "\n",
    "    # last message is model output\n",
    "    last_message = final_state[\"messages\"][-1]\n",
    "\n",
    "    return last_message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "cdb4dd81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q: what is langchain?\n",
      "---CALL AGENT---\n",
      "A: **LangChain** is an open‑source framework that makes it easier to build applications powered by large language models (LLMs). It provides a collection of modular building blocks—**chains**, **agents**, **memory**, **prompts**, **document loaders**, **vector stores**, **retrievers**, and more—so developers can quickly assemble complex LLM‑driven workflows without reinventing the wheel.\n",
      "\n",
      "### Core ideas\n",
      "\n",
      "| Concept | What it does | Typical use‑case |\n",
      "|---------|--------------|-----------------|\n",
      "| **Chains** | Sequentially link together components (e.g., prompt → LLM → post‑processing). | Simple pipelines like “ask a question → get answer → format”. |\n",
      "| **Agents** | Decide which actions to take based on LLM output, often invoking external tools or APIs. | Chatbots that can browse the web, run code, or query a database. |\n",
      "| **Memory** | Persist context across turns of a conversation so the LLM can refer back to earlier information. | Conversational assistants that remember user preferences. |\n",
      "| **Prompt Templates** | Parameterized prompt strings that can be reused and rendered with variables. | Standardizing prompts for QA, summarization, etc. |\n",
      "| **Document Loaders** | Helpers to ingest data from PDFs, Word docs, webpages, databases, etc. | Building a knowledge base from existing documents. |\n",
      "| **Vector Stores / Retrievers** | Store embeddings of text chunks and retrieve the most relevant pieces for a query. | Retrieval‑augmented generation (RAG) where the LLM answers using up‑to‑date facts. |\n",
      "| **Tool Integrations** | Connect to APIs, databases, code execution environments, etc. | “Ask the assistant to calculate a mortgage payment” → calls a calculator tool. |\n",
      "\n",
      "### Why use LangChain?\n",
      "\n",
      "1. **Modularity** – You can swap out any component (different LLM provider, vector DB, prompt style) without rewriting the whole app.\n",
      "2. **Productivity** – Common patterns (RAG, tool‑using agents, multi‑step reasoning) are already implemented, letting you focus on the unique parts of your product.\n",
      "3. **Ecosystem** – Supports many LLM providers (OpenAI, Anthropic, Cohere, Hugging Face, Azure, etc.), vector stores (Pinecone, Weaviate, Chroma, FAISS, etc.), and integrates with popular Python and JavaScript/TypeScript environments.\n",
      "4. **Community & Documentation** – Rich examples, tutorials, and a vibrant open‑source community that contributes new integrations and patterns.\n",
      "\n",
      "### Typical architecture of a LangChain‑powered app\n",
      "\n",
      "```\n",
      "User Input\n",
      "   ↓\n",
      "PromptTemplate (fills in variables)\n",
      "   ↓\n",
      "Chain / Agent\n",
      "   ├─ Retriever (searches vector store for relevant docs)\n",
      "   │     ↓\n",
      "   │   Retrieved chunks → added to prompt\n",
      "   ├─ LLM (generates response)\n",
      "   │     ↓\n",
      "   └─ Output Parser / Post‑processor\n",
      "   ↓\n",
      "Final Answer (sent back to user)\n",
      "```\n",
      "\n",
      "### Example (Python)\n",
      "\n",
      "```python\n",
      "from langchain import OpenAI, PromptTemplate, LLMChain\n",
      "from langchain.vectorstores import Chroma\n",
      "from langchain.embeddings import OpenAIEmbeddings\n",
      "from langchain.retrievers import BM25Retriever\n",
      "\n",
      "# 1️⃣ Load a vector store with document embeddings\n",
      "embeddings = OpenAIEmbeddings()\n",
      "vector_store = Chroma(persist_directory=\"db\", embedding_function=embeddings)\n",
      "\n",
      "# 2️⃣ Create a retriever\n",
      "retriever = vector_store.as_retriever(search_kwargs={\"k\": 4})\n",
      "\n",
      "# 3️⃣ Define a prompt that includes retrieved context\n",
      "prompt = PromptTemplate(\n",
      "    template=\"\"\"\n",
      "    Use the following context to answer the question.\n",
      "    Context: {context}\n",
      "    Question: {question}\n",
      "    Answer:\"\"\",\n",
      "    input_variables=[\"context\", \"question\"],\n",
      ")\n",
      "\n",
      "# 4️⃣ Build a chain that retrieves, formats the prompt, calls the LLM, and returns the answer\n",
      "llm = OpenAI(model=\"gpt-4\")\n",
      "chain = LLMChain(\n",
      "    llm=llm,\n",
      "    prompt=prompt,\n",
      "    retriever=retriever,\n",
      "    # optional: combine retrieved docs into a single string\n",
      "    combine_documents_chain=lambda docs: \"\\n\".join([d.page_content for d in docs]),\n",
      ")\n",
      "\n",
      "# 5️⃣ Run it\n",
      "answer = chain.run({\"question\": \"What is LangChain?\"})\n",
      "print(answer)\n",
      "```\n",
      "\n",
      "### Getting started\n",
      "\n",
      "- **Python**: `pip install langchain openai` (plus any vector‑store client you need). The official docs have a “quick start” notebook.\n",
      "- **JavaScript/TypeScript**: `npm install langchain`. The API mirrors the Python one, with similar concepts.\n",
      "- **Tutorials**: Check out the LangChain “Examples” repo on GitHub; it contains ready‑to‑run notebooks for RAG, agents, chatbots, and more.\n",
      "\n",
      "---\n",
      "\n",
      "**In short:** LangChain is a toolbox that abstracts away the plumbing required to turn raw LLM calls into full‑featured, production‑ready AI applications. It lets you focus on the *logic* of your product while handling the *integration* details for you.\n",
      "✅ PASS\n",
      "\n",
      "Q: what is langgraph?\n",
      "---CALL AGENT---\n",
      "A: **LangGraph** is an open‑source Python library (maintained by the LangChain team) that makes it easy to build **state‑ful, graph‑structured workflows** for LLM‑driven applications.\n",
      "\n",
      "---\n",
      "\n",
      "## Why a separate library?\n",
      "\n",
      "- **LangChain** excels at chaining together LLM calls, tools, and memory, but most of its primitives are *linear* (a simple sequence of steps).  \n",
      "- Real‑world AI assistants, agents, and data pipelines often need **branches, loops, conditionals, and parallelism**—exactly the kinds of structures you’d draw in a flowchart.  \n",
      "- LangGraph fills that gap by letting you declare a **directed graph** of “nodes” (functions that run LLM calls, tool invocations, or arbitrary Python) and a **state object** that travels along the edges.\n",
      "\n",
      "---\n",
      "\n",
      "## Core Concepts\n",
      "\n",
      "| Concept | What it is | Typical use |\n",
      "|---------|------------|-------------|\n",
      "| **State** | A mutable Python dict (or any pydantic model) that holds the current context: user input, LLM outputs, intermediate variables, tool results, etc. | Acts like the “blackboard” that every node can read from and write to. |\n",
      "| **Node** | A callable (async or sync) that receives the current `state` and returns an updated `state`. | Could be an LLM prompt, a tool call, a database query, etc. |\n",
      "| **Edge** | A rule that decides which node to execute next, based on the current `state`. | Enables conditionals (`if/else`), loops (`while`), and branching (`switch`). |\n",
      "| **Graph** | The collection of nodes + edges, defined declaratively (often with the `@graph` decorator). | The executable workflow. |\n",
      "| **Runner** | The engine that steps through the graph, feeding the state from node to node until a termination condition is met. | Handles async execution, retries, logging, and checkpointing. |\n",
      "\n",
      "---\n",
      "\n",
      "## How It Looks in Code (simplified)\n",
      "\n",
      "```python\n",
      "from langgraph.graph import Graph\n",
      "from langchain.llms import OpenAI\n",
      "from typing import Dict, Any\n",
      "\n",
      "# 1️⃣ Define the shared state shape (optional but helpful)\n",
      "State = Dict[str, Any]\n",
      "\n",
      "# 2️⃣ Create the LLM you’ll use\n",
      "llm = OpenAI(model=\"gpt-4o-mini\")\n",
      "\n",
      "# 3️⃣ Define node functions\n",
      "def greet(state: State) -> State:\n",
      "    state[\"messages\"] = [{\"role\": \"assistant\", \"content\": \"Hello! How can I help you today?\"}]\n",
      "    return state\n",
      "\n",
      "def ask_question(state: State) -> State:\n",
      "    # Use the LLM with the current messages\n",
      "    response = llm.invoke(state[\"messages\"])\n",
      "    state[\"messages\"].append({\"role\": \"assistant\", \"content\": response})\n",
      "    return state\n",
      "\n",
      "def decide_next(state: State) -> str:\n",
      "    # Simple conditional edge: if user says \"bye\" we end, else loop\n",
      "    last = state[\"messages\"][-1][\"content\"].lower()\n",
      "    return \"end\" if \"bye\" in last else \"ask_question\"\n",
      "\n",
      "# 4️⃣ Build the graph\n",
      "graph = Graph()\n",
      "graph.add_node(\"greet\", greet)\n",
      "graph.add_node(\"ask_question\", ask_question)\n",
      "\n",
      "# Edge definitions (source, target, condition)\n",
      "graph.add_edge(\"greet\", \"ask_question\")          # after greeting go to ask_question\n",
      "graph.add_edge(\"ask_question\", \"ask_question\", condition=decide_next)  # loop\n",
      "graph.set_terminal(\"end\")                       # termination node\n",
      "\n",
      "# 5️⃣ Run it\n",
      "runner = graph.get_runner()\n",
      "final_state = runner.run(initial_state={})\n",
      "print(final_state[\"messages\"])\n",
      "```\n",
      "\n",
      "*What you see*:  \n",
      "- The graph starts at `greet`, then moves to `ask_question`.  \n",
      "- After each answer, `decide_next` inspects the state and either loops back or terminates.  \n",
      "- All data (conversation history, flags, tool outputs) lives in the shared `state`.\n",
      "\n",
      "---\n",
      "\n",
      "## Key Features\n",
      "\n",
      "| Feature | What it gives you |\n",
      "|---------|-------------------|\n",
      "| **Explicit state passing** | No hidden globals; debugging is straightforward because you can inspect the state at any step. |\n",
      "| **Conditional branching** | `if/else`, `switch`, or custom Python predicates to choose the next node. |\n",
      "| **Loops & retries** | Naturally express “keep asking until the answer meets a confidence threshold” or “retry on tool failure”. |\n",
      "| **Parallel sub‑graphs** | Run independent branches concurrently (via `asyncio.gather` under the hood). |\n",
      "| **Check‑pointing & persistence** | Serialize the state to a DB or file, then resume later—useful for long‑running agents. |\n",
      "| **Tool integration** | Nodes can call any Python function, external API, or LangChain tool; the result is just another piece of state. |\n",
      "| **Observability** | Built‑in logging, tracing (compatible with LangChain’s `CallbackManager`), and optional LangChain‑style `Callbacks` for each node. |\n",
      "| **Typed state (pydantic)** | Optional schema enforcement to catch bugs early. |\n",
      "| **Composable sub‑graphs** | A graph can be treated as a node inside a larger graph, enabling hierarchical workflow design. |\n",
      "\n",
      "---\n",
      "\n",
      "## When to Use LangGraph vs. Plain LangChain\n",
      "\n",
      "| Situation | Recommended |\n",
      "|-----------|--------------|\n",
      "| **Linear chain of prompts** (e.g., “prompt → LLM → prompt → LLM”) | Plain LangChain (`SequentialChain`, `RunnableSequence`). |\n",
      "| **Dynamic decision making** (branch based on user intent, confidence, tool output) | LangGraph. |\n",
      "| **Iterative refinement** (loop until answer satisfies a validator) | LangGraph. |\n",
      "| **Multi‑agent coordination** (different agents talk to each other, share a common state) | LangGraph. |\n",
      "| **Long‑running processes that may be paused/resumed** | LangGraph (state checkpointing). |\n",
      "\n",
      "---\n",
      "\n",
      "## Relationship to LangChain\n",
      "\n",
      "- **LangChain** provides the *building blocks*: LLM wrappers, prompt templates, memory, tools, retrievers, etc.  \n",
      "- **LangGraph** is a *orchestration layer* that arranges those blocks in a **graph** rather than a simple linear pipeline.  \n",
      "- You can import any LangChain component inside a LangGraph node, and you can even embed an entire LangChain `Runnable` as a node.\n",
      "\n",
      "---\n",
      "\n",
      "## Ecosystem & Community\n",
      "\n",
      "| Resource | Link |\n",
      "|----------|------|\n",
      "| **GitHub repository** | `https://github.com/langchain-ai/langgraph` |\n",
      "| **Documentation** | `https://langchain.com/docs/langgraph/` |\n",
      "| **Tutorial notebooks** | “Getting Started with LangGraph” Jupyter notebooks in the repo. |\n",
      "| **Discord / Community** | LangChain Discord `#langgraph` channel for Q&A and examples. |\n",
      "| **Blog posts** | “Stateful LLM Agents with LangGraph” (official LangChain blog). |\n",
      "\n",
      "---\n",
      "\n",
      "## Simple Real‑World Example: Customer‑Support Bot\n",
      "\n",
      "```python\n",
      "from langgraph import Graph\n",
      "from langchain import OpenAI, tool\n",
      "\n",
      "# LLM\n",
      "llm = OpenAI(model=\"gpt-4o-mini\")\n",
      "\n",
      "# Tools\n",
      "@tool\n",
      "def lookup_order(order_id: str) -> str:\n",
      "    # pretend DB call\n",
      "    return f\"Order {order_id} is shipped and will arrive tomorrow.\"\n",
      "\n",
      "# Nodes\n",
      "def greet(state):\n",
      "    state[\"messages\"] = [{\"role\":\"assistant\",\"content\":\"Hi! I’m your support bot. How can I help?\"}]\n",
      "    return state\n",
      "\n",
      "def parse_intent(state):\n",
      "    # LLM decides what the user wants\n",
      "    response = llm.invoke(state[\"messages\"])\n",
      "    state[\"messages\"].append({\"role\":\"assistant\",\"content\":response})\n",
      "    # simple heuristic: if \"order\" in response → go to order_lookup\n",
      "    if \"order\" in response.lower():\n",
      "        state[\"next\"] = \"order_lookup\"\n",
      "    else:\n",
      "        state[\"next\"] = \"fallback\"\n",
      "    return state\n",
      "\n",
      "def order_lookup(state):\n",
      "    # extract order id (very naive)\n",
      "    last = state[\"messages\"][-1][\"content\"]\n",
      "    order_id = last.split()[-1]  # assume last token is id\n",
      "    result = lookup_order(order_id)\n",
      "    state[\"messages\"].append({\"role\":\"assistant\",\"content\":result})\n",
      "    return state\n",
      "\n",
      "def fallback(state):\n",
      "    state[\"messages\"].append({\"role\":\"assistant\",\"content\":\"I’m not sure how to help with that. Let me connect you to a human.\"})\n",
      "    return state\n",
      "\n",
      "# Build graph\n",
      "g = Graph()\n",
      "g.add_node(\"greet\", greet)\n",
      "g.add_node(\"parse_intent\", parse_intent)\n",
      "g.add_node(\"order_lookup\", order_lookup)\n",
      "g.add_node(\"fallback\", fallback)\n",
      "\n",
      "g.add_edge(\"greet\", \"parse_intent\")\n",
      "g.add_edge(\"parse_intent\", \"order_lookup\", condition=lambda s: s[\"next\"]==\"order_lookup\")\n",
      "g.add_edge(\"parse_intent\", \"fallback\", condition=lambda s: s[\"next\"]==\"fallback\")\n",
      "g.set_terminal(\"end\")\n",
      "\n",
      "# Run\n",
      "runner = g.get_runner()\n",
      "final = runner.run(initial_state={})\n",
      "print(\"\\n\".join(m[\"content\"] for m in final[\"messages\"]))\n",
      "```\n",
      "\n",
      "*Result*: The bot greets, decides (via the LLM) that the user is asking about an order, calls the `lookup_order` tool, and returns the answer—all orchestrated by the graph.\n",
      "\n",
      "---\n",
      "\n",
      "## Getting Started Checklist\n",
      "\n",
      "1. **Install**  \n",
      "   ```bash\n",
      "   pip install langgraph langchain openai   # plus any provider SDKs you need\n",
      "   ```\n",
      "\n",
      "2. **Create a simple graph** (use the “Hello world” example from the docs).\n",
      "\n",
      "3. **Define a state schema** (optional but recommended with `pydantic.BaseModel`).\n",
      "\n",
      "4. **Add nodes** that wrap your existing LangChain prompts, tools, or custom Python.\n",
      "\n",
      "5. **Wire edges** using simple condition functions or the built‑in `switch` helper.\n",
      "\n",
      "6. **Run** with `graph.get_runner().run(initial_state={})`.\n",
      "\n",
      "7. **Iterate** – add loops, parallel branches, or checkpoint persistence as the workflow grows.\n",
      "\n",
      "---\n",
      "\n",
      "### TL;DR\n",
      "\n",
      "**LangGraph** = a lightweight framework for **stateful, graph‑based orchestration** of LLM calls, tools, and arbitrary Python. It builds on LangChain’s components but gives you the expressive power of conditionals, loops, and parallelism while keeping the entire conversation/context in a single, inspectable state object. Perfect for agents, assistants, and any LLM workflow that isn’t just a straight line.\n",
      "✅ PASS\n",
      "\n",
      "Q: What is the weather in Delhi?\n",
      "---CALL AGENT---\n",
      "---CHECK RELEVANCE---\n",
      "---DECISION: DOCS RELEVANT---\n",
      "---GENERATE---\n",
      "A: The temperature in Delhi is 2.4 °C.\n",
      "✅ PASS\n",
      "\n",
      "Final accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "def run_eval(eval_set):\n",
    "    passed = 0\n",
    "    results = []\n",
    "\n",
    "    for item in eval_set:\n",
    "        q = item[\"question\"]\n",
    "        expected = item[\"expected_keywords\"]\n",
    "\n",
    "        print(f\"\\nQ: {q}\")\n",
    "        answer = call_agent(q)\n",
    "        print(f\"A: {answer}\")\n",
    "\n",
    "        score = all(word.lower() in answer.lower() for word in expected)\n",
    "\n",
    "        if score:\n",
    "            print(\"✅ PASS\")\n",
    "            passed += 1\n",
    "        else:\n",
    "            print(\"❌ FAIL\")\n",
    "\n",
    "        results.append({\n",
    "            \"question\": q,\n",
    "            \"answer\": answer,\n",
    "            \"passed\": score\n",
    "        })\n",
    "\n",
    "    accuracy = passed / len(eval_set)\n",
    "    print(f\"\\nFinal accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "    return results, accuracy\n",
    "\n",
    "results, accuracy = run_eval(EVAL_SET)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3bfb612d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q: what is langchain?\n",
      "---CALL AGENT---\n",
      "A: **LangChain** is an open‑source framework that makes it easy to build applications powered by large language models (LLMs). It provides a set of modular, composable building blocks that let developers:\n",
      "\n",
      "| Category | What it does | Typical use‑cases |\n",
      "|----------|--------------|-------------------|\n",
      "| **Prompt Management** | Templates, dynamic variable injection, prompt optimization | Chatbots, summarization, Q&A |\n",
      "| **LLM Wrappers** | Unified API for OpenAI, Anthropic, Cohere, HuggingFace, etc. | Switch models without changing code |\n",
      "| **Chains** | Sequential or conditional pipelines that connect prompts, LLM calls, and other functions | Retrieval‑augmented generation, tool‑use workflows |\n",
      "| **Agents** | LLMs that can decide which tool (e.g., search, calculator, database) to invoke and then act on the result | Autonomous assistants, code‑generation bots |\n",
      "| **Memory** | State‑keeping across turns (conversation history, summaries, vector store retrieval) | Conversational agents that remember context |\n",
      "| **Data Connectors** | Integrations with databases, APIs, file systems, and vector stores (FAISS, Pinecone, Chroma, etc.) | Retrieval‑augmented generation, data‑driven Q&A |\n",
      "| **Evaluation & Debugging** | Utilities for tracing, logging, and testing prompts | Rapid iteration and quality control |\n",
      "| **Deployment Helpers** | Streamlit, FastAPI, LangServe, LangFuse, LangGraph (for stateful workflows) | Turning prototypes into production services |\n",
      "\n",
      "### Why use LangChain?\n",
      "1. **Modularity** – You can mix‑and‑match components (prompts, LLMs, tools, memory) without rewriting boilerplate.\n",
      "2. **Abstraction over providers** – The same code works with OpenAI, Azure, Anthropic, etc., making it easy to experiment or switch vendors.\n",
      "3. **Built‑in best practices** – Prompt templating, output parsing, token‑budget handling, and retry logic are already handled.\n",
      "4. **Community & Ecosystem** – A vibrant open‑source community contributes integrations (vector DBs, document loaders, evaluation suites) and examples.\n",
      "5. **Scalability** – LangChain works locally, on cloud functions, or within orchestrators like LangGraph for complex, stateful workflows.\n",
      "\n",
      "### Typical Architecture of a LangChain App\n",
      "```\n",
      "User Input\n",
      "   ↓\n",
      "PromptTemplate → LLM (via LLMWrapper)\n",
      "   ↓\n",
      "(Optional) Tool Calls (Search, Calculator, DB)\n",
      "   ↓\n",
      "Memory Update (store conversation, retrieve relevant docs)\n",
      "   ↓\n",
      "Output Formatting / Post‑processing\n",
      "   ↓\n",
      "Response to User\n",
      "```\n",
      "\n",
      "### Example (Python)\n",
      "\n",
      "```python\n",
      "from langchain import LLMChain, PromptTemplate\n",
      "from langchain.llms import OpenAI\n",
      "from langchain.memory import ConversationBufferMemory\n",
      "\n",
      "# 1️⃣ Prompt template\n",
      "template = \"\"\"You are a helpful assistant. Answer the question below.\n",
      "\n",
      "Question: {question}\n",
      "Answer:\"\"\"\n",
      "prompt = PromptTemplate.from_template(template)\n",
      "\n",
      "# 2️⃣ LLM wrapper\n",
      "llm = OpenAI(model=\"gpt-4o-mini\", temperature=0.2)\n",
      "\n",
      "# 3️⃣ Memory (keeps conversation history)\n",
      "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
      "\n",
      "# 4️⃣ Chain that ties everything together\n",
      "chain = LLMChain(prompt=prompt, llm=llm, memory=memory)\n",
      "\n",
      "# 5️⃣ Run\n",
      "response = chain.run({\"question\": \"What is LangChain?\"})\n",
      "print(response)\n",
      "```\n",
      "\n",
      "The chain automatically:\n",
      "* Fills the `{question}` placeholder,\n",
      "* Sends the prompt to the LLM,\n",
      "* Stores the interaction in `memory` so the next call can reference prior context.\n",
      "\n",
      "### When to reach for LangChain\n",
      "- **Retrieval‑augmented generation** (search a vector DB, then answer)\n",
      "- **Tool‑using agents** (e.g., “find the weather in Paris and tell me if I need an umbrella”)\n",
      "- **Multi‑step reasoning** (chain together summarization → extraction → transformation)\n",
      "- **Production‑grade apps** where you need logging, retries, and easy swapping of components.\n",
      "\n",
      "In short, LangChain is the “glue” that lets you combine language models with external data, tools, and state, turning raw LLM APIs into full‑featured, maintainable applications.\n",
      "✅ PASS\n",
      "\n",
      "Q: what is langgraph?\n",
      "---CALL AGENT---\n",
      "A: \n",
      "❌ FAIL\n",
      "\n",
      "Q: What is the weather in Delhi?\n",
      "---CALL AGENT---\n",
      "A: \n",
      "❌ FAIL\n",
      "\n",
      "Final accuracy: 33.33%\n"
     ]
    }
   ],
   "source": [
    "results, accuracy = run_eval(EVAL_SET)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7c2427a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q: what is langchain?\n",
      "---CALL AGENT---\n",
      "A: **LangChain** is an open‑source framework that makes it easier to build applications powered by large language models (LLMs).  \n",
      "It was created by **Richard Vlasov** and **Harrison Chase** in 2022 and has quickly become one of the most popular toolkits for developers who want to go beyond “prompt‑and‑complete” interactions and build more sophisticated, “agent‑like” systems.\n",
      "\n",
      "---\n",
      "\n",
      "## Why LangChain exists\n",
      "\n",
      "LLMs are great at generating text, but most real‑world products need additional capabilities:\n",
      "\n",
      "| Need | What you typically have to add yourself | How LangChain helps |\n",
      "|------|------------------------------------------|---------------------|\n",
      "| **Memory** – keep track of prior conversation turns or state | Custom data structures, vector stores, databases | Built‑in memory abstractions (ConversationBuffer, SummaryMemory, etc.) |\n",
      "| **Tool use** – call APIs, run code, fetch documents | Write glue code for each tool | A unified “Tool” interface and ready‑made wrappers (search, calculator, web‑scraper, etc.) |\n",
      "| **Prompt management** – templates, dynamic variables, few‑shot examples | Manual string interpolation, error‑prone | `PromptTemplate`, `FewShotPromptTemplate`, `ChatPromptTemplate` |\n",
      "| **Chaining** – combine several LLM calls into a pipeline | Orchestrate calls manually | `Chain` objects (SimpleSequentialChain, LLMChain, etc.) |\n",
      "| **Agents** – decide which tool to call next based on LLM output | Build custom decision logic | `AgentExecutor`, `ZeroShotAgent`, `ReActAgent`, etc. |\n",
      "| **Retrieval‑augmented generation (RAG)** – retrieve relevant docs before answering | Implement vector search, chunking, embeddings yourself | `Retriever` abstractions, `VectorStoreRetriever`, `DocumentLoaders` |\n",
      "| **Evaluation & tracing** – log runs, debug, benchmark | Write your own logging/monitoring | Integrated callbacks, LangSmith (hosted tracing & evaluation platform) |\n",
      "\n",
      "In short, LangChain supplies **building blocks** (prompts, memory, tools, retrievers, agents, chains, callbacks) that you can mix‑and‑match, so you can focus on the *application logic* rather than on boilerplate integration code.\n",
      "\n",
      "---\n",
      "\n",
      "## Core Concepts (at a glance)\n",
      "\n",
      "| Concept | What it is | Typical use |\n",
      "|---------|------------|-------------|\n",
      "| **PromptTemplate** | A reusable template with placeholders (`{question}`, `{context}`) | Standardize prompts across your app |\n",
      "| **LLM / ChatModel** | Wrapper around an LLM provider (OpenAI, Anthropic, Cohere, etc.) | Call `invoke`/`stream` to get completions |\n",
      "| **Chain** | A sequence of steps where each step can be an LLM call, a tool call, or custom Python code | Build “question → retrieve docs → answer” pipelines |\n",
      "| **Agent** | An LLM that decides *which* tool to call next, based on a **tool‑use** format (e.g., ReAct) | Autonomous assistants, code‑writing bots |\n",
      "| **Tool** | A function (or API) that the LLM can invoke (e.g., calculator, web search) | Expose external capabilities to the model |\n",
      "| **Memory** | State that persists across turns (e.g., conversation history, summary) | Chatbots that remember earlier context |\n",
      "| **Retriever / VectorStore** | Stores embeddings and performs similarity search | RAG: fetch relevant documents before answering |\n",
      "| **Document Loader** | Utilities to ingest PDFs, CSVs, webpages, etc., and split them into chunks | Preparing data for vector stores |\n",
      "| **Callbacks** | Hooks for logging, tracing, streaming tokens, handling errors | Observability, LangSmith integration |\n",
      "| **LangSmith** | A hosted service (free tier) for experiment tracking, evaluation, and prompt versioning | Systematic development and monitoring |\n",
      "\n",
      "---\n",
      "\n",
      "## Typical Workflow (example)\n",
      "\n",
      "```python\n",
      "from langchain import OpenAI, PromptTemplate, LLMChain\n",
      "from langchain.vectorstores import FAISS\n",
      "from langchain.embeddings import OpenAIEmbeddings\n",
      "from langchain.document_loaders import PyPDFLoader\n",
      "from langchain.chains import RetrievalQA\n",
      "\n",
      "# 1️⃣ Load & index docs\n",
      "loader = PyPDFLoader(\"my_knowledge_base.pdf\")\n",
      "docs = loader.load_and_split()\n",
      "vectorstore = FAISS.from_documents(docs, OpenAIEmbeddings())\n",
      "\n",
      "# 2️⃣ Create a retriever (RAG)\n",
      "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
      "\n",
      "# 3️⃣ Build a prompt template\n",
      "prompt = PromptTemplate(\n",
      "    template=\"\"\"\n",
      "    Use the following pieces of context to answer the question.\n",
      "    Context: {context}\n",
      "    Question: {question}\n",
      "    \"\"\",\n",
      "    input_variables=[\"context\", \"question\"],\n",
      ")\n",
      "\n",
      "# 4️⃣ Wrap LLM + prompt in a chain\n",
      "llm = OpenAI(model=\"gpt-4o-mini\")\n",
      "qa_chain = LLMChain(llm=llm, prompt=prompt)\n",
      "\n",
      "# 5️⃣ Combine retriever + chain into a RetrievalQA object\n",
      "qa = RetrievalQA(combine_documents_chain=qa_chain,\n",
      "                 retriever=retriever)\n",
      "\n",
      "# 6️⃣ Ask a question\n",
      "answer = qa.run(\"What are the main safety guidelines for using the equipment?\")\n",
      "print(answer)\n",
      "```\n",
      "\n",
      "The code above shows how **LangChain abstracts away**:\n",
      "* document ingestion and chunking,\n",
      "* embedding + vector search,\n",
      "* prompt templating,\n",
      "* the “retrieve‑then‑answer” pattern, all in a few lines.\n",
      "\n",
      "---\n",
      "\n",
      "## Ecosystem & Integrations\n",
      "\n",
      "| Category | Notable integrations |\n",
      "|----------|----------------------|\n",
      "| **LLM providers** | OpenAI, Anthropic, Cohere, Azure OpenAI, Gemini, Llama‑CPP, Ollama, HuggingFace Inference |\n",
      "| **Vector stores** | FAISS, Pinecone, Weaviate, Milvus, Qdrant, Chroma, Elasticsearch |\n",
      "| **Document loaders** | PDFs, DOCX, CSV, HTML, Notion, Google Drive, Confluence, S3, Azure Blob |\n",
      "| **Tool wrappers** | SerpAPI (search), WolframAlpha, Python REPL, SQL DB, GitHub, Zapier, Slack, Twilio |\n",
      "| **Observability** | LangSmith, OpenTelemetry, custom callbacks |\n",
      "| **Deployment** | FastAPI, Flask, Django, Streamlit, Gradio, LangServe (micro‑service API), LangChainJS (Node/TS), LangChainGo (experimental) |\n",
      "\n",
      "Because the framework is modular, you can swap any component (e.g., replace FAISS with Pinecone) without rewriting the rest of your logic.\n",
      "\n",
      "---\n",
      "\n",
      "## When to Use LangChain\n",
      "\n",
      "| Good fit | Not ideal |\n",
      "|----------|-----------|\n",
      "| You need **retrieval‑augmented generation** (search your own docs before answering) | Pure “single‑prompt” use cases where a simple API call to OpenAI suffices |\n",
      "| You want a **chatbot/assistant** that remembers prior turns or can call external tools | Very low‑latency, on‑device inference where adding extra Python layers adds overhead |\n",
      "| Your product requires **multiple LLM steps** (e.g., generate a plan, then execute it) | Projects that need only static text generation without any dynamic logic |\n",
      "| You want **observability, versioning, and evaluation** built‑in (LangSmith) | Tiny scripts where adding a dependency feels heavyweight |\n",
      "\n",
      "---\n",
      "\n",
      "## Getting Started\n",
      "\n",
      "1. **Install**  \n",
      "   ```bash\n",
      "   pip install langchain openai   # plus any vector store you need, e.g., faiss-cpu\n",
      "   ```\n",
      "\n",
      "2. **Pick an LLM** – set your API key (`export OPENAI_API_KEY=...`) or configure another provider.\n",
      "\n",
      "3. **Follow a tutorial** – the official docs have “Quickstart” notebooks for:\n",
      "   * **Chatbot with memory**  \n",
      "   * **RAG with PDF**  \n",
      "   * **Agent that uses a calculator and web search**  \n",
      "\n",
      "4. **Iterate & monitor** – use LangSmith’s free tier to log runs, compare prompt versions, and run automated evaluations.\n",
      "\n",
      "---\n",
      "\n",
      "## TL;DR Summary\n",
      "\n",
      "- **LangChain** = a toolbox for *orchestrating* LLMs, prompts, memory, retrieval, and external tools.  \n",
      "- It abstracts repetitive boilerplate so you can compose **chains** and **agents** with a few lines of code.  \n",
      "- Ideal for building **assistants, RAG systems, autonomous agents, and any multi‑step LLM application**.  \n",
      "- Extensible across LLM providers, vector stores, document sources, and deployment platforms.  \n",
      "\n",
      "If you’re looking to move from “just call ChatGPT” to “build a full‑featured AI product,” LangChain is the most widely‑adopted, well‑documented, and community‑supported framework to do it.\n",
      "✅ PASS\n",
      "\n",
      "Q: what is langgraph?\n",
      "---CALL AGENT---\n",
      "A: **LangGraph** is an open‑source Python library (maintained by the LangChain team) that lets you build **state‑driven, graph‑structured workflows for LLM‑powered applications**.  \n",
      "\n",
      "---\n",
      "\n",
      "## 1️⃣ Core Idea  \n",
      "\n",
      "Instead of writing a linear chain of prompts (the classic “prompt‑then‑response” pattern), LangGraph lets you model the **control flow** of an LLM app as a **graph**:\n",
      "\n",
      "| **Node** | **What it does** |\n",
      "|----------|------------------|\n",
      "| **Tool/LLM call** | Executes a prompt, calls an external tool, or runs any Python function. |\n",
      "| **Condition** | Branches based on the current *state* (e.g., “if user wants a summary → go to summarizer”). |\n",
      "| **Loop** | Re‑enters a sub‑graph until a stopping condition is met (e.g., “keep asking clarification questions”). |\n",
      "| **Sub‑graph** | Re‑usable sub‑workflow that can be nested, enabling modular design. |\n",
      "\n",
      "The **state** is a mutable dictionary that travels with the execution, so every node can read/write information that later nodes can use. This makes it easy to implement multi‑turn conversations, tool‑use loops, and complex agent orchestration.\n",
      "\n",
      "---\n",
      "\n",
      "## 2️⃣ How It Relates to LangChain  \n",
      "\n",
      "| LangChain | LangGraph |\n",
      "|-----------|-----------|\n",
      "| Provides **building blocks** (LLM wrappers, prompts, memory, retrievers, tools, agents). | Provides a **graph engine** that wires those blocks together with explicit control flow. |\n",
      "| Emphasizes **sequential chains** (Chain, SequentialChain, AgentExecutor). | Emphasizes **non‑linear graphs** (nodes, edges, conditional routing, loops). |\n",
      "| Good for simple pipelines or single‑agent agents. | Ideal for multi‑agent systems, dynamic tool‑use, or any workflow where the path isn’t known ahead of time. |\n",
      "| Uses `Runnable` interface for composability. | Extends `Runnable` with a `StateGraph` class that tracks state across nodes. |\n",
      "\n",
      "You can think of LangGraph as the “orchestrator” layer on top of LangChain’s primitives.\n",
      "\n",
      "---\n",
      "\n",
      "## 3️⃣ Main Concepts & API  \n",
      "\n",
      "| Concept | Typical Class / Function | Description |\n",
      "|---------|--------------------------|-------------|\n",
      "| **StateGraph** | `StateGraph(state_schema)` | Create a graph; `state_schema` (a Pydantic model or dict) declares the keys that will live in the shared state. |\n",
      "| **Node** | `graph.add_node(name, callable)` | Register a function (LLM call, tool, or plain Python) that receives the current state and returns a new state (or part of it). |\n",
      "| **Edge** | `graph.add_edge(source, target, condition=None)` | Connect nodes. If `condition` is supplied, the edge is taken only when the lambda returns `True`. |\n",
      "| **Conditional Edge** | `graph.add_conditional_edges(start, decision_fn, {\"yes\": node_a, \"no\": node_b})` | Shortcut for branching based on a decision function. |\n",
      "| **Loop / Sub‑graph** | `graph.add_edge(node, node)` or `graph.add_subgraph(name, subgraph)` | Re‑enter a node or embed another `StateGraph`. |\n",
      "| **Compiling** | `graph.compile()` | Produces a `Runnable` that can be invoked like any LangChain component (`graph.invoke(state)`). |\n",
      "| **Memory / Persistence** | Use LangChain `Memory` objects inside nodes or store the state externally (Redis, DB). | Keeps context across multiple invocations. |\n",
      "\n",
      "**Simple example** (pseudo‑code):\n",
      "\n",
      "```python\n",
      "from langgraph.graph import StateGraph\n",
      "from langchain.llms import OpenAI\n",
      "from langchain.prompts import PromptTemplate\n",
      "\n",
      "# 1️⃣ Define the state schema\n",
      "class MyState(BaseModel):\n",
      "    user_input: str\n",
      "    intent: str | None = None\n",
      "    answer: str | None = None\n",
      "\n",
      "# 2️⃣ Create the graph\n",
      "graph = StateGraph(state_schema=MyState)\n",
      "\n",
      "# 3️⃣ Nodes\n",
      "def classify_intent(state):\n",
      "    # LLM call that extracts intent from user_input\n",
      "    intent = llm.invoke(f\"Classify intent: {state.user_input}\")\n",
      "    return {\"intent\": intent}\n",
      "\n",
      "def answer_question(state):\n",
      "    # LLM generates answer based on intent\n",
      "    answer = llm.invoke(f\"Answer the {state.intent} question.\")\n",
      "    return {\"answer\": answer}\n",
      "\n",
      "def fallback(state):\n",
      "    return {\"answer\": \"Sorry, I didn't understand that.\"}\n",
      "\n",
      "graph.add_node(\"classify\", classify_intent)\n",
      "graph.add_node(\"answer\", answer_question)\n",
      "graph.add_node(\"fallback\", fallback)\n",
      "\n",
      "# 4️⃣ Edges (conditional routing)\n",
      "graph.add_conditional_edges(\n",
      "    \"classify\",\n",
      "    lambda s: \"question\" if s[\"intent\"] == \"question\" else \"fallback\",\n",
      "    {\"question\": \"answer\", \"fallback\": \"fallback\"},\n",
      ")\n",
      "\n",
      "graph.add_edge(\"answer\", \"END\")\n",
      "graph.add_edge(\"fallback\", \"END\")\n",
      "\n",
      "# 5️⃣ Compile\n",
      "app = graph.compile()\n",
      "\n",
      "# 6️⃣ Run\n",
      "result = app.invoke({\"user_input\": \"What is the capital of France?\"})\n",
      "print(result[\"answer\"])   # → \"Paris\"\n",
      "```\n",
      "\n",
      "The graph automatically:\n",
      "* Calls the `classify` node,\n",
      "* Routes to `answer` or `fallback` based on the intent,\n",
      "* Returns the final state when the `END` node is reached.\n",
      "\n",
      "---\n",
      "\n",
      "## 4️⃣ Typical Use‑Cases  \n",
      "\n",
      "| Use‑Case | Why LangGraph shines |\n",
      "|----------|----------------------|\n",
      "| **Multi‑agent collaboration** (e.g., a planner agent that delegates to specialist agents) | Nodes can represent each agent; edges decide which agent runs next. |\n",
      "| **Tool‑use loops** (e.g., “ask the user for clarification until the tool can be called”) | Loop edges let you repeat a sub‑graph until a condition (tool‑success) is met. |\n",
      "| **Dynamic workflows** (e.g., “if the user asks for a summary, run a summarizer; if they ask for translation, run a translator”) | Conditional edges make the path data‑driven, not hard‑coded. |\n",
      "| **Complex business processes** (approval pipelines, data enrichment, QA) | The graph visualizes the process, and state provides a single source of truth for all steps. |\n",
      "| **Debugging & observability** | Each node is a separate, named step; you can log state before/after each node, and LangGraph can render a diagram of the graph. |\n",
      "\n",
      "---\n",
      "\n",
      "## 5️⃣ Advantages  \n",
      "\n",
      "| Benefit | Explanation |\n",
      "|---------|-------------|\n",
      "| **Explicit control flow** – No hidden recursion; you see exactly how the LLM moves through the workflow. |\n",
      "| **State‑centric** – All intermediate data lives in a single dict, making it easy to persist, checkpoint, or replay. |\n",
      "| **Composable** – Sub‑graphs can be reused across projects, promoting DRY code. |\n",
      "| **Observability** – Built‑in tracing (via LangChain callbacks) lets you monitor node execution, timings, and LLM usage. |\n",
      "| **Tool integration** – Nodes can call any Python function, external APIs, or LangChain tools, enabling “LLM‑as‑orchestrator” patterns. |\n",
      "| **Loop & termination handling** – You can define maximum loop counts or custom stop conditions to avoid runaway LLM calls. |\n",
      "\n",
      "---\n",
      "\n",
      "## 6️⃣ Getting Started  \n",
      "\n",
      "1. **Install**  \n",
      "\n",
      "```bash\n",
      "pip install langgraph\n",
      "# LangGraph pulls in LangChain as a dependency, so you already have the LLM wrappers.\n",
      "```\n",
      "\n",
      "2. **Read the docs** – The official docs (https://langgraph.dev) include a “Getting Started” notebook, a visual graph editor, and a gallery of real‑world examples (agentic search, multi‑turn QA, retrieval‑augmented generation, etc.).\n",
      "\n",
      "3. **Try the tutorial** – The repo ships with a minimal “Hello‑World” graph (the one shown above) and a more advanced “Planner‑Executor” example that demonstrates multi‑agent coordination.\n",
      "\n",
      "4. **Integrate with your stack** – Because the compiled graph is a `Runnable`, you can drop it into any LangChain pipeline, FastAPI endpoint, or serverless function.\n",
      "\n",
      "---\n",
      "\n",
      "## 7️⃣ Frequently Asked Questions  \n",
      "\n",
      "| Question | Answer |\n",
      "|----------|--------|\n",
      "| *Do I need to know LangChain first?* | Not strictly, but LangGraph reuses LangChain’s `Runnable` interface and LLM wrappers, so familiarity speeds things up. |\n",
      "| *Can I persist the graph state between HTTP requests?* | Yes. Store the state dict in a session store (Redis, DB, JWT, etc.) and pass it back to `graph.invoke(state)` on the next request. |\n",
      "| *Is LangGraph only for LLMs?* | No. Any callable can be a node—SQL queries, web scrapers, data‑validation functions, etc. LLMs are just a common use case. |\n",
      "| *How does it handle parallelism?* | The core engine is synchronous, but you can make a node launch asynchronous tasks (e.g., `asyncio.gather`) and await them before returning the new state. Future releases may add native parallel branches. |\n",
      "| *Is there a visual editor?* | The library can export the graph to DOT/GraphViz or Mermaid, which you can render in notebooks or docs. Some community tools provide a drag‑and‑drop UI, but the core library is code‑first. |\n",
      "| *Is it production‑ready?* | Yes. It’s used internally at OpenAI, LangChain, and several startups for production chat‑bots and autonomous agents. The library follows semantic‑versioning and has CI tests for stability. |\n",
      "\n",
      "---\n",
      "\n",
      "## 8️⃣ TL;DR  \n",
      "\n",
      "- **LangGraph** = a **graph‑based orchestration layer** built on top of LangChain.  \n",
      "- You define a **state schema**, add **nodes** (LLM calls, tools, Python functions), and connect them with **edges** that can be conditional or looping.  \n",
      "- The compiled graph is a **Runnable** you can invoke like any other LangChain component.  \n",
      "- It shines for **multi‑turn, multi‑agent, tool‑use, and dynamic workflows** where the execution path depends on data produced during the run.  \n",
      "\n",
      "If you’re already using LangChain and find yourself writing a lot of `if/else` or recursive agent loops, give LangGraph a try—it makes those patterns explicit, testable, and much easier to reason about.\n",
      "✅ PASS\n",
      "\n",
      "Q: What is the weather in Delhi?\n",
      "---CALL AGENT---\n",
      "---CHECK RELEVANCE---\n",
      "---DECISION: DOCS RELEVANT---\n",
      "---GENERATE---\n",
      "A: The temperature in Delhi is 2.4 °C.\n",
      "✅ PASS\n",
      "\n",
      "Final accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# ===== CI ENTRYPOINT =====\n",
    "\n",
    "results, accuracy = run_eval(EVAL_SET)\n",
    "\n",
    "# Fail CI if accuracy is too low\n",
    "if accuracy < 0.7:\n",
    "    raise Exception(\"Evaluation accuracy below threshold\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agenticaiworkspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
